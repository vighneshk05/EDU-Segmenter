{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDFcy5fhcnpi",
    "outputId": "4bbafcbd-546f-4f7b-aa22-909aa149d365"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DISRPT-Segmenter'...\n",
      "remote: Enumerating objects: 167, done.\u001b[K\n",
      "remote: Counting objects: 100% (167/167), done.\u001b[K\n",
      "remote: Compressing objects: 100% (112/112), done.\u001b[K\n",
      "remote: Total 167 (delta 81), reused 133 (delta 54), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (167/167), 59.89 KiB | 139.00 KiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "/content/DISRPT-Segmenter\n",
      "Collecting uv\n",
      "  Downloading uv-0.9.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.9.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m129.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.9.11\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
      "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!git clone -b main --single-branch https://github.com/Arpnik/DISRPT-Segmenter.git\n",
    "%cd /content/DISRPT-Segmenter\n",
    "\n",
    "\n",
    "!pip install uv\n",
    "!uv venv .venv\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93v6NoKpdYdX",
    "outputId": "b8e685a4-b938-4eaa-cded-80f30a7916b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 1.04s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m91 packages\u001b[0m \u001b[2min 35.23s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m91 packages\u001b[0m \u001b[2min 353ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdisrpt-segmenter\u001b[0m\u001b[2m==0.1.0 (from file:///content/DISRPT-Segmenter)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.60.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.9.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.45.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.22.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make sure we're in the repo\n",
    "%cd /content/DISRPT-Segmenter\n",
    "\n",
    "# Install project deps into the .venv using uv\n",
    "!uv pip install -e . -p .venv/bin/python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd3X1PG1vdYp",
    "outputId": "6572ae6c-47e6-4799-8d60-264be6677b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DISRPT-Segmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hL9Xvc81dYf2",
    "outputId": "b53a0bc6-5e74-4ce7-9965-23603d2f7b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "\n",
      "===================================\n",
      "Multi-Dataset EDU Segmentation Loader\n",
      "===================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.erst.gum/eng.erst.gum_train.conllu\n",
      "âœ“ Downloaded: gum/train.conllu\n",
      "âœ“ gum: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.erst.gum/eng.erst.gum_dev.conllu\n",
      "âœ“ Downloaded: gum/dev.conllu\n",
      "âœ“ gum: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.erst.gum/eng.erst.gum_test.conllu\n",
      "âœ“ Downloaded: gum/test.conllu\n",
      "âœ“ gum: test split complete\n",
      "\n",
      "âœ… gum: Download complete! All splits ready for training.\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.dep.scidtb/eng.dep.scidtb_train.conllu\n",
      "âœ“ Downloaded: scidtb/train.conllu\n",
      "âœ“ scidtb: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.dep.scidtb/eng.dep.scidtb_dev.conllu\n",
      "âœ“ Downloaded: scidtb/dev.conllu\n",
      "âœ“ scidtb: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.dep.scidtb/eng.dep.scidtb_test.conllu\n",
      "âœ“ Downloaded: scidtb/test.conllu\n",
      "âœ“ scidtb: test split complete\n",
      "\n",
      "âœ… scidtb: Download complete! All splits ready for training.\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.oll/eng.rst.oll_train.conllu\n",
      "âœ“ Downloaded: oll/train.conllu\n",
      "âœ“ oll: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.oll/eng.rst.oll_dev.conllu\n",
      "âœ“ Downloaded: oll/dev.conllu\n",
      "âœ“ oll: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.oll/eng.rst.oll_test.conllu\n",
      "âœ“ Downloaded: oll/test.conllu\n",
      "âœ“ oll: test split complete\n",
      "\n",
      "âœ… oll: Download complete! All splits ready for training.\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.sts/eng.rst.sts_train.conllu\n",
      "âœ“ Downloaded: sts/train.conllu\n",
      "âœ“ sts: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.sts/eng.rst.sts_dev.conllu\n",
      "âœ“ Downloaded: sts/dev.conllu\n",
      "âœ“ sts: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.sts/eng.rst.sts_test.conllu\n",
      "âœ“ Downloaded: sts/test.conllu\n",
      "âœ“ sts: test split complete\n",
      "\n",
      "âœ… sts: Download complete! All splits ready for training.\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.umuc/eng.rst.umuc_train.conllu\n",
      "âœ“ Downloaded: umuc/train.conllu\n",
      "âœ“ umuc: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.umuc/eng.rst.umuc_dev.conllu\n",
      "âœ“ Downloaded: umuc/dev.conllu\n",
      "âœ“ umuc: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.umuc/eng.rst.umuc_test.conllu\n",
      "âœ“ Downloaded: umuc/test.conllu\n",
      "âœ“ umuc: test split complete\n",
      "\n",
      "âœ… umuc: Download complete! All splits ready for training.\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.msdc/eng.sdrt.msdc_train.conllu\n",
      "âœ“ Downloaded: msdc/train.conllu\n",
      "âœ“ msdc: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.msdc/eng.sdrt.msdc_dev.conllu\n",
      "âœ“ Downloaded: msdc/dev.conllu\n",
      "âœ“ msdc: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.msdc/eng.sdrt.msdc_test.conllu\n",
      "âœ“ Downloaded: msdc/test.conllu\n",
      "âœ“ msdc: test split complete\n",
      "\n",
      "âœ… msdc: Download complete! All splits ready for training.\n",
      "\n",
      "ğŸ“¥ Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.stac/eng.sdrt.stac_train.conllu\n",
      "âœ“ Downloaded: stac/train.conllu\n",
      "âœ“ stac: train split complete\n",
      "\n",
      "ğŸ“¥ Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.stac/eng.sdrt.stac_dev.conllu\n",
      "âœ“ Downloaded: stac/dev.conllu\n",
      "âœ“ stac: dev split complete\n",
      "\n",
      "ğŸ“¥ Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.stac/eng.sdrt.stac_test.conllu\n",
      "âœ“ Downloaded: stac/test.conllu\n",
      "âœ“ stac: test split complete\n",
      "\n",
      "âœ… stac: Download complete! All splits ready for training.\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Tokenizer\n",
      "======================================================================\n",
      "Loading DistilBERT tokenizer...\n",
      "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 325kB/s]\n",
      "config.json: 100% 483/483 [00:00<00:00, 3.35MB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 1.09MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 1.14MB/s]\n",
      "âœ“ Tokenizer loaded\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Load and Process Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  âœ“ gum/train.conllu\n",
      "  âœ“ gum/dev.conllu\n",
      "  âœ“ gum/test.conllu\n",
      "  âœ“ scidtb/train.conllu\n",
      "  âœ“ scidtb/dev.conllu\n",
      "  âœ“ scidtb/test.conllu\n",
      "  âœ“ oll/train.conllu\n",
      "  âœ“ oll/dev.conllu\n",
      "  âœ“ oll/test.conllu\n",
      "  âœ“ stac/train.conllu\n",
      "  âœ“ stac/dev.conllu\n",
      "  âœ“ stac/test.conllu\n",
      "  âœ“ sts/train.conllu\n",
      "  âœ“ sts/dev.conllu\n",
      "  âœ“ sts/test.conllu\n",
      "  âœ“ umuc/train.conllu\n",
      "  âœ“ umuc/dev.conllu\n",
      "  âœ“ umuc/test.conllu\n",
      "  âœ“ msdc/train.conllu\n",
      "  âœ“ msdc/dev.conllu\n",
      "  âœ“ msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "âœ“ Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "âœ“ Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "âœ“ Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "âœ“ Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "âœ“ Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "âœ“ Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "âœ“ Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "âœ“ Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "âœ“ Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "âœ“ Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "âœ“ Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "âœ“ Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "âœ“ Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "âœ“ Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "âœ“ Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "âœ“ Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "âœ“ Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "âœ“ Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "âœ“ Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Sample Data Point\n",
      "======================================================================\n",
      "Input IDs shape:      torch.Size([512])\n",
      "Attention mask shape: torch.Size([512])\n",
      "Labels shape:         torch.Size([512])\n",
      "\n",
      "First 20 tokens (IDs): [101, 12465, 12284, 1998, 3009, 2396, 1024, 20062, 2013, 3239, 1011, 9651, 6249, 8925, 1011, 5811, 6249, 1012, 8925, 1011]\n",
      "First 20 labels:       [-100, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100, -100, -100]\n",
      "\n",
      "Label meanings:\n",
      "  -100 = Ignore (special tokens or subword continuations)\n",
      "     0 = Token continues current EDU\n",
      "     1 = Token starts new EDU\n",
      "\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "Dataset ready for training!\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "\n",
      "======================================================================\n",
      "VALIDATION: Detailed Token & Label Inspection\n",
      "======================================================================\n",
      "\n",
      "First 40 tokens with their labels:\n",
      "Idx   Token                Label      Meaning\n",
      "------------------------------------------------------------\n",
      "0     [CLS]                -100       IGNORE (special/subword)\n",
      "1     aesthetic            1          EDU START â† New segment begins\n",
      "2     appreciation         0          CONTINUE (within EDU)\n",
      "3     and                  0          CONTINUE (within EDU)\n",
      "4     spanish              0          CONTINUE (within EDU)\n",
      "5     art                  0          CONTINUE (within EDU)\n",
      "6     :                    0          CONTINUE (within EDU)\n",
      "7     insights             1          EDU START â† New segment begins\n",
      "8     from                 0          CONTINUE (within EDU)\n",
      "9     eye                  0          CONTINUE (within EDU)\n",
      "10    -                    0          CONTINUE (within EDU)\n",
      "11    tracking             0          CONTINUE (within EDU)\n",
      "12    claire               1          EDU START â† New segment begins\n",
      "13    bailey               0          CONTINUE (within EDU)\n",
      "14    -                    0          CONTINUE (within EDU)\n",
      "15    ross                 0          CONTINUE (within EDU)\n",
      "16    claire               0          CONTINUE (within EDU)\n",
      "17    .                    -100       IGNORE (special/subword)\n",
      "18    bailey               -100       IGNORE (special/subword)\n",
      "19    -                    -100       IGNORE (special/subword)\n",
      "20    ross                 -100       IGNORE (special/subword)\n",
      "21    @                    -100       IGNORE (special/subword)\n",
      "22    port                 -100       IGNORE (special/subword)\n",
      "23    .                    -100       IGNORE (special/subword)\n",
      "24    ac                   -100       IGNORE (special/subword)\n",
      "25    .                    -100       IGNORE (special/subword)\n",
      "26    uk                   -100       IGNORE (special/subword)\n",
      "27    university           0          CONTINUE (within EDU)\n",
      "28    of                   0          CONTINUE (within EDU)\n",
      "29    portsmouth           0          CONTINUE (within EDU)\n",
      "30    ,                    0          CONTINUE (within EDU)\n",
      "31    united               0          CONTINUE (within EDU)\n",
      "32    kingdom              0          CONTINUE (within EDU)\n",
      "33    andrew               1          EDU START â† New segment begins\n",
      "34    be                   0          CONTINUE (within EDU)\n",
      "35    ##res                -100       IGNORE (special/subword)\n",
      "36    ##ford               -100       IGNORE (special/subword)\n",
      "37    a                    0          CONTINUE (within EDU)\n",
      "38    .                    -100       IGNORE (special/subword)\n",
      "39    m                    -100       IGNORE (special/subword)\n",
      "\n",
      "======================================================================\n",
      "Label Distribution Statistics\n",
      "======================================================================\n",
      "Total tokens:           512\n",
      "Ignored (-100):            67 ( 13.1%) [Special tokens + subwords]\n",
      "EDU starts (1):            42 (  8.2%) [New segments]\n",
      "EDU continues (0):        403 ( 78.7%) [Within segments]\n",
      "\n",
      "Class Balance (excluding ignored):\n",
      "  Positive (EDU start):    9.4%\n",
      "  Negative (continue):     90.6%\n",
      "  Imbalance ratio:         9.6:1\n",
      "\n",
      "======================================================================\n",
      "Tensor Shape Validation\n",
      "======================================================================\n",
      "input_ids shape:      torch.Size([512])\n",
      "attention_mask shape: torch.Size([512])\n",
      "labels shape:         torch.Size([512])\n",
      "âœ“ All shapes match correctly\n",
      "\n",
      "======================================================================\n",
      "DATASET-WIDE ANALYSIS (Per Dataset)\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š GUM Dataset:\n",
      "  Examples:             390\n",
      "  Mean length:          509.7 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       257/512 tokens\n",
      "  Mean EDUs/sequence:   57.89\n",
      "  Total EDUs:           22578\n",
      "  Mean valid tokens:    443.4\n",
      "\n",
      "ğŸ“Š MSDC Dataset:\n",
      "  Examples:             348\n",
      "  Mean length:          490.0 tokens\n",
      "  Median length:        504.0 tokens\n",
      "  Min/Max length:       136/512 tokens\n",
      "  Mean EDUs/sequence:   46.71\n",
      "  Total EDUs:           16255\n",
      "  Mean valid tokens:    478.3\n",
      "\n",
      "ğŸ“Š OLL Dataset:\n",
      "  Examples:             76\n",
      "  Mean length:          508.0 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       372/512 tokens\n",
      "  Mean EDUs/sequence:   32.29\n",
      "  Total EDUs:           2454\n",
      "  Mean valid tokens:    473.5\n",
      "\n",
      "ğŸ“Š SCIDTB Dataset:\n",
      "  Examples:             128\n",
      "  Mean length:          512.0 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       512/512 tokens\n",
      "  Mean EDUs/sequence:   46.81\n",
      "  Total EDUs:           5992\n",
      "  Mean valid tokens:    428.8\n",
      "\n",
      "ğŸ“Š STAC Dataset:\n",
      "  Examples:             86\n",
      "  Mean length:          510.4 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       376/512 tokens\n",
      "  Mean EDUs/sequence:   106.35\n",
      "  Total EDUs:           9146\n",
      "  Mean valid tokens:    446.0\n",
      "\n",
      "ğŸ“Š STS Dataset:\n",
      "  Examples:             117\n",
      "  Mean length:          509.7 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       447/512 tokens\n",
      "  Mean EDUs/sequence:   21.23\n",
      "  Total EDUs:           2484\n",
      "  Mean valid tokens:    458.6\n",
      "\n",
      "ğŸ“Š UMUC Dataset:\n",
      "  Examples:             102\n",
      "  Mean length:          507.1 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       308/512 tokens\n",
      "  Mean EDUs/sequence:   41.43\n",
      "  Total EDUs:           4226\n",
      "  Mean valid tokens:    471.0\n",
      "\n",
      "ğŸ“Š COMBINED Dataset:\n",
      "  Total examples:       1247\n",
      "  Mean length:          504.2 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       136/512 tokens\n",
      "  Mean EDUs/sequence:   50.63\n",
      "  Total EDUs:           63135\n",
      "  Mean valid tokens:    457.3\n",
      "\n",
      "======================================================================\n",
      "Sample Sequences (from each dataset):\n",
      "======================================================================\n",
      "\n",
      "[GUM]\n",
      "  Length: 512 tokens | EDUs: 42\n",
      "  Text: aesthetic appreciation and spanish art : insights from eye - tracking claire bailey - ross claire. bailey - ross @ port. ac. uk university of portsmou...\n",
      "\n",
      "[SCIDTB]\n",
      "  Length: 512 tokens | EDUs: 38\n",
      "  Text: we propose a neural network approach to benefit from the non - linearity of corpus - wide statistics for part - of - speech ( pos ) tagging. we invest...\n",
      "\n",
      "[OLL]\n",
      "  Length: 512 tokens | EDUs: 34\n",
      "  Text: there is a popular hci term often floating around called \" intuitiveness \". we often read about products being rated for intuitiveness - - how well a ...\n",
      "\n",
      "[STAC]\n",
      "  Length: 512 tokens | EDUs: 74\n",
      "  Text: ello just got a connection reset hm, should n ' t happen ( and has n ' t before ). let me know if you have problems. got a socketexception error appea...\n",
      "\n",
      "[STS]\n",
      "  Length: 512 tokens | EDUs: 29\n",
      "  Text: relativism... and the enlightenment : response to marks et al. ' relativism ' is a term that has outlived its usefulness in science studies, and now d...\n",
      "\n",
      "[UMUC]\n",
      "  Length: 506 tokens | EDUs: 36\n",
      "  Text: many times, we have put forth in this chamber the essence of the russian position regarding the cause and development of the internal crisis in ukrain...\n",
      "\n",
      "[MSDC]\n",
      "  Length: 479 tokens | EDUs: 42\n",
      "  Text: mission has started. alright, start with a row of 5 orange ones on the ground any direction near the center preferably i was just about to ask place o...\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DISRPT-Segmenter\n",
    "!TOKENIZERS_PARALLELISM=true ./.venv/bin/python -m com.disrpt.segmenter.dataset_prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihA7mXSGd4Fy",
    "outputId": "90240424-de2f-4b11-ef1d-604ac8323fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_with_mlp_classifier.py  fine_tune_with_linear_head.py  __init__.py  utils\n",
      "dataset_prep.py\t\t     fine_tune_with_mlp.py\t    __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls com/disrpt/segmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yKN0lN9h63Q",
    "outputId": "970723b0-634d-400f-a336-462f1bc5ffcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"721741755abc53c01670a1316f5fcb1f112ec711\"\n",
    "\n",
    "import wandb\n",
    "wandb.login()   # optional, but nice to verify it's working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SGDR4-CX99oY",
    "outputId": "ae47054e-c534-4ae0-822a-18ce7c26ac3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + MLP\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run 1uannfh9 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run 1uannfh9 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run 1uannfh9 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_220248-1uannfh9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMLPbert-base-uncased_dim_512_256_128_cw0.5_r16_a128_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/1uannfh9\u001b[0m\n",
      "âœ“ W&B initialized: Multiple-Run-edu-segmentation/MLPbert-base-uncased_dim_512_256_128_cw0.5_r16_a128_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 188kB/s]\n",
      "config.json: 100% 570/570 [00:00<00:00, 2.96MB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 32.9MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.05MB/s]\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  âœ“ gum/train.conllu\n",
      "  âœ“ gum/dev.conllu\n",
      "  âœ“ gum/test.conllu\n",
      "  âœ“ scidtb/train.conllu\n",
      "  âœ“ scidtb/dev.conllu\n",
      "  âœ“ scidtb/test.conllu\n",
      "  âœ“ oll/train.conllu\n",
      "  âœ“ oll/dev.conllu\n",
      "  âœ“ oll/test.conllu\n",
      "  âœ“ stac/train.conllu\n",
      "  âœ“ stac/dev.conllu\n",
      "  âœ“ stac/test.conllu\n",
      "  âœ“ sts/train.conllu\n",
      "  âœ“ sts/dev.conllu\n",
      "  âœ“ sts/test.conllu\n",
      "  âœ“ umuc/train.conllu\n",
      "  âœ“ umuc/dev.conllu\n",
      "  âœ“ umuc/test.conllu\n",
      "  âœ“ msdc/train.conllu\n",
      "  âœ“ msdc/dev.conllu\n",
      "  âœ“ msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "âœ“ Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "âœ“ Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "âœ“ Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "âœ“ Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "âœ“ Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "âœ“ Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "âœ“ Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "âœ“ Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "âœ“ Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "âœ“ Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "âœ“ Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "âœ“ Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "âœ“ Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "âœ“ Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "âœ“ Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "âœ“ Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "âœ“ Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "âœ“ Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "âœ“ Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA + MLP Classifier\n",
      "======================================================================\n",
      "model.safetensors: 100% 440M/440M [00:01<00:00, 272MB/s]\n",
      "\n",
      "ğŸ—ï¸  MLP Classifier Architecture:\n",
      "   Input:  768 (BERT hidden size)\n",
      "   Layer 1: Linear(128 â†’ 512) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 2: Linear(512 â†’ 256) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 3: Linear(256 â†’ 128) â†’ GELU â†’ Dropout(0.3)\n",
      "   Output: Linear(128 â†’ 2)\n",
      "\n",
      "âœ“ LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 128\n",
      "  Effective LR scale:    8.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       classifier\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.FEATURE_EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "trainable params: 1,250,946 || all params: 111,188,484 || trainable%: 1.1251\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "W&B logging:              Enabled\n",
      "======================================================================\n",
      "\n",
      "âš–ï¸ Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "âš–ï¸ Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "TRAINING STARTED\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "\n",
      "{'loss': 0.7056, 'grad_norm': 0.18162187933921814, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "{'loss': 0.7129, 'grad_norm': 0.33454445004463196, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.13}\n",
      "{'loss': 0.7037, 'grad_norm': 0.26794975996017456, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6657, 'grad_norm': 0.3337583839893341, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.38}\n",
      "{'loss': 0.6147, 'grad_norm': 0.3179066479206085, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.5194, 'grad_norm': 0.6280010938644409, 'learning_rate': 0.000147, 'epoch': 0.64}\n",
      "{'loss': 0.3375, 'grad_norm': 2.8919553756713867, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      "{'loss': 0.253, 'grad_norm': 1.3687137365341187, 'learning_rate': 0.00020699999999999996, 'epoch': 0.9}\n",
      " 10% 77/780 [00:06<00:47, 14.71it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.1674056053161621, 'eval_accuracy': 0.939023958247234, 'eval_precision': 0.666060606060606, 'eval_recall': 0.9208211143695014, 'eval_f1': 0.7729910321786531, 'eval_precision_class_0': 0.989424946495265, 'eval_precision_class_1': 0.666060606060606, 'eval_recall_class_0': 0.9413369531946607, 'eval_recall_class_1': 0.9208211143695014, 'eval_f1_class_0': 0.9647821046170634, 'eval_f1_class_1': 0.7729910321786531, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5872, 'eval_samples_per_second': 321.878, 'eval_steps_per_second': 10.218, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:47, 14.71it/s]\n",
      "100% 6/6 [00:00<00:00, 37.13it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1674\n",
      "  eval/accuracy             0.9390\n",
      "  eval/precision            0.6661\n",
      "  eval/recall               0.9208\n",
      "  eval/f1                   0.7730\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9648\n",
      "    eval/f1_class_1         0.7730\n",
      "    eval/precision_class_0  0.9894\n",
      "    eval/precision_class_1  0.6661\n",
      "    eval/recall_class_0     0.9413\n",
      "    eval/recall_class_1     0.9208\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1855, 'grad_norm': 1.440504550933838, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1717, 'grad_norm': 1.0628700256347656, 'learning_rate': 0.000267, 'epoch': 1.15}\n",
      "{'loss': 0.1608, 'grad_norm': 0.6022511720657349, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1414, 'grad_norm': 0.25013384222984314, 'learning_rate': 0.0002960294117647059, 'epoch': 1.41}\n",
      "{'loss': 0.1451, 'grad_norm': 0.6507297158241272, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1398, 'grad_norm': 0.3579403758049011, 'learning_rate': 0.00028720588235294116, 'epoch': 1.67}\n",
      "{'loss': 0.1277, 'grad_norm': 0.5312455892562866, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      "{'loss': 0.1247, 'grad_norm': 0.3311057686805725, 'learning_rate': 0.0002783823529411765, 'epoch': 1.92}\n",
      " 20% 155/780 [00:12<00:41, 14.95it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11726266145706177, 'eval_accuracy': 0.9633954822940406, 'eval_precision': 0.7857649352951604, 'eval_recall': 0.9284666945957268, 'eval_f1': 0.8511761881901104, 'eval_precision_class_0': 0.9906957102183716, 'eval_precision_class_1': 0.7857649352951604, 'eval_recall_class_0': 0.9678338057784698, 'eval_recall_class_1': 0.9284666945957268, 'eval_f1_class_0': 0.9791313245550259, 'eval_f1_class_1': 0.8511761881901104, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.577, 'eval_samples_per_second': 327.57, 'eval_steps_per_second': 10.399, 'epoch': 2.0}\n",
      " 20% 156/780 [00:12<00:41, 14.95it/s]\n",
      "100% 6/6 [00:00<00:00, 38.34it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1173\n",
      "  eval/accuracy             0.9634\n",
      "  eval/precision            0.7858\n",
      "  eval/recall               0.9285\n",
      "  eval/f1                   0.8512\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9791\n",
      "    eval/f1_class_1         0.8512\n",
      "    eval/precision_class_0  0.9907\n",
      "    eval/precision_class_1  0.7858\n",
      "    eval/recall_class_0     0.9678\n",
      "    eval/recall_class_1     0.9285\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1177, 'grad_norm': 0.30712294578552246, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1186, 'grad_norm': 0.5797178745269775, 'learning_rate': 0.00026955882352941175, 'epoch': 2.18}\n",
      "{'loss': 0.1174, 'grad_norm': 0.20899775624275208, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1107, 'grad_norm': 0.33436083793640137, 'learning_rate': 0.000260735294117647, 'epoch': 2.44}\n",
      "{'loss': 0.1203, 'grad_norm': 0.3028441071510315, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1182, 'grad_norm': 0.9300163388252258, 'learning_rate': 0.00025191176470588234, 'epoch': 2.69}\n",
      "{'loss': 0.1058, 'grad_norm': 0.5287975668907166, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      "{'loss': 0.0989, 'grad_norm': 0.24815496802330017, 'learning_rate': 0.00024308823529411763, 'epoch': 2.95}\n",
      " 30% 233/780 [00:18<00:37, 14.61it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09745790809392929, 'eval_accuracy': 0.9683666119566886, 'eval_precision': 0.8070081344417628, 'eval_recall': 0.9455383326351068, 'eval_f1': 0.8707981673498915, 'eval_precision_class_0': 0.9929253625751681, 'eval_precision_class_1': 0.8070081344417628, 'eval_recall_class_0': 0.9712673507139911, 'eval_recall_class_1': 0.9455383326351068, 'eval_f1_class_0': 0.9819769514877928, 'eval_f1_class_1': 0.8707981673498915, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5753, 'eval_samples_per_second': 328.537, 'eval_steps_per_second': 10.43, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.61it/s]\n",
      "100% 6/6 [00:00<00:00, 38.24it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0975\n",
      "  eval/accuracy             0.9684\n",
      "  eval/precision            0.8070\n",
      "  eval/recall               0.9455\n",
      "  eval/f1                   0.8708\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9820\n",
      "    eval/f1_class_1         0.8708\n",
      "    eval/precision_class_0  0.9929\n",
      "    eval/precision_class_1  0.8070\n",
      "    eval/recall_class_0     0.9713\n",
      "    eval/recall_class_1     0.9455\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0999, 'grad_norm': 0.26974937319755554, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1004, 'grad_norm': 0.21916426718235016, 'learning_rate': 0.00023426470588235293, 'epoch': 3.21}\n",
      "{'loss': 0.1001, 'grad_norm': 0.23543424904346466, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.0978, 'grad_norm': 0.20717351138591766, 'learning_rate': 0.00022544117647058822, 'epoch': 3.46}\n",
      "{'loss': 0.0876, 'grad_norm': 0.2878369092941284, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1009, 'grad_norm': 0.2560633420944214, 'learning_rate': 0.00021661764705882352, 'epoch': 3.72}\n",
      "{'loss': 0.0964, 'grad_norm': 0.27293336391448975, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      "{'loss': 0.0944, 'grad_norm': 0.3513500988483429, 'learning_rate': 0.0002077941176470588, 'epoch': 3.97}\n",
      " 40% 311/780 [00:24<00:31, 14.68it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.0886615589261055, 'eval_accuracy': 0.969830792664927, 'eval_precision': 0.8115477145148356, 'eval_recall': 0.9539170506912442, 'eval_f1': 0.8769919599441529, 'eval_precision_class_0': 0.9940108349440557, 'eval_precision_class_1': 0.8115477145148356, 'eval_recall_class_0': 0.9718529165169482, 'eval_recall_class_1': 0.9539170506912442, 'eval_f1_class_0': 0.9828070010161029, 'eval_f1_class_1': 0.8769919599441529, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.594, 'eval_samples_per_second': 318.2, 'eval_steps_per_second': 10.102, 'epoch': 4.0}\n",
      " 40% 312/780 [00:25<00:31, 14.68it/s]\n",
      "100% 6/6 [00:00<00:00, 38.29it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0887\n",
      "  eval/accuracy             0.9698\n",
      "  eval/precision            0.8115\n",
      "  eval/recall               0.9539\n",
      "  eval/f1                   0.8770\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9828\n",
      "    eval/f1_class_1         0.8770\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.8115\n",
      "    eval/recall_class_0     0.9719\n",
      "    eval/recall_class_1     0.9539\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0882, 'grad_norm': 0.9203989505767822, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.0905, 'grad_norm': 0.24473974108695984, 'learning_rate': 0.00019897058823529408, 'epoch': 4.23}\n",
      "{'loss': 0.0884, 'grad_norm': 0.2745179533958435, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.0851, 'grad_norm': 0.30936533212661743, 'learning_rate': 0.00019014705882352937, 'epoch': 4.49}\n",
      "{'loss': 0.0864, 'grad_norm': 0.21493659913539886, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.0909, 'grad_norm': 0.23669663071632385, 'learning_rate': 0.00018132352941176467, 'epoch': 4.74}\n",
      "{'loss': 0.0877, 'grad_norm': 0.3291669487953186, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      "{'loss': 0.0876, 'grad_norm': 0.30427607893943787, 'learning_rate': 0.00017249999999999996, 'epoch': 5.0}\n",
      " 50% 390/780 [00:31<00:26, 14.66it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08683592081069946, 'eval_accuracy': 0.971023391467605, 'eval_precision': 0.8173765211166786, 'eval_recall': 0.9567448680351907, 'eval_f1': 0.8815865662999421, 'eval_precision_class_0': 0.9943819460503585, 'eval_precision_class_1': 0.8173765211166786, 'eval_recall_class_0': 0.9728377317310124, 'eval_recall_class_1': 0.9567448680351907, 'eval_f1_class_0': 0.98349186702006, 'eval_f1_class_1': 0.8815865662999421, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6023, 'eval_samples_per_second': 313.807, 'eval_steps_per_second': 9.962, 'epoch': 5.0}\n",
      " 50% 390/780 [00:31<00:26, 14.66it/s]\n",
      "100% 6/6 [00:00<00:00, 38.28it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0868\n",
      "  eval/accuracy             0.9710\n",
      "  eval/precision            0.8174\n",
      "  eval/recall               0.9567\n",
      "  eval/f1                   0.8816\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9835\n",
      "    eval/f1_class_1         0.8816\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.8174\n",
      "    eval/recall_class_0     0.9728\n",
      "    eval/recall_class_1     0.9567\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0911, 'grad_norm': 0.22755366563796997, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0855, 'grad_norm': 0.36476898193359375, 'learning_rate': 0.00016367647058823526, 'epoch': 5.26}\n",
      "{'loss': 0.0817, 'grad_norm': 0.22704853117465973, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.0834, 'grad_norm': 0.3812757730484009, 'learning_rate': 0.00015485294117647058, 'epoch': 5.51}\n",
      "{'loss': 0.0823, 'grad_norm': 0.23949536681175232, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0807, 'grad_norm': 0.3380051255226135, 'learning_rate': 0.00014602941176470588, 'epoch': 5.77}\n",
      "{'loss': 0.0806, 'grad_norm': 0.2130783498287201, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:37<00:21, 14.67it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08502405136823654, 'eval_accuracy': 0.9705038434743591, 'eval_precision': 0.8118365180467091, 'eval_recall': 0.9611436950146628, 'eval_f1': 0.8802033378093228, 'eval_precision_class_0': 0.994944470940928, 'eval_precision_class_1': 0.8118365180467091, 'eval_recall_class_0': 0.9716932167525053, 'eval_recall_class_1': 0.9611436950146628, 'eval_f1_class_0': 0.9831813958498848, 'eval_f1_class_1': 0.8802033378093228, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5822, 'eval_samples_per_second': 324.625, 'eval_steps_per_second': 10.306, 'epoch': 6.0}\n",
      " 60% 468/780 [00:37<00:21, 14.67it/s]\n",
      "100% 6/6 [00:00<00:00, 38.31it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0850\n",
      "  eval/accuracy             0.9705\n",
      "  eval/precision            0.8118\n",
      "  eval/recall               0.9611\n",
      "  eval/f1                   0.8802\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9832\n",
      "    eval/f1_class_1         0.8802\n",
      "    eval/precision_class_0  0.9949\n",
      "    eval/precision_class_1  0.8118\n",
      "    eval/recall_class_0     0.9717\n",
      "    eval/recall_class_1     0.9611\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0735, 'grad_norm': 0.25655439496040344, 'learning_rate': 0.00013720588235294117, 'epoch': 6.03}\n",
      "{'loss': 0.0829, 'grad_norm': 0.24450261890888214, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.074, 'grad_norm': 0.2765529751777649, 'learning_rate': 0.00012838235294117647, 'epoch': 6.28}\n",
      "{'loss': 0.0788, 'grad_norm': 0.31611722707748413, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0799, 'grad_norm': 0.22712044417858124, 'learning_rate': 0.00011955882352941176, 'epoch': 6.54}\n",
      "{'loss': 0.079, 'grad_norm': 0.2622566521167755, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0761, 'grad_norm': 0.2872715890407562, 'learning_rate': 0.00011073529411764706, 'epoch': 6.79}\n",
      "{'loss': 0.0771, 'grad_norm': 0.20061220228672028, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:43<00:15, 14.69it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08531701564788818, 'eval_accuracy': 0.9716255948234127, 'eval_precision': 0.8179795282599021, 'eval_recall': 0.9625052366987851, 'eval_f1': 0.8843766539960545, 'eval_precision_class_0': 0.9951262014321889, 'eval_precision_class_1': 0.8179795282599021, 'eval_recall_class_0': 0.9727844984761981, 'eval_recall_class_1': 0.9625052366987851, 'eval_f1_class_0': 0.9838285272048185, 'eval_f1_class_1': 0.8843766539960545, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5939, 'eval_samples_per_second': 318.257, 'eval_steps_per_second': 10.103, 'epoch': 7.0}\n",
      " 70% 546/780 [00:44<00:15, 14.69it/s]\n",
      "100% 6/6 [00:00<00:00, 38.27it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0853\n",
      "  eval/accuracy             0.9716\n",
      "  eval/precision            0.8180\n",
      "  eval/recall               0.9625\n",
      "  eval/f1                   0.8844\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9838\n",
      "    eval/f1_class_1         0.8844\n",
      "    eval/precision_class_0  0.9951\n",
      "    eval/precision_class_1  0.8180\n",
      "    eval/recall_class_0     0.9728\n",
      "    eval/recall_class_1     0.9625\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0747, 'grad_norm': 0.270598828792572, 'learning_rate': 0.00010191176470588235, 'epoch': 7.05}\n",
      "{'loss': 0.0833, 'grad_norm': 0.3112872838973999, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0832, 'grad_norm': 0.2846396863460541, 'learning_rate': 9.308823529411765e-05, 'epoch': 7.31}\n",
      "{'loss': 0.0678, 'grad_norm': 0.21426382660865784, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0742, 'grad_norm': 0.2051481008529663, 'learning_rate': 8.426470588235294e-05, 'epoch': 7.56}\n",
      "{'loss': 0.0749, 'grad_norm': 0.2651653587818146, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0744, 'grad_norm': 0.29250913858413696, 'learning_rate': 7.544117647058822e-05, 'epoch': 7.82}\n",
      "{'loss': 0.0744, 'grad_norm': 0.2960277795791626, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:49<00:10, 14.83it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08258891850709915, 'eval_accuracy': 0.9741052557002681, 'eval_precision': 0.8372306281522237, 'eval_recall': 0.956221198156682, 'eval_f1': 0.8927785654916149, 'eval_precision_class_0': 0.9943348151360729, 'eval_precision_class_1': 0.8372306281522237, 'eval_recall_class_0': 0.9763777431761621, 'eval_recall_class_1': 0.956221198156682, 'eval_f1_class_0': 0.9852744670135974, 'eval_f1_class_1': 0.8927785654916149, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5852, 'eval_samples_per_second': 322.97, 'eval_steps_per_second': 10.253, 'epoch': 8.0}\n",
      " 80% 624/780 [00:50<00:10, 14.83it/s]\n",
      "100% 6/6 [00:00<00:00, 38.46it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0826\n",
      "  eval/accuracy             0.9741\n",
      "  eval/precision            0.8372\n",
      "  eval/recall               0.9562\n",
      "  eval/f1                   0.8928\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9853\n",
      "    eval/f1_class_1         0.8928\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.8372\n",
      "    eval/recall_class_0     0.9764\n",
      "    eval/recall_class_1     0.9562\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0635, 'grad_norm': 0.2787621021270752, 'learning_rate': 6.661764705882352e-05, 'epoch': 8.08}\n",
      "{'loss': 0.0754, 'grad_norm': 0.3545718789100647, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0721, 'grad_norm': 0.3107244074344635, 'learning_rate': 5.779411764705882e-05, 'epoch': 8.33}\n",
      "{'loss': 0.0704, 'grad_norm': 0.22668646275997162, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0712, 'grad_norm': 0.21056132018566132, 'learning_rate': 4.897058823529411e-05, 'epoch': 8.59}\n",
      "{'loss': 0.0796, 'grad_norm': 0.22522366046905518, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0707, 'grad_norm': 0.3132506012916565, 'learning_rate': 4.014705882352941e-05, 'epoch': 8.85}\n",
      "{'loss': 0.0762, 'grad_norm': 0.20647314190864563, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:55<00:05, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.0811060443520546, 'eval_accuracy': 0.9739045212483322, 'eval_precision': 0.8343966460080204, 'eval_recall': 0.958839547549225, 'eval_f1': 0.8923001949317739, 'eval_precision_class_0': 0.9946688009550035, 'eval_precision_class_1': 0.8343966460080204, 'eval_recall_class_0': 0.9758187940006122, 'eval_recall_class_1': 0.958839547549225, 'eval_f1_class_0': 0.9851536363514222, 'eval_f1_class_1': 0.8923001949317739, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5901, 'eval_samples_per_second': 320.306, 'eval_steps_per_second': 10.168, 'epoch': 9.0}\n",
      " 90% 702/780 [00:56<00:05, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.21it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0811\n",
      "  eval/accuracy             0.9739\n",
      "  eval/precision            0.8344\n",
      "  eval/recall               0.9588\n",
      "  eval/f1                   0.8923\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9852\n",
      "    eval/f1_class_1         0.8923\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8344\n",
      "    eval/recall_class_0     0.9758\n",
      "    eval/recall_class_1     0.9588\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0715, 'grad_norm': 0.21462160348892212, 'learning_rate': 3.13235294117647e-05, 'epoch': 9.1}\n",
      "{'loss': 0.0683, 'grad_norm': 0.2544584572315216, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0685, 'grad_norm': 0.22126422822475433, 'learning_rate': 2.2499999999999998e-05, 'epoch': 9.36}\n",
      "{'loss': 0.0636, 'grad_norm': 0.3647344410419464, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0737, 'grad_norm': 0.2536460757255554, 'learning_rate': 1.3676470588235293e-05, 'epoch': 9.62}\n",
      "{'loss': 0.0707, 'grad_norm': 0.2403654009103775, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0735, 'grad_norm': 0.3521984815597534, 'learning_rate': 4.852941176470589e-06, 'epoch': 9.87}\n",
      "{'loss': 0.0749, 'grad_norm': 0.2720091640949249, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:02<00:00, 14.84it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08214913308620453, 'eval_accuracy': 0.9742705664253918, 'eval_precision': 0.8376867381541564, 'eval_recall': 0.9572685379136993, 'eval_f1': 0.8934943056845398, 'eval_precision_class_0': 0.9944698961750115, 'eval_precision_class_1': 0.8376867381541564, 'eval_recall_class_0': 0.9764309764309764, 'eval_recall_class_1': 0.9572685379136993, 'eval_f1_class_0': 0.9853678845546908, 'eval_f1_class_1': 0.8934943056845398, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5873, 'eval_samples_per_second': 321.8, 'eval_steps_per_second': 10.216, 'epoch': 10.0}\n",
      "100% 780/780 [01:02<00:00, 14.84it/s]\n",
      "100% 6/6 [00:00<00:00, 38.06it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0821\n",
      "  eval/accuracy             0.9743\n",
      "  eval/precision            0.8377\n",
      "  eval/recall               0.9573\n",
      "  eval/f1                   0.8935\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9854\n",
      "    eval/f1_class_1         0.8935\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.8377\n",
      "    eval/recall_class_0     0.9764\n",
      "    eval/recall_class_1     0.9573\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 62.9913, 'train_samples_per_second': 197.964, 'train_steps_per_second': 12.383, 'train_loss': 0.132697083399846, 'epoch': 10.0}\n",
      "100% 780/780 [01:02<00:00, 12.38it/s]\n",
      "\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "TRAINING COMPLETED\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 62.9913\n",
      "train_samples_per_second.......................... 197.9640\n",
      "train_steps_per_second............................ 12.3830\n",
      "total_flos........................................ 0.0000\n",
      "train_loss........................................ 0.1327\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.29it/s]\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0821\n",
      "  eval/accuracy             0.9743\n",
      "  eval/precision            0.8377\n",
      "  eval/recall               0.9573\n",
      "  eval/f1                   0.8935\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9854\n",
      "    eval/f1_class_1         0.8935\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.8377\n",
      "    eval/recall_class_0     0.9764\n",
      "    eval/recall_class_1     0.9573\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.53it/s]\n",
      "eval_loss......................................... 0.0821\n",
      "eval_accuracy..................................... 0.9743\n",
      "eval_precision.................................... 0.8377\n",
      "eval_recall....................................... 0.9573\n",
      "eval_f1........................................... 0.8935\n",
      "eval_precision_class_0............................ 0.9945\n",
      "eval_precision_class_1............................ 0.8377\n",
      "eval_recall_class_0............................... 0.9764\n",
      "eval_recall_class_1............................... 0.9573\n",
      "eval_f1_class_0................................... 0.9854\n",
      "eval_f1_class_1................................... 0.8935\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.5889\n",
      "eval_samples_per_second........................... 320.9150\n",
      "eval_steps_per_second............................. 10.1880\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model)... Done. 0.0s\n",
      "âœ“ Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "\n",
      "ğŸ—ï¸  MLP Classifier Architecture:\n",
      "   Input:  768 (BERT hidden size)\n",
      "   Layer 1: Linear(128 â†’ 512) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 2: Linear(512 â†’ 256) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 3: Linear(256 â†’ 128) â†’ GELU â†’ Dropout(0.3)\n",
      "   Output: Linear(128 â†’ 2)\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.09it/s]\n",
      "\n",
      "ğŸ“Š TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0614\n",
      "eval_model_preparation_time....................... 0.0047\n",
      "eval_accuracy..................................... 0.9783\n",
      "eval_precision.................................... 0.8640\n",
      "eval_recall....................................... 0.9544\n",
      "eval_f1........................................... 0.9070\n",
      "eval_precision_class_0............................ 0.9942\n",
      "eval_precision_class_1............................ 0.8640\n",
      "eval_recall_class_0............................... 0.9813\n",
      "eval_recall_class_1............................... 0.9544\n",
      "eval_f1_class_0................................... 0.9877\n",
      "eval_f1_class_1................................... 0.9070\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8089\n",
      "eval_samples_per_second........................... 302.8690\n",
      "eval_steps_per_second............................. 19.7790\n",
      "100% 16/16 [00:00<00:00, 25.20it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9942    0.9813    0.9877     99651\n",
      "   EDU Start (1)     0.8640    0.9544    0.9070     12438\n",
      "\n",
      "        accuracy                         0.9783    112089\n",
      "       macro avg     0.9291    0.9678    0.9473    112089\n",
      "    weighted avg     0.9798    0.9783    0.9787    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "ğŸ“Š FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9743               0.9783              \n",
      "Precision                      0.8377               0.8640              \n",
      "Recall                         0.9573               0.9544              \n",
      "F1                             0.8935               0.9070              \n",
      "======================================================================\n",
      "\n",
      "âœ… Model saved at: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "âœ… Logs saved at: ./MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-1uannfh9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-1uannfh9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-1uannfh9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-1uannfh9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-1uannfh9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-1uannfh9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-1uannfh9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-1uannfh9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 38.2KB/38.2KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 38.2KB/38.2KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading output.log 38.2KB/38.2KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading output.log 38.2KB/38.2KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading output.log 38.2KB/38.2KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading output.log 38.2KB/38.2KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading config.yaml 13.3KB/13.3KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading output.log 38.2KB/38.2KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading config.yaml 13.3KB/13.3KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading output.log 38.2KB/38.2KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading config.yaml 13.3KB/13.3KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 38.2KB/38.2KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading config.yaml 13.3KB/13.3KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-1uannfh9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading media/table/final_results_table_196_54a8f27a70a69cde3a2b.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 38.2KB/38.2KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading config.yaml 13.3KB/13.3KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-1uannfh9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 193-196, summary, console lines 699-779 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss â–ˆâ–ˆâ–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 â–â–â–ƒâ–ƒâ–…â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall â–â–â–‚â–‚â–…â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–‡â–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.97427\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.89349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98537\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.89349\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.08215\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.83769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99447\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.83769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95727\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mMLPbert-base-uncased_dim_512_256_128_cw0.5_r16_a128_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/1uannfh9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_220248-1uannfh9/logs\u001b[0m\n",
      "âœ… W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_mlp \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp6_Final_Models  \\\n",
    "  --wandb_run_name \"MLPbert-base-uncased_dim_512_256_128_cw0.5_r16_a128_do0.1_lr0.0003_ep10\" \\\n",
    "  --class_1_weight_multiplier 0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --mlp_dims 512 256 128 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2ELsxS-r7szB",
    "outputId": "50251a94-1176-490f-9e65-7f24078261ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + MLP\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run nc235y00 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run nc235y00 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_220550-nc235y00\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mMLPdistilbert-base-uncased_dim_512_256_128_cw0.5_r32_a128_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/nc235y00\u001b[0m\n",
      "âœ“ W&B initialized: Multiple-Run-edu-segmentation/MLPdistilbert-base-uncased_dim_512_256_128_cw0.5_r32_a128_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  âœ“ gum/train.conllu\n",
      "  âœ“ gum/dev.conllu\n",
      "  âœ“ gum/test.conllu\n",
      "  âœ“ scidtb/train.conllu\n",
      "  âœ“ scidtb/dev.conllu\n",
      "  âœ“ scidtb/test.conllu\n",
      "  âœ“ oll/train.conllu\n",
      "  âœ“ oll/dev.conllu\n",
      "  âœ“ oll/test.conllu\n",
      "  âœ“ stac/train.conllu\n",
      "  âœ“ stac/dev.conllu\n",
      "  âœ“ stac/test.conllu\n",
      "  âœ“ sts/train.conllu\n",
      "  âœ“ sts/dev.conllu\n",
      "  âœ“ sts/test.conllu\n",
      "  âœ“ umuc/train.conllu\n",
      "  âœ“ umuc/dev.conllu\n",
      "  âœ“ umuc/test.conllu\n",
      "  âœ“ msdc/train.conllu\n",
      "  âœ“ msdc/dev.conllu\n",
      "  âœ“ msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "âœ“ Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "âœ“ Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "âœ“ Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "âœ“ Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "âœ“ Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "âœ“ Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "âœ“ Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "âœ“ Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "âœ“ Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "âœ“ Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "âœ“ Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "âœ“ Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "âœ“ Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "âœ“ Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "âœ“ Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "âœ“ Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "âœ“ Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "âœ“ Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "âœ“ Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing distilbert-base-uncased with LoRA + MLP Classifier\n",
      "======================================================================\n",
      "model.safetensors: 100% 268M/268M [00:01<00:00, 143MB/s]\n",
      "\n",
      "ğŸ—ï¸  MLP Classifier Architecture:\n",
      "   Input:  768 (BERT hidden size)\n",
      "   Layer 1: Linear(128 â†’ 512) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 2: Linear(512 â†’ 256) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 3: Linear(256 â†’ 128) â†’ GELU â†’ Dropout(0.3)\n",
      "   Output: Linear(128 â†’ 2)\n",
      "\n",
      "âœ“ LoRA target validation passed\n",
      "  Target modules: ['q_lin', 'v_lin']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 128\n",
      "  Effective LR scale:    4.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        q_lin, v_lin\n",
      "  Modules to save:       classifier\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.FEATURE_EXTRACTION\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "trainable params: 1,199,490 || all params: 68,069,124 || trainable%: 1.7622\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    distilbert-base-uncased\n",
      "Output directory:         ./MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "W&B logging:              Enabled\n",
      "======================================================================\n",
      "\n",
      "âš–ï¸ Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "âš–ï¸ Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "TRAINING STARTED\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "\n",
      "{'loss': 0.7073, 'grad_norm': 0.18544071912765503, 'learning_rate': 0.0, 'epoch': 0.01}\n",
      "{'loss': 0.7162, 'grad_norm': 0.3367721140384674, 'learning_rate': 2.6999999999999996e-05, 'epoch': 0.13}\n",
      "{'loss': 0.7073, 'grad_norm': 0.27233922481536865, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6648, 'grad_norm': 0.24566882848739624, 'learning_rate': 8.699999999999999e-05, 'epoch': 0.38}\n",
      "{'loss': 0.621, 'grad_norm': 0.15964937210083008, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.5682, 'grad_norm': 0.4928062856197357, 'learning_rate': 0.000147, 'epoch': 0.64}\n",
      "{'loss': 0.3965, 'grad_norm': 1.4807881116867065, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      "{'loss': 0.2721, 'grad_norm': 0.7459000945091248, 'learning_rate': 0.00020699999999999996, 'epoch': 0.9}\n",
      " 10% 76/780 [00:03<00:24, 28.24it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.18117903172969818, 'eval_accuracy': 0.9298492130028693, 'eval_precision': 0.629608336327704, 'eval_recall': 0.9175743611227483, 'eval_f1': 0.7467928227421898, 'eval_precision_class_0': 0.9888800972108401, 'eval_precision_class_1': 0.629608336327704, 'eval_recall_class_0': 0.931408951171797, 'eval_recall_class_1': 0.9175743611227483, 'eval_f1_class_0': 0.9592845149573381, 'eval_f1_class_1': 0.7467928227421898, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4964, 'eval_samples_per_second': 380.755, 'eval_steps_per_second': 12.087, 'epoch': 1.0}\n",
      " 10% 78/780 [00:04<00:24, 28.24it/s]\n",
      "100% 6/6 [00:00<00:00, 45.66it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1812\n",
      "  eval/accuracy             0.9298\n",
      "  eval/precision            0.6296\n",
      "  eval/recall               0.9176\n",
      "  eval/f1                   0.7468\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9593\n",
      "    eval/f1_class_1         0.7468\n",
      "    eval/precision_class_0  0.9889\n",
      "    eval/precision_class_1  0.6296\n",
      "    eval/recall_class_0     0.9314\n",
      "    eval/recall_class_1     0.9176\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2037, 'grad_norm': 0.6280575394630432, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1837, 'grad_norm': 1.010608196258545, 'learning_rate': 0.000267, 'epoch': 1.15}\n",
      "{'loss': 0.1774, 'grad_norm': 0.6319028735160828, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1517, 'grad_norm': 0.7852182984352112, 'learning_rate': 0.0002960294117647059, 'epoch': 1.41}\n",
      "{'loss': 0.157, 'grad_norm': 0.5506337285041809, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1501, 'grad_norm': 0.24324434995651245, 'learning_rate': 0.00028720588235294116, 'epoch': 1.67}\n",
      "{'loss': 0.1427, 'grad_norm': 0.7391499280929565, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      "{'loss': 0.1338, 'grad_norm': 0.2758180797100067, 'learning_rate': 0.0002783823529411765, 'epoch': 1.92}\n",
      " 20% 154/780 [00:07<00:22, 27.66it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.12326565384864807, 'eval_accuracy': 0.9616242959534296, 'eval_precision': 0.7811607142857143, 'eval_recall': 0.9163175534143276, 'eval_f1': 0.843358395989975, 'eval_precision_class_0': 0.9891276245424485, 'eval_precision_class_1': 0.7811607142857143, 'eval_recall_class_0': 0.9673813231125484, 'eval_recall_class_1': 0.9163175534143276, 'eval_f1_class_0': 0.9781336203996501, 'eval_f1_class_1': 0.843358395989975, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4856, 'eval_samples_per_second': 389.21, 'eval_steps_per_second': 12.356, 'epoch': 2.0}\n",
      " 20% 156/780 [00:07<00:22, 27.66it/s]\n",
      "100% 6/6 [00:00<00:00, 46.69it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1233\n",
      "  eval/accuracy             0.9616\n",
      "  eval/precision            0.7812\n",
      "  eval/recall               0.9163\n",
      "  eval/f1                   0.8434\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9781\n",
      "    eval/f1_class_1         0.8434\n",
      "    eval/precision_class_0  0.9891\n",
      "    eval/precision_class_1  0.7812\n",
      "    eval/recall_class_0     0.9674\n",
      "    eval/recall_class_1     0.9163\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1311, 'grad_norm': 0.28505972027778625, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.13, 'grad_norm': 0.5351685285568237, 'learning_rate': 0.00026955882352941175, 'epoch': 2.18}\n",
      "{'loss': 0.1283, 'grad_norm': 0.27010124921798706, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1241, 'grad_norm': 0.3367713689804077, 'learning_rate': 0.000260735294117647, 'epoch': 2.44}\n",
      "{'loss': 0.1347, 'grad_norm': 0.1610964834690094, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1288, 'grad_norm': 0.9758322834968567, 'learning_rate': 0.00025191176470588234, 'epoch': 2.69}\n",
      "{'loss': 0.1163, 'grad_norm': 0.6339319348335266, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      "{'loss': 0.1122, 'grad_norm': 0.2910519242286682, 'learning_rate': 0.00024308823529411763, 'epoch': 2.95}\n",
      " 30% 232/780 [00:10<00:20, 27.17it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.1071217954158783, 'eval_accuracy': 0.9642338438285964, 'eval_precision': 0.7847470953088146, 'eval_recall': 0.9408253037285296, 'eval_f1': 0.8557275541795666, 'eval_precision_class_0': 0.9922858469184348, 'eval_precision_class_1': 0.7847470953088146, 'eval_recall_class_0': 0.967208315034402, 'eval_recall_class_1': 0.9408253037285296, 'eval_f1_class_0': 0.9795866103259807, 'eval_f1_class_1': 0.8557275541795666, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4976, 'eval_samples_per_second': 379.857, 'eval_steps_per_second': 12.059, 'epoch': 3.0}\n",
      " 30% 234/780 [00:11<00:20, 27.17it/s]\n",
      "100% 6/6 [00:00<00:00, 46.66it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1071\n",
      "  eval/accuracy             0.9642\n",
      "  eval/precision            0.7847\n",
      "  eval/recall               0.9408\n",
      "  eval/f1                   0.8557\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9796\n",
      "    eval/f1_class_1         0.8557\n",
      "    eval/precision_class_0  0.9923\n",
      "    eval/precision_class_1  0.7847\n",
      "    eval/recall_class_0     0.9672\n",
      "    eval/recall_class_1     0.9408\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1149, 'grad_norm': 0.24117229878902435, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1131, 'grad_norm': 0.2581051290035248, 'learning_rate': 0.00023426470588235293, 'epoch': 3.21}\n",
      "{'loss': 0.1102, 'grad_norm': 0.2625510096549988, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1103, 'grad_norm': 0.15577556192874908, 'learning_rate': 0.00022544117647058822, 'epoch': 3.46}\n",
      "{'loss': 0.0992, 'grad_norm': 0.15240252017974854, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.113, 'grad_norm': 0.21734380722045898, 'learning_rate': 0.00021661764705882352, 'epoch': 3.72}\n",
      "{'loss': 0.1068, 'grad_norm': 0.21263788640499115, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      "{'loss': 0.1015, 'grad_norm': 0.1741693913936615, 'learning_rate': 0.0002077941176470588, 'epoch': 3.97}\n",
      " 40% 310/780 [00:14<00:16, 27.67it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09743314236402512, 'eval_accuracy': 0.9687090413158733, 'eval_precision': 0.8095494525219888, 'eval_recall': 0.9447004608294931, 'eval_f1': 0.8719188013533108, 'eval_precision_class_0': 0.992820917236597, 'eval_precision_class_1': 0.8095494525219888, 'eval_recall_class_0': 0.9717597583210231, 'eval_recall_class_1': 0.9447004608294931, 'eval_f1_class_0': 0.9821774453890024, 'eval_f1_class_1': 0.8719188013533108, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5006, 'eval_samples_per_second': 377.518, 'eval_steps_per_second': 11.985, 'epoch': 4.0}\n",
      " 40% 312/780 [00:15<00:16, 27.67it/s]\n",
      "100% 6/6 [00:00<00:00, 48.93it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0974\n",
      "  eval/accuracy             0.9687\n",
      "  eval/precision            0.8095\n",
      "  eval/recall               0.9447\n",
      "  eval/f1                   0.8719\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9822\n",
      "    eval/f1_class_1         0.8719\n",
      "    eval/precision_class_0  0.9928\n",
      "    eval/precision_class_1  0.8095\n",
      "    eval/recall_class_0     0.9718\n",
      "    eval/recall_class_1     0.9447\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0975, 'grad_norm': 0.20789673924446106, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1011, 'grad_norm': 0.16209091246128082, 'learning_rate': 0.00019897058823529408, 'epoch': 4.23}\n",
      "{'loss': 0.0958, 'grad_norm': 0.2201201617717743, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.0947, 'grad_norm': 0.3975391387939453, 'learning_rate': 0.00019014705882352937, 'epoch': 4.49}\n",
      "{'loss': 0.0981, 'grad_norm': 0.20570538938045502, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1029, 'grad_norm': 0.2498307228088379, 'learning_rate': 0.00018132352941176467, 'epoch': 4.74}\n",
      "{'loss': 0.0968, 'grad_norm': 0.2209191620349884, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      "{'loss': 0.0963, 'grad_norm': 0.2237202674150467, 'learning_rate': 0.00017249999999999996, 'epoch': 5.0}\n",
      " 50% 390/780 [00:18<00:14, 27.06it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09227994829416275, 'eval_accuracy': 0.9671031657003861, 'eval_precision': 0.7948717948717948, 'eval_recall': 0.9545454545454546, 'eval_f1': 0.8674217188540972, 'eval_precision_class_0': 0.9940729005913442, 'eval_precision_class_1': 0.7948717948717948, 'eval_recall_class_0': 0.9686988461692019, 'eval_recall_class_1': 0.9545454545454546, 'eval_f1_class_0': 0.9812218597503437, 'eval_f1_class_1': 0.8674217188540972, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5058, 'eval_samples_per_second': 373.634, 'eval_steps_per_second': 11.861, 'epoch': 5.0}\n",
      " 50% 390/780 [00:18<00:14, 27.06it/s]\n",
      "100% 6/6 [00:00<00:00, 46.61it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0923\n",
      "  eval/accuracy             0.9671\n",
      "  eval/precision            0.7949\n",
      "  eval/recall               0.9545\n",
      "  eval/f1                   0.8674\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9812\n",
      "    eval/f1_class_1         0.8674\n",
      "    eval/precision_class_0  0.9941\n",
      "    eval/precision_class_1  0.7949\n",
      "    eval/recall_class_0     0.9687\n",
      "    eval/recall_class_1     0.9545\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0979, 'grad_norm': 0.205532044172287, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0949, 'grad_norm': 0.2730824649333954, 'learning_rate': 0.00016367647058823526, 'epoch': 5.26}\n",
      "{'loss': 0.0913, 'grad_norm': 0.2903529405593872, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.0951, 'grad_norm': 0.21687978506088257, 'learning_rate': 0.00015485294117647058, 'epoch': 5.51}\n",
      "{'loss': 0.0913, 'grad_norm': 0.23460794985294342, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0906, 'grad_norm': 0.260582834482193, 'learning_rate': 0.00014602941176470588, 'epoch': 5.77}\n",
      "{'loss': 0.0891, 'grad_norm': 0.2046499401330948, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 466/780 [00:21<00:11, 27.38it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09247070550918579, 'eval_accuracy': 0.9697481373023651, 'eval_precision': 0.81508208551326, 'eval_recall': 0.9463762044407206, 'eval_f1': 0.8758359988368711, 'eval_precision_class_0': 0.9930437618031873, 'eval_precision_class_1': 0.81508208551326, 'eval_recall_class_0': 0.9727179569076803, 'eval_recall_class_1': 0.9463762044407206, 'eval_f1_class_0': 0.9827757758296133, 'eval_f1_class_1': 0.8758359988368711, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.518, 'eval_samples_per_second': 364.864, 'eval_steps_per_second': 11.583, 'epoch': 6.0}\n",
      " 60% 468/780 [00:22<00:11, 27.38it/s]\n",
      "100% 6/6 [00:00<00:00, 46.51it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0925\n",
      "  eval/accuracy             0.9697\n",
      "  eval/precision            0.8151\n",
      "  eval/recall               0.9464\n",
      "  eval/f1                   0.8758\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9828\n",
      "    eval/f1_class_1         0.8758\n",
      "    eval/precision_class_0  0.9930\n",
      "    eval/precision_class_1  0.8151\n",
      "    eval/recall_class_0     0.9727\n",
      "    eval/recall_class_1     0.9464\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.08, 'grad_norm': 0.18755778670310974, 'learning_rate': 0.00013720588235294117, 'epoch': 6.03}\n",
      "{'loss': 0.094, 'grad_norm': 0.16460852324962616, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0828, 'grad_norm': 0.21268971264362335, 'learning_rate': 0.00012838235294117647, 'epoch': 6.28}\n",
      "{'loss': 0.0861, 'grad_norm': 0.1984816938638687, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0892, 'grad_norm': 0.19050846993923187, 'learning_rate': 0.00011955882352941176, 'epoch': 6.54}\n",
      "{'loss': 0.0874, 'grad_norm': 0.22297753393650055, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0868, 'grad_norm': 0.18006952106952667, 'learning_rate': 0.00011073529411764706, 'epoch': 6.79}\n",
      "{'loss': 0.0859, 'grad_norm': 0.22488002479076385, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 544/780 [00:25<00:08, 27.84it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08942320197820663, 'eval_accuracy': 0.9708580807424814, 'eval_precision': 0.8190338860850757, 'eval_recall': 0.9518223711772099, 'eval_f1': 0.8804495252857973, 'eval_precision_class_0': 0.9937494055141114, 'eval_precision_class_1': 0.8190338860850757, 'eval_recall_class_0': 0.9732769060832301, 'eval_recall_class_1': 0.9518223711772099, 'eval_f1_class_0': 0.9834066185270348, 'eval_f1_class_1': 0.8804495252857973, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4926, 'eval_samples_per_second': 383.692, 'eval_steps_per_second': 12.181, 'epoch': 7.0}\n",
      " 70% 546/780 [00:25<00:08, 27.84it/s]\n",
      "100% 6/6 [00:00<00:00, 48.22it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0894\n",
      "  eval/accuracy             0.9709\n",
      "  eval/precision            0.8190\n",
      "  eval/recall               0.9518\n",
      "  eval/f1                   0.8804\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9834\n",
      "    eval/f1_class_1         0.8804\n",
      "    eval/precision_class_0  0.9937\n",
      "    eval/precision_class_1  0.8190\n",
      "    eval/recall_class_0     0.9733\n",
      "    eval/recall_class_1     0.9518\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0845, 'grad_norm': 0.1528027504682541, 'learning_rate': 0.00010191176470588235, 'epoch': 7.05}\n",
      "{'loss': 0.096, 'grad_norm': 0.255889892578125, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0918, 'grad_norm': 0.22221554815769196, 'learning_rate': 9.308823529411765e-05, 'epoch': 7.31}\n",
      "{'loss': 0.0774, 'grad_norm': 0.19824427366256714, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0836, 'grad_norm': 0.19465793669223785, 'learning_rate': 8.426470588235294e-05, 'epoch': 7.56}\n",
      "{'loss': 0.0843, 'grad_norm': 0.16423316299915314, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0811, 'grad_norm': 0.1857495754957199, 'learning_rate': 7.544117647058822e-05, 'epoch': 7.82}\n",
      "{'loss': 0.0839, 'grad_norm': 0.1745167225599289, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 622/780 [00:28<00:05, 27.98it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08888313174247742, 'eval_accuracy': 0.9716728264591623, 'eval_precision': 0.8247478877078223, 'eval_recall': 0.9507750314201927, 'eval_f1': 0.8832887375334468, 'eval_precision_class_0': 0.9936212372085448, 'eval_precision_class_1': 0.8247478877078223, 'eval_recall_class_0': 0.9743282628658123, 'eval_recall_class_1': 0.9507750314201927, 'eval_f1_class_0': 0.9838801798109164, 'eval_f1_class_1': 0.8832887375334468, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4965, 'eval_samples_per_second': 380.692, 'eval_steps_per_second': 12.085, 'epoch': 8.0}\n",
      " 80% 624/780 [00:29<00:05, 27.98it/s]\n",
      "100% 6/6 [00:00<00:00, 47.87it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0889\n",
      "  eval/accuracy             0.9717\n",
      "  eval/precision            0.8247\n",
      "  eval/recall               0.9508\n",
      "  eval/f1                   0.8833\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9839\n",
      "    eval/f1_class_1         0.8833\n",
      "    eval/precision_class_0  0.9936\n",
      "    eval/precision_class_1  0.8247\n",
      "    eval/recall_class_0     0.9743\n",
      "    eval/recall_class_1     0.9508\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0721, 'grad_norm': 0.23008839786052704, 'learning_rate': 6.661764705882352e-05, 'epoch': 8.08}\n",
      "{'loss': 0.0852, 'grad_norm': 0.2844894528388977, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0812, 'grad_norm': 0.35271912813186646, 'learning_rate': 5.779411764705882e-05, 'epoch': 8.33}\n",
      "{'loss': 0.0797, 'grad_norm': 0.2241356372833252, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0835, 'grad_norm': 0.281940758228302, 'learning_rate': 4.897058823529411e-05, 'epoch': 8.59}\n",
      "{'loss': 0.0886, 'grad_norm': 0.1861950010061264, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0774, 'grad_norm': 0.15759381651878357, 'learning_rate': 4.014705882352941e-05, 'epoch': 8.85}\n",
      "{'loss': 0.0857, 'grad_norm': 0.1669786423444748, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 700/780 [00:32<00:02, 26.78it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08769442141056061, 'eval_accuracy': 0.9715429394608509, 'eval_precision': 0.8223446531791907, 'eval_recall': 0.953602848764139, 'eval_f1': 0.8831231813773036, 'eval_precision_class_0': 0.9939823682029966, 'eval_precision_class_1': 0.8223446531791907, 'eval_recall_class_0': 0.9738225469450766, 'eval_recall_class_1': 0.953602848764139, 'eval_f1_class_0': 0.9837991906317644, 'eval_f1_class_1': 0.8831231813773036, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5098, 'eval_samples_per_second': 370.725, 'eval_steps_per_second': 11.769, 'epoch': 9.0}\n",
      " 90% 702/780 [00:33<00:02, 26.78it/s]\n",
      "100% 6/6 [00:00<00:00, 46.69it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0877\n",
      "  eval/accuracy             0.9715\n",
      "  eval/precision            0.8223\n",
      "  eval/recall               0.9536\n",
      "  eval/f1                   0.8831\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9838\n",
      "    eval/f1_class_1         0.8831\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.8223\n",
      "    eval/recall_class_0     0.9738\n",
      "    eval/recall_class_1     0.9536\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0792, 'grad_norm': 0.16512174904346466, 'learning_rate': 3.13235294117647e-05, 'epoch': 9.1}\n",
      "{'loss': 0.0776, 'grad_norm': 0.1831408590078354, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0759, 'grad_norm': 0.1718207746744156, 'learning_rate': 2.2499999999999998e-05, 'epoch': 9.36}\n",
      "{'loss': 0.0708, 'grad_norm': 0.20154379308223724, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0847, 'grad_norm': 0.21239082515239716, 'learning_rate': 1.3676470588235293e-05, 'epoch': 9.62}\n",
      "{'loss': 0.0793, 'grad_norm': 0.15932953357696533, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0844, 'grad_norm': 0.1699361503124237, 'learning_rate': 4.852941176470589e-06, 'epoch': 9.87}\n",
      "{'loss': 0.0857, 'grad_norm': 0.24204668402671814, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [00:36<00:00, 27.70it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08784038573503494, 'eval_accuracy': 0.9716137869144753, 'eval_precision': 0.8231994209192907, 'eval_recall': 0.952869710934227, 'eval_f1': 0.8833009708737865, 'eval_precision_class_0': 0.9938889417005038, 'eval_precision_class_1': 0.8231994209192907, 'eval_recall_class_0': 0.973995555023223, 'eval_recall_class_1': 0.952869710934227, 'eval_f1_class_0': 0.9838416970250978, 'eval_f1_class_1': 0.8833009708737865, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4926, 'eval_samples_per_second': 383.684, 'eval_steps_per_second': 12.18, 'epoch': 10.0}\n",
      "100% 780/780 [00:36<00:00, 27.70it/s]\n",
      "100% 6/6 [00:00<00:00, 46.90it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0878\n",
      "  eval/accuracy             0.9716\n",
      "  eval/precision            0.8232\n",
      "  eval/recall               0.9529\n",
      "  eval/f1                   0.8833\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9838\n",
      "    eval/f1_class_1         0.8833\n",
      "    eval/precision_class_0  0.9939\n",
      "    eval/precision_class_1  0.8232\n",
      "    eval/recall_class_0     0.9740\n",
      "    eval/recall_class_1     0.9529\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 36.937, 'train_samples_per_second': 337.602, 'train_steps_per_second': 21.117, 'train_loss': 0.14383550599599496, 'epoch': 10.0}\n",
      "100% 780/780 [00:36<00:00, 21.12it/s]\n",
      "\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "TRAINING COMPLETED\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 36.9370\n",
      "train_samples_per_second.......................... 337.6020\n",
      "train_steps_per_second............................ 21.1170\n",
      "total_flos........................................ 0.0000\n",
      "train_loss........................................ 0.1438\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      "100% 6/6 [00:00<00:00, 48.63it/s]\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0878\n",
      "  eval/accuracy             0.9716\n",
      "  eval/precision            0.8232\n",
      "  eval/recall               0.9529\n",
      "  eval/f1                   0.8833\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9838\n",
      "    eval/f1_class_1         0.8833\n",
      "    eval/precision_class_0  0.9939\n",
      "    eval/precision_class_1  0.8232\n",
      "    eval/recall_class_0     0.9740\n",
      "    eval/recall_class_1     0.9529\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 16.92it/s]\n",
      "eval_loss......................................... 0.0878\n",
      "eval_accuracy..................................... 0.9716\n",
      "eval_precision.................................... 0.8232\n",
      "eval_recall....................................... 0.9529\n",
      "eval_f1........................................... 0.8833\n",
      "eval_precision_class_0............................ 0.9939\n",
      "eval_precision_class_1............................ 0.8232\n",
      "eval_recall_class_0............................... 0.9740\n",
      "eval_recall_class_1............................... 0.9529\n",
      "eval_f1_class_0................................... 0.9838\n",
      "eval_f1_class_1................................... 0.8833\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.4930\n",
      "eval_samples_per_second........................... 383.4020\n",
      "eval_steps_per_second............................. 12.1710\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model)... Done. 0.0s\n",
      "âœ“ Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "\n",
      "ğŸ—ï¸  MLP Classifier Architecture:\n",
      "   Input:  768 (BERT hidden size)\n",
      "   Layer 1: Linear(128 â†’ 512) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 2: Linear(512 â†’ 256) â†’ GELU â†’ Dropout(0.3)\n",
      "   Layer 3: Linear(256 â†’ 128) â†’ GELU â†’ Dropout(0.3)\n",
      "   Output: Linear(128 â†’ 2)\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 30.67it/s]\n",
      "\n",
      "ğŸ“Š TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0635\n",
      "eval_model_preparation_time....................... 0.0024\n",
      "eval_accuracy..................................... 0.9765\n",
      "eval_precision.................................... 0.8553\n",
      "eval_recall....................................... 0.9490\n",
      "eval_f1........................................... 0.8997\n",
      "eval_precision_class_0............................ 0.9935\n",
      "eval_precision_class_1............................ 0.8553\n",
      "eval_recall_class_0............................... 0.9800\n",
      "eval_recall_class_1............................... 0.9490\n",
      "eval_f1_class_0................................... 0.9867\n",
      "eval_f1_class_1................................... 0.8997\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.6665\n",
      "eval_samples_per_second........................... 367.5860\n",
      "eval_steps_per_second............................. 24.0060\n",
      "100% 16/16 [00:00<00:00, 31.90it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9935    0.9800    0.9867     99651\n",
      "   EDU Start (1)     0.8553    0.9490    0.8997     12438\n",
      "\n",
      "        accuracy                         0.9765    112089\n",
      "       macro avg     0.9244    0.9645    0.9432    112089\n",
      "    weighted avg     0.9782    0.9765    0.9771    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "ğŸ“Š FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9716               0.9765              \n",
      "Precision                      0.8232               0.8553              \n",
      "Recall                         0.9529               0.9490              \n",
      "F1                             0.8833               0.8997              \n",
      "======================================================================\n",
      "\n",
      "âœ… Model saved at: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "âœ… Logs saved at: ./MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-nc235y00-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-nc235y00-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-nc235y00-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-nc235y00-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-nc235y00-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-nc235y00-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-nc235y00-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-nc235y00-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-nc235y00-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-nc235y00-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-nc235y00-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading media/table/final_results_table_196_86bee189b2eea82d7928.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 38.1KB/38.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-nc235y00-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading media/table/final_results_table_196_86bee189b2eea82d7928.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading output.log 38.1KB/38.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-nc235y00-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading media/table/final_results_table_196_86bee189b2eea82d7928.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading output.log 38.1KB/38.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-nc235y00-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading media/table/final_results_table_196_86bee189b2eea82d7928.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading output.log 38.1KB/38.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-nc235y00-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading output.log 38.1KB/38.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading config.yaml 13.2KB/13.2KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-nc235y00-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading output.log 38.1KB/38.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading config.yaml 13.2KB/13.2KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-nc235y00-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading output.log 38.1KB/38.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading config.yaml 13.2KB/13.2KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-nc235y00-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 38.1KB/38.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading config.yaml 13.2KB/13.2KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-nc235y00-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 38.1KB/38.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 4.0KB/4.0KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading config.yaml 13.2KB/13.2KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-nc235y00-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-nc235y00-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-nc235y00-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-nc235y00-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-nc235y00-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-nc235y00-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-nc235y00-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-nc235y00-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-nc235y00-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-nc235y00-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 195-196, summary, console lines 740-775 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch â–â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–„â–…â–…â–†â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy â–â–â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 â–â–â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss â–ˆâ–ˆâ–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 â–â–â–â–â–†â–†â–†â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 â–â–â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall â–â–â–â–â–…â–…â–†â–†â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.97161\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.8833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98384\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.8833\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.08784\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.8232\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.8232\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mMLPdistilbert-base-uncased_dim_512_256_128_cw0.5_r32_a128_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/nc235y00\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_220550-nc235y00/logs\u001b[0m\n",
      "âœ… W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_mlp \\\n",
    "  --model_name distilbert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp6_Final_Models   \\\n",
    "  --wandb_run_name \"MLPdistilbert-base-uncased_dim_512_256_128_cw0.5_r32_a128_do0.1_lr0.0003_ep10\" \\\n",
    "  --class_1_weight_multiplier 0.5 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --mlp_dims 512 256 128 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4ecoGwuedQh",
    "outputId": "d0734f4b-f693-4926-9403-b6c1439b66f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run mh9dz6q9 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run mh9dz6q9 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run mh9dz6q9 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_220711-mh9dz6q9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLinearbert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/mh9dz6q9\u001b[0m\n",
      "âœ“ W&B initialized: Multiple-Run-edu-segmentation/Linearbert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  âœ“ gum/train.conllu\n",
      "  âœ“ gum/dev.conllu\n",
      "  âœ“ gum/test.conllu\n",
      "  âœ“ scidtb/train.conllu\n",
      "  âœ“ scidtb/dev.conllu\n",
      "  âœ“ scidtb/test.conllu\n",
      "  âœ“ oll/train.conllu\n",
      "  âœ“ oll/dev.conllu\n",
      "  âœ“ oll/test.conllu\n",
      "  âœ“ stac/train.conllu\n",
      "  âœ“ stac/dev.conllu\n",
      "  âœ“ stac/test.conllu\n",
      "  âœ“ sts/train.conllu\n",
      "  âœ“ sts/dev.conllu\n",
      "  âœ“ sts/test.conllu\n",
      "  âœ“ umuc/train.conllu\n",
      "  âœ“ umuc/dev.conllu\n",
      "  âœ“ umuc/test.conllu\n",
      "  âœ“ msdc/train.conllu\n",
      "  âœ“ msdc/dev.conllu\n",
      "  âœ“ msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "âœ“ Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "âœ“ Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "âœ“ Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "âœ“ Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "âœ“ Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "âœ“ Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "âœ“ Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "âœ“ Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "âœ“ Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "âœ“ Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "âœ“ Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "âœ“ Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "âœ“ Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "âœ“ Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "âœ“ Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "âœ“ Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "âœ“ Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "âœ“ Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "âœ“ Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "âœ“ LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 128\n",
      "  Effective LR scale:    8.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "âš–ï¸ Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "âš–ï¸ Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "TRAINING STARTED\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "\n",
      "{'loss': 0.6623, 'grad_norm': 0.7156248688697815, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5469, 'grad_norm': 0.895911693572998, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.3322, 'grad_norm': 1.3553223609924316, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.59it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.16464856266975403, 'eval_accuracy': 0.946640059511861, 'eval_precision': 0.7043810452735105, 'eval_recall': 0.907624633431085, 'eval_f1': 0.7931902430094733, 'eval_precision_class_0': 0.9878153234050784, 'eval_precision_class_1': 0.7043810452735105, 'eval_recall_class_0': 0.9515976630601136, 'eval_recall_class_1': 0.907624633431085, 'eval_f1_class_0': 0.9693683190195693, 'eval_f1_class_1': 0.7931902430094733, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6037, 'eval_samples_per_second': 313.046, 'eval_steps_per_second': 9.938, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.59it/s]\n",
      "100% 6/6 [00:00<00:00, 37.67it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1646\n",
      "  eval/accuracy             0.9466\n",
      "  eval/precision            0.7044\n",
      "  eval/recall               0.9076\n",
      "  eval/f1                   0.7932\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9694\n",
      "    eval/f1_class_1         0.7932\n",
      "    eval/precision_class_0  0.9878\n",
      "    eval/precision_class_1  0.7044\n",
      "    eval/recall_class_0     0.9516\n",
      "    eval/recall_class_1     0.9076\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.199, 'grad_norm': 2.3508524894714355, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1645, 'grad_norm': 0.7532150745391846, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1448, 'grad_norm': 0.9227691888809204, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1344, 'grad_norm': 1.202217936515808, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:43, 14.38it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.12174304574728012, 'eval_accuracy': 0.966489154435641, 'eval_precision': 0.8127913481260488, 'eval_recall': 0.9130708001675744, 'eval_f1': 0.8600177567327612, 'eval_precision_class_0': 0.9887781728702189, 'eval_precision_class_1': 0.8127913481260488, 'eval_recall_class_0': 0.9732769060832301, 'eval_recall_class_1': 0.9130708001675744, 'eval_f1_class_0': 0.9809663053975749, 'eval_f1_class_1': 0.8600177567327612, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6108, 'eval_samples_per_second': 309.407, 'eval_steps_per_second': 9.822, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:43, 14.38it/s]\n",
      "100% 6/6 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1217\n",
      "  eval/accuracy             0.9665\n",
      "  eval/precision            0.8128\n",
      "  eval/recall               0.9131\n",
      "  eval/f1                   0.8600\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9810\n",
      "    eval/f1_class_1         0.8600\n",
      "    eval/precision_class_0  0.9888\n",
      "    eval/precision_class_1  0.8128\n",
      "    eval/recall_class_0     0.9733\n",
      "    eval/recall_class_1     0.9131\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1236, 'grad_norm': 0.536709189414978, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1178, 'grad_norm': 0.4939906597137451, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1174, 'grad_norm': 0.28063806891441345, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1137, 'grad_norm': 0.5053512454032898, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.51it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09788148105144501, 'eval_accuracy': 0.9662293804390181, 'eval_precision': 0.7938488576449912, 'eval_recall': 0.9461667364893172, 'eval_f1': 0.8633409785932722, 'eval_precision_class_0': 0.9929885825751272, 'eval_precision_class_1': 0.7938488576449912, 'eval_recall_class_0': 0.9687786960514233, 'eval_recall_class_1': 0.9461667364893172, 'eval_f1_class_0': 0.9807342539575614, 'eval_f1_class_1': 0.8633409785932722, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.601, 'eval_samples_per_second': 314.497, 'eval_steps_per_second': 9.984, 'epoch': 3.0}\n",
      " 30% 234/780 [00:20<00:37, 14.51it/s]\n",
      "100% 6/6 [00:00<00:00, 38.57it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0979\n",
      "  eval/accuracy             0.9662\n",
      "  eval/precision            0.7938\n",
      "  eval/recall               0.9462\n",
      "  eval/f1                   0.8633\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9807\n",
      "    eval/f1_class_1         0.8633\n",
      "    eval/precision_class_0  0.9930\n",
      "    eval/precision_class_1  0.7938\n",
      "    eval/recall_class_0     0.9688\n",
      "    eval/recall_class_1     0.9462\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1019, 'grad_norm': 0.27088063955307007, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1018, 'grad_norm': 0.30214908719062805, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.0953, 'grad_norm': 0.30335700511932373, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1002, 'grad_norm': 0.29625436663627625, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:26<00:32, 14.34it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08974827826023102, 'eval_accuracy': 0.9709407361050432, 'eval_precision': 0.8221070811744386, 'eval_recall': 0.9472140762463344, 'eval_f1': 0.8802374811426347, 'eval_precision_class_0': 0.9931603517533384, 'eval_precision_class_1': 0.8221070811744386, 'eval_recall_class_0': 0.9739556300821123, 'eval_recall_class_1': 0.9472140762463344, 'eval_f1_class_0': 0.9834642441997191, 'eval_f1_class_1': 0.8802374811426347, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6137, 'eval_samples_per_second': 307.956, 'eval_steps_per_second': 9.776, 'epoch': 4.0}\n",
      " 40% 312/780 [00:27<00:32, 14.34it/s]\n",
      "100% 6/6 [00:00<00:00, 38.08it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0897\n",
      "  eval/accuracy             0.9709\n",
      "  eval/precision            0.8221\n",
      "  eval/recall               0.9472\n",
      "  eval/f1                   0.8802\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9835\n",
      "    eval/f1_class_1         0.8802\n",
      "    eval/precision_class_0  0.9932\n",
      "    eval/precision_class_1  0.8221\n",
      "    eval/recall_class_0     0.9740\n",
      "    eval/recall_class_1     0.9472\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.093, 'grad_norm': 0.294431209564209, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.0913, 'grad_norm': 0.3815585970878601, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.0889, 'grad_norm': 0.2512257993221283, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.0906, 'grad_norm': 0.3189448416233063, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:33<00:26, 14.56it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08723121881484985, 'eval_accuracy': 0.9719680241825975, 'eval_precision': 0.8257355612059571, 'eval_recall': 0.9523460410557185, 'eval_f1': 0.8845330739299611, 'eval_precision_class_0': 0.9938243956730052, 'eval_precision_class_1': 0.8257355612059571, 'eval_recall_class_0': 0.974461346002848, 'eval_recall_class_1': 0.9523460410557185, 'eval_f1_class_0': 0.9840476286470723, 'eval_f1_class_1': 0.8845330739299611, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.599, 'eval_samples_per_second': 315.551, 'eval_steps_per_second': 10.017, 'epoch': 5.0}\n",
      " 50% 390/780 [00:34<00:26, 14.56it/s]\n",
      "100% 6/6 [00:00<00:00, 38.49it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0872\n",
      "  eval/accuracy             0.9720\n",
      "  eval/precision            0.8257\n",
      "  eval/recall               0.9523\n",
      "  eval/f1                   0.8845\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9840\n",
      "    eval/f1_class_1         0.8845\n",
      "    eval/precision_class_0  0.9938\n",
      "    eval/precision_class_1  0.8257\n",
      "    eval/recall_class_0     0.9745\n",
      "    eval/recall_class_1     0.9523\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0892, 'grad_norm': 0.40178802609443665, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0853, 'grad_norm': 0.3419097363948822, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.0842, 'grad_norm': 0.2304520308971405, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0825, 'grad_norm': 0.2168925702571869, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:40<00:21, 14.32it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08613493293523788, 'eval_accuracy': 0.9711178547391043, 'eval_precision': 0.8169969648277093, 'eval_recall': 0.9585253456221198, 'eval_f1': 0.8821204819277109, 'eval_precision_class_0': 0.9946112917931063, 'eval_precision_class_1': 0.8169969648277093, 'eval_recall_class_0': 0.9727179569076803, 'eval_recall_class_1': 0.9585253456221198, 'eval_f1_class_0': 0.9835428048550744, 'eval_f1_class_1': 0.8821204819277109, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6167, 'eval_samples_per_second': 306.458, 'eval_steps_per_second': 9.729, 'epoch': 6.0}\n",
      " 60% 468/780 [00:41<00:21, 14.32it/s]\n",
      "100% 6/6 [00:00<00:00, 38.44it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0861\n",
      "  eval/accuracy             0.9711\n",
      "  eval/precision            0.8170\n",
      "  eval/recall               0.9585\n",
      "  eval/f1                   0.8821\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9835\n",
      "    eval/f1_class_1         0.8821\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.8170\n",
      "    eval/recall_class_0     0.9727\n",
      "    eval/recall_class_1     0.9585\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.079, 'grad_norm': 0.27138757705688477, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0768, 'grad_norm': 0.31750455498695374, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0797, 'grad_norm': 0.27476951479911804, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0795, 'grad_norm': 0.3443634808063507, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:47<00:16, 14.38it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08593804389238358, 'eval_accuracy': 0.9728063857171534, 'eval_precision': 0.8293481225565961, 'eval_recall': 0.9553833263510683, 'eval_f1': 0.8879155107801625, 'eval_precision_class_0': 0.9942190256479848, 'eval_precision_class_1': 0.8293481225565961, 'eval_recall_class_0': 0.9750202951783979, 'eval_recall_class_1': 0.9553833263510683, 'eval_f1_class_0': 0.9845260731971162, 'eval_f1_class_1': 0.8879155107801625, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6103, 'eval_samples_per_second': 309.659, 'eval_steps_per_second': 9.83, 'epoch': 7.0}\n",
      " 70% 546/780 [00:47<00:16, 14.38it/s]\n",
      "100% 6/6 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0859\n",
      "  eval/accuracy             0.9728\n",
      "  eval/precision            0.8293\n",
      "  eval/recall               0.9554\n",
      "  eval/f1                   0.8879\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9845\n",
      "    eval/f1_class_1         0.8879\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.8293\n",
      "    eval/recall_class_0     0.9750\n",
      "    eval/recall_class_1     0.9554\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0807, 'grad_norm': 0.2599294185638428, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0754, 'grad_norm': 0.23547843098640442, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0757, 'grad_norm': 0.3107384741306305, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.075, 'grad_norm': 0.2684534192085266, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:54<00:10, 14.40it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08278939872980118, 'eval_accuracy': 0.973349549528274, 'eval_precision': 0.8321640091116174, 'eval_recall': 0.9565354000837872, 'eval_f1': 0.8900258246845003, 'eval_precision_class_0': 0.9943701332175706, 'eval_precision_class_1': 0.8321640091116174, 'eval_recall_class_0': 0.9754860861580229, 'eval_recall_class_1': 0.9565354000837872, 'eval_f1_class_0': 0.9848375936313862, 'eval_f1_class_1': 0.8900258246845003, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6148, 'eval_samples_per_second': 307.412, 'eval_steps_per_second': 9.759, 'epoch': 8.0}\n",
      " 80% 624/780 [00:54<00:10, 14.40it/s]\n",
      "100% 6/6 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0828\n",
      "  eval/accuracy             0.9733\n",
      "  eval/precision            0.8322\n",
      "  eval/recall               0.9565\n",
      "  eval/f1                   0.8900\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9848\n",
      "    eval/f1_class_1         0.8900\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.8322\n",
      "    eval/recall_class_0     0.9755\n",
      "    eval/recall_class_1     0.9565\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0712, 'grad_norm': 0.5382362604141235, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0713, 'grad_norm': 0.2470225989818573, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0754, 'grad_norm': 0.2508133351802826, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0746, 'grad_norm': 0.28421077132225037, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [01:00<00:05, 14.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08205733448266983, 'eval_accuracy': 0.9730307359869641, 'eval_precision': 0.8280939476061427, 'eval_recall': 0.9600963552576456, 'eval_f1': 0.8892230090212436, 'eval_precision_class_0': 0.9948247055787229, 'eval_precision_class_1': 0.8280939476061427, 'eval_recall_class_0': 0.9746742790221051, 'eval_recall_class_1': 0.9600963552576456, 'eval_f1_class_0': 0.9846464103253563, 'eval_f1_class_1': 0.8892230090212436, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.616, 'eval_samples_per_second': 306.801, 'eval_steps_per_second': 9.74, 'epoch': 9.0}\n",
      " 90% 702/780 [01:01<00:05, 14.49it/s]\n",
      "100% 6/6 [00:00<00:00, 38.61it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0821\n",
      "  eval/accuracy             0.9730\n",
      "  eval/precision            0.8281\n",
      "  eval/recall               0.9601\n",
      "  eval/f1                   0.8892\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9846\n",
      "    eval/f1_class_1         0.8892\n",
      "    eval/precision_class_0  0.9948\n",
      "    eval/precision_class_1  0.8281\n",
      "    eval/recall_class_0     0.9747\n",
      "    eval/recall_class_1     0.9601\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0709, 'grad_norm': 0.24068941175937653, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.066, 'grad_norm': 0.36236608028411865, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0739, 'grad_norm': 0.23895929753780365, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0764, 'grad_norm': 0.3235097825527191, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:07<00:00, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08249472826719284, 'eval_accuracy': 0.9738100579768328, 'eval_precision': 0.8334849863512284, 'eval_recall': 0.9593632174277336, 'eval_f1': 0.8920050637842049, 'eval_precision_class_0': 0.9947353424062741, 'eval_precision_class_1': 0.8334849863512284, 'eval_recall_class_0': 0.9756457859224658, 'eval_recall_class_1': 0.9593632174277336, 'eval_f1_class_0': 0.9850980919107767, 'eval_f1_class_1': 0.8920050637842049, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6081, 'eval_samples_per_second': 310.809, 'eval_steps_per_second': 9.867, 'epoch': 10.0}\n",
      "100% 780/780 [01:08<00:00, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.53it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0825\n",
      "  eval/accuracy             0.9738\n",
      "  eval/precision            0.8335\n",
      "  eval/recall               0.9594\n",
      "  eval/f1                   0.8920\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9851\n",
      "    eval/f1_class_1         0.8920\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8335\n",
      "    eval/recall_class_0     0.9756\n",
      "    eval/recall_class_1     0.9594\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 68.956, 'train_samples_per_second': 180.84, 'train_steps_per_second': 11.312, 'train_loss': 0.127235569862219, 'epoch': 10.0}\n",
      "100% 780/780 [01:08<00:00, 11.31it/s]\n",
      "\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "TRAINING COMPLETED\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 68.9560\n",
      "train_samples_per_second.......................... 180.8400\n",
      "train_steps_per_second............................ 11.3120\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1272\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.66it/s]\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0825\n",
      "  eval/accuracy             0.9738\n",
      "  eval/precision            0.8335\n",
      "  eval/recall               0.9594\n",
      "  eval/f1                   0.8920\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9851\n",
      "    eval/f1_class_1         0.8920\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8335\n",
      "    eval/recall_class_0     0.9756\n",
      "    eval/recall_class_1     0.9594\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.71it/s]\n",
      "eval_loss......................................... 0.0825\n",
      "eval_accuracy..................................... 0.9738\n",
      "eval_precision.................................... 0.8335\n",
      "eval_recall....................................... 0.9594\n",
      "eval_f1........................................... 0.8920\n",
      "eval_precision_class_0............................ 0.9947\n",
      "eval_precision_class_1............................ 0.8335\n",
      "eval_recall_class_0............................... 0.9756\n",
      "eval_recall_class_1............................... 0.9594\n",
      "eval_f1_class_0................................... 0.9851\n",
      "eval_f1_class_1................................... 0.8920\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.5976\n",
      "eval_samples_per_second........................... 316.2410\n",
      "eval_steps_per_second............................. 10.0390\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model)... Done. 0.0s\n",
      "âœ“ Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "âœ“ PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.41it/s]\n",
      "\n",
      "ğŸ“Š TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0621\n",
      "eval_model_preparation_time....................... 0.0047\n",
      "eval_accuracy..................................... 0.9777\n",
      "eval_precision.................................... 0.8586\n",
      "eval_recall....................................... 0.9564\n",
      "eval_f1........................................... 0.9049\n",
      "eval_precision_class_0............................ 0.9945\n",
      "eval_precision_class_1............................ 0.8586\n",
      "eval_recall_class_0............................... 0.9803\n",
      "eval_recall_class_1............................... 0.9564\n",
      "eval_f1_class_0................................... 0.9874\n",
      "eval_f1_class_1................................... 0.9049\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8085\n",
      "eval_samples_per_second........................... 303.0270\n",
      "eval_steps_per_second............................. 19.7890\n",
      "100% 16/16 [00:00<00:00, 24.83it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9945    0.9803    0.9874     99651\n",
      "   EDU Start (1)     0.8586    0.9564    0.9049     12438\n",
      "\n",
      "        accuracy                         0.9777    112089\n",
      "       macro avg     0.9265    0.9684    0.9461    112089\n",
      "    weighted avg     0.9794    0.9777    0.9782    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "ğŸ“Š FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9738               0.9777              \n",
      "Precision                      0.8335               0.8586              \n",
      "Recall                         0.9594               0.9564              \n",
      "F1                             0.8920               0.9049              \n",
      "======================================================================\n",
      "\n",
      "âœ… Model saved at: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "âœ… Logs saved at: ./Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-mh9dz6q9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-mh9dz6q9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-mh9dz6q9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-mh9dz6q9-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading media/table/final_results_table_116_05a179c2663e2b59ba52.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading media/table/final_results_table_116_05a179c2663e2b59ba52.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading media/table/final_results_table_116_05a179c2663e2b59ba52.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading media/table/final_results_table_116_05a179c2663e2b59ba52.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading media/table/final_results_table_116_05a179c2663e2b59ba52.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-mh9dz6q9-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 113-116, summary, console lines 665-721 (0.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy â–â–â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 â–â–â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 â–â–â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 â–â–â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss â–ˆâ–ˆâ–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision â–â–â–‡â–‡â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 â–â–â–‚â–‚â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 â–â–â–‡â–‡â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall â–â–â–‚â–‚â–†â–†â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.97381\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.89201\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.9851\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.89201\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.08249\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.83348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99474\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.83348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95936\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mLinearbert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/mh9dz6q9\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_220711-mh9dz6q9/logs\u001b[0m\n",
      "âœ… W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp6_Final_Models  \\\n",
    "  --wandb_run_name \"Linearbert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\" \\\n",
    "  --class_1_weight_multiplier 0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XHSEEJfle59s",
    "outputId": "cc91fdfc-d01f-4d2b-c202-a70f02e5c4b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m setting up run w0f0p6yo (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m setting up run w0f0p6yo (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m setting up run w0f0p6yo (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_220904-w0f0p6yo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mLineardistilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/w0f0p6yo\u001b[0m\n",
      "âœ“ W&B initialized: Multiple-Run-edu-segmentation/Lineardistilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  âœ“ gum/train.conllu\n",
      "  âœ“ gum/dev.conllu\n",
      "  âœ“ gum/test.conllu\n",
      "  âœ“ scidtb/train.conllu\n",
      "  âœ“ scidtb/dev.conllu\n",
      "  âœ“ scidtb/test.conllu\n",
      "  âœ“ oll/train.conllu\n",
      "  âœ“ oll/dev.conllu\n",
      "  âœ“ oll/test.conllu\n",
      "  âœ“ stac/train.conllu\n",
      "  âœ“ stac/dev.conllu\n",
      "  âœ“ stac/test.conllu\n",
      "  âœ“ sts/train.conllu\n",
      "  âœ“ sts/dev.conllu\n",
      "  âœ“ sts/test.conllu\n",
      "  âœ“ umuc/train.conllu\n",
      "  âœ“ umuc/dev.conllu\n",
      "  âœ“ umuc/test.conllu\n",
      "  âœ“ msdc/train.conllu\n",
      "  âœ“ msdc/dev.conllu\n",
      "  âœ“ msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "âœ“ Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "âœ“ Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "âœ“ Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "âœ“ Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "âœ“ Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "âœ“ Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "âœ“ Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "âœ“ Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "âœ“ Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "âœ“ Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "âœ“ Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "âœ“ Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "âœ“ Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "âœ“ Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "âœ“ Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "âœ“ Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "âœ“ Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "âœ“ Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "âœ“ Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "ğŸ“š Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "âœ“ Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing distilbert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "âœ“ LoRA target validation passed\n",
      "  Target modules: ['q_lin', 'v_lin']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 128\n",
      "  Effective LR scale:    4.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        q_lin, v_lin\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "ğŸ“Š Trainable Parameters:\n",
      "trainable params: 642,818 || all params: 66,955,780 || trainable%: 0.9601\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    distilbert-base-uncased\n",
      "Output directory:         ./Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "âš–ï¸ Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "âš–ï¸ Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "TRAINING STARTED\n",
      "ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€ğŸš€\n",
      "\n",
      "{'loss': 0.7004, 'grad_norm': 0.6686978340148926, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6011, 'grad_norm': 0.6562354564666748, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4214, 'grad_norm': 2.3399875164031982, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 76/780 [00:03<00:25, 28.13it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.19307951629161835, 'eval_accuracy': 0.9267791566791437, 'eval_precision': 0.6191527233891064, 'eval_recall': 0.9107666527021365, 'eval_f1': 0.7371678040096639, 'eval_precision_class_0': 0.9879395277730593, 'eval_precision_class_1': 0.6191527233891064, 'eval_recall_class_0': 0.9288138299996007, 'eval_recall_class_1': 0.9107666527021365, 'eval_f1_class_0': 0.9574647597489454, 'eval_f1_class_1': 0.7371678040096639, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4999, 'eval_samples_per_second': 378.059, 'eval_steps_per_second': 12.002, 'epoch': 1.0}\n",
      " 10% 78/780 [00:04<00:24, 28.13it/s]\n",
      "100% 6/6 [00:00<00:00, 48.28it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1931\n",
      "  eval/accuracy             0.9268\n",
      "  eval/precision            0.6192\n",
      "  eval/recall               0.9108\n",
      "  eval/f1                   0.7372\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9575\n",
      "    eval/f1_class_1         0.7372\n",
      "    eval/precision_class_0  0.9879\n",
      "    eval/precision_class_1  0.6192\n",
      "    eval/recall_class_0     0.9288\n",
      "    eval/recall_class_1     0.9108\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2391, 'grad_norm': 1.903685212135315, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1941, 'grad_norm': 1.5593466758728027, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1632, 'grad_norm': 0.8846463561058044, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.151, 'grad_norm': 0.4885837137699127, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 154/780 [00:07<00:22, 28.31it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.1298730969429016, 'eval_accuracy': 0.960443505059689, 'eval_precision': 0.7757608115323011, 'eval_recall': 0.9130708001675744, 'eval_f1': 0.8388338304628115, 'eval_precision_class_0': 0.9886999496262815, 'eval_precision_class_1': 0.7757608115323011, 'eval_recall_class_0': 0.966463049467002, 'eval_recall_class_1': 0.9130708001675744, 'eval_f1_class_0': 0.9774550446861204, 'eval_f1_class_1': 0.8388338304628115, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4849, 'eval_samples_per_second': 389.79, 'eval_steps_per_second': 12.374, 'epoch': 2.0}\n",
      " 20% 156/780 [00:08<00:22, 28.31it/s]\n",
      "100% 6/6 [00:00<00:00, 48.77it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1299\n",
      "  eval/accuracy             0.9604\n",
      "  eval/precision            0.7758\n",
      "  eval/recall               0.9131\n",
      "  eval/f1                   0.8388\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9775\n",
      "    eval/f1_class_1         0.8388\n",
      "    eval/precision_class_0  0.9887\n",
      "    eval/precision_class_1  0.7758\n",
      "    eval/recall_class_0     0.9665\n",
      "    eval/recall_class_1     0.9131\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.136, 'grad_norm': 0.29922473430633545, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1359, 'grad_norm': 0.21032528579235077, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1309, 'grad_norm': 0.2100580334663391, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1231, 'grad_norm': 0.6051647067070007, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 232/780 [00:11<00:19, 27.82it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10996084660291672, 'eval_accuracy': 0.9627224314846083, 'eval_precision': 0.7779904306220096, 'eval_recall': 0.9366359447004609, 'eval_f1': 0.8499738630423419, 'eval_precision_class_0': 0.9917342951608056, 'eval_precision_class_1': 0.7779904306220096, 'eval_recall_class_0': 0.9660371834284878, 'eval_recall_class_1': 0.9366359447004609, 'eval_f1_class_0': 0.97871709306637, 'eval_f1_class_1': 0.8499738630423419, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4898, 'eval_samples_per_second': 385.836, 'eval_steps_per_second': 12.249, 'epoch': 3.0}\n",
      " 30% 234/780 [00:12<00:19, 27.82it/s]\n",
      "100% 6/6 [00:00<00:00, 47.42it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1100\n",
      "  eval/accuracy             0.9627\n",
      "  eval/precision            0.7780\n",
      "  eval/recall               0.9366\n",
      "  eval/f1                   0.8500\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8500\n",
      "    eval/precision_class_0  0.9917\n",
      "    eval/precision_class_1  0.7780\n",
      "    eval/recall_class_0     0.9660\n",
      "    eval/recall_class_1     0.9366\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1173, 'grad_norm': 0.48010486364364624, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1148, 'grad_norm': 0.2048516571521759, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1092, 'grad_norm': 0.19102977216243744, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1143, 'grad_norm': 0.17949864268302917, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 310/780 [00:15<00:16, 28.35it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10260969400405884, 'eval_accuracy': 0.9677880244187557, 'eval_precision': 0.8084856160665822, 'eval_recall': 0.9360075408462505, 'eval_f1': 0.8675856712940492, 'eval_precision_class_0': 0.9917023154749779, 'eval_precision_class_1': 0.8084856160665822, 'eval_recall_class_0': 0.971826299889541, 'eval_recall_class_1': 0.9360075408462505, 'eval_f1_class_0': 0.9816637092004087, 'eval_f1_class_1': 0.8675856712940492, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5048, 'eval_samples_per_second': 374.386, 'eval_steps_per_second': 11.885, 'epoch': 4.0}\n",
      " 40% 312/780 [00:16<00:16, 28.35it/s]\n",
      "100% 6/6 [00:00<00:00, 47.90it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1026\n",
      "  eval/accuracy             0.9678\n",
      "  eval/precision            0.8085\n",
      "  eval/recall               0.9360\n",
      "  eval/f1                   0.8676\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9817\n",
      "    eval/f1_class_1         0.8676\n",
      "    eval/precision_class_0  0.9917\n",
      "    eval/precision_class_1  0.8085\n",
      "    eval/recall_class_0     0.9718\n",
      "    eval/recall_class_1     0.9360\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1051, 'grad_norm': 0.3021628260612488, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1046, 'grad_norm': 0.31911972165107727, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1009, 'grad_norm': 0.2690565288066864, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1036, 'grad_norm': 0.27627134323120117, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 388/780 [00:19<00:13, 28.37it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09708569943904877, 'eval_accuracy': 0.9662175725300807, 'eval_precision': 0.7916775713164094, 'eval_recall': 0.9504608294930875, 'eval_f1': 0.8638332302127457, 'eval_precision_class_0': 0.9935405457078087, 'eval_precision_class_1': 0.7916775713164094, 'eval_recall_class_0': 0.9682197468758733, 'eval_recall_class_1': 0.9504608294930875, 'eval_f1_class_0': 0.9807167362014464, 'eval_f1_class_1': 0.8638332302127457, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5014, 'eval_samples_per_second': 376.914, 'eval_steps_per_second': 11.966, 'epoch': 5.0}\n",
      " 50% 390/780 [00:20<00:13, 28.37it/s]\n",
      "100% 6/6 [00:00<00:00, 46.77it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0971\n",
      "  eval/accuracy             0.9662\n",
      "  eval/precision            0.7917\n",
      "  eval/recall               0.9505\n",
      "  eval/f1                   0.8638\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9807\n",
      "    eval/f1_class_1         0.8638\n",
      "    eval/precision_class_0  0.9935\n",
      "    eval/precision_class_1  0.7917\n",
      "    eval/recall_class_0     0.9682\n",
      "    eval/recall_class_1     0.9505\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1033, 'grad_norm': 0.27899813652038574, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0962, 'grad_norm': 0.32460999488830566, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.0979, 'grad_norm': 0.20756997168064117, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0953, 'grad_norm': 0.18146729469299316, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 466/780 [00:23<00:11, 28.11it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09635476022958755, 'eval_accuracy': 0.9680241825975038, 'eval_precision': 0.8020667726550079, 'eval_recall': 0.9510892333472979, 'eval_f1': 0.8702443699089603, 'eval_precision_class_0': 0.9936347404146278, 'eval_precision_class_1': 0.8020667726550079, 'eval_recall_class_0': 0.9701760689902983, 'eval_recall_class_1': 0.9510892333472979, 'eval_f1_class_0': 0.9817652921054758, 'eval_f1_class_1': 0.8702443699089603, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4939, 'eval_samples_per_second': 382.707, 'eval_steps_per_second': 12.149, 'epoch': 6.0}\n",
      " 60% 468/780 [00:24<00:11, 28.11it/s]\n",
      "100% 6/6 [00:00<00:00, 49.07it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0964\n",
      "  eval/accuracy             0.9680\n",
      "  eval/precision            0.8021\n",
      "  eval/recall               0.9511\n",
      "  eval/f1                   0.8702\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9818\n",
      "    eval/f1_class_1         0.8702\n",
      "    eval/precision_class_0  0.9936\n",
      "    eval/precision_class_1  0.8021\n",
      "    eval/recall_class_0     0.9702\n",
      "    eval/recall_class_1     0.9511\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0902, 'grad_norm': 0.2098938524723053, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.089, 'grad_norm': 0.23014017939567566, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0937, 'grad_norm': 0.23378053307533264, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0923, 'grad_norm': 0.2214786410331726, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 544/780 [00:27<00:08, 28.02it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.094151571393013, 'eval_accuracy': 0.9668788154305754, 'eval_precision': 0.792132397539208, 'eval_recall': 0.9574780058651027, 'eval_f1': 0.866992270852103, 'eval_precision_class_0': 0.9944496090118663, 'eval_precision_class_1': 0.792132397539208, 'eval_recall_class_0': 0.9680733554251341, 'eval_recall_class_1': 0.9574780058651027, 'eval_f1_class_0': 0.9810842341643682, 'eval_f1_class_1': 0.866992270852103, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5049, 'eval_samples_per_second': 374.355, 'eval_steps_per_second': 11.884, 'epoch': 7.0}\n",
      " 70% 546/780 [00:28<00:08, 28.02it/s]\n",
      "100% 6/6 [00:00<00:00, 47.25it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0942\n",
      "  eval/accuracy             0.9669\n",
      "  eval/precision            0.7921\n",
      "  eval/recall               0.9575\n",
      "  eval/f1                   0.8670\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9811\n",
      "    eval/f1_class_1         0.8670\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7921\n",
      "    eval/recall_class_0     0.9681\n",
      "    eval/recall_class_1     0.9575\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0948, 'grad_norm': 0.2714560925960541, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0897, 'grad_norm': 0.18343466520309448, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0878, 'grad_norm': 0.17542625963687897, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0876, 'grad_norm': 0.3448408246040344, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 622/780 [00:31<00:05, 28.23it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09254825115203857, 'eval_accuracy': 0.9692758209448689, 'eval_precision': 0.8090957636169456, 'eval_recall': 0.952136573104315, 'eval_f1': 0.8748075442648191, 'eval_precision_class_0': 0.9937783344451554, 'eval_precision_class_1': 0.8090957636169456, 'eval_recall_class_0': 0.971453667105841, 'eval_recall_class_1': 0.952136573104315, 'eval_f1_class_0': 0.9824891987563428, 'eval_f1_class_1': 0.8748075442648191, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5061, 'eval_samples_per_second': 373.467, 'eval_steps_per_second': 11.856, 'epoch': 8.0}\n",
      " 80% 624/780 [00:32<00:05, 28.23it/s]\n",
      "100% 6/6 [00:00<00:00, 47.26it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0925\n",
      "  eval/accuracy             0.9693\n",
      "  eval/precision            0.8091\n",
      "  eval/recall               0.9521\n",
      "  eval/f1                   0.8748\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9825\n",
      "    eval/f1_class_1         0.8748\n",
      "    eval/precision_class_0  0.9938\n",
      "    eval/precision_class_1  0.8091\n",
      "    eval/recall_class_0     0.9715\n",
      "    eval/recall_class_1     0.9521\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0839, 'grad_norm': 0.24973128736019135, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0863, 'grad_norm': 0.2872574031352997, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0903, 'grad_norm': 0.27403852343559265, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0851, 'grad_norm': 0.21010665595531464, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 700/780 [00:35<00:02, 28.65it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09238643944263458, 'eval_accuracy': 0.9697245214844903, 'eval_precision': 0.8113409415121255, 'eval_recall': 0.9530791788856305, 'eval_f1': 0.8765170487382007, 'eval_precision_class_0': 0.9939025220148898, 'eval_precision_class_1': 0.8113409415121255, 'eval_recall_class_0': 0.9718396082032446, 'eval_recall_class_1': 0.9530791788856305, 'eval_f1_class_0': 0.9827472512683866, 'eval_f1_class_1': 0.8765170487382007, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4984, 'eval_samples_per_second': 379.19, 'eval_steps_per_second': 12.038, 'epoch': 9.0}\n",
      " 90% 702/780 [00:36<00:02, 28.65it/s]\n",
      "100% 6/6 [00:00<00:00, 47.37it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0924\n",
      "  eval/accuracy             0.9697\n",
      "  eval/precision            0.8113\n",
      "  eval/recall               0.9531\n",
      "  eval/f1                   0.8765\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9827\n",
      "    eval/f1_class_1         0.8765\n",
      "    eval/precision_class_0  0.9939\n",
      "    eval/precision_class_1  0.8113\n",
      "    eval/recall_class_0     0.9718\n",
      "    eval/recall_class_1     0.9531\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0837, 'grad_norm': 0.2109208106994629, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0788, 'grad_norm': 0.23045815527439117, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0873, 'grad_norm': 0.19128352403640747, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0897, 'grad_norm': 0.23816591501235962, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [00:40<00:00, 27.84it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09166820347309113, 'eval_accuracy': 0.9694883633057422, 'eval_precision': 0.8092911707230414, 'eval_recall': 0.9542312526183494, 'eval_f1': 0.8758050562337787, 'eval_precision_class_0': 0.9940488349607114, 'eval_precision_class_1': 0.8092911707230414, 'eval_recall_class_0': 0.9714270504784339, 'eval_recall_class_1': 0.9542312526183494, 'eval_f1_class_0': 0.9826077592009261, 'eval_f1_class_1': 0.8758050562337787, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5053, 'eval_samples_per_second': 374.064, 'eval_steps_per_second': 11.875, 'epoch': 10.0}\n",
      "100% 780/780 [00:40<00:00, 27.84it/s]\n",
      "100% 6/6 [00:00<00:00, 47.26it/s]\u001b[A\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0917\n",
      "  eval/accuracy             0.9695\n",
      "  eval/precision            0.8093\n",
      "  eval/recall               0.9542\n",
      "  eval/f1                   0.8758\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9826\n",
      "    eval/f1_class_1         0.8758\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.8093\n",
      "    eval/recall_class_0     0.9714\n",
      "    eval/recall_class_1     0.9542\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 41.2066, 'train_samples_per_second': 302.622, 'train_steps_per_second': 18.929, 'train_loss': 0.1453535055502867, 'epoch': 10.0}\n",
      "100% 780/780 [00:41<00:00, 18.93it/s]\n",
      "\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "TRAINING COMPLETED\n",
      "âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…âœ…\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 41.2066\n",
      "train_samples_per_second.......................... 302.6220\n",
      "train_steps_per_second............................ 18.9290\n",
      "total_flos........................................ 1651897958277120.0000\n",
      "train_loss........................................ 0.1454\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      "100% 6/6 [00:00<00:00, 47.03it/s]\n",
      "======================================================================\n",
      "ğŸ“Š Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0924\n",
      "  eval/accuracy             0.9697\n",
      "  eval/precision            0.8113\n",
      "  eval/recall               0.9531\n",
      "  eval/f1                   0.8765\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9827\n",
      "    eval/f1_class_1         0.8765\n",
      "    eval/precision_class_0  0.9939\n",
      "    eval/precision_class_1  0.8113\n",
      "    eval/recall_class_0     0.9718\n",
      "    eval/recall_class_1     0.9531\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 16.70it/s]\n",
      "eval_loss......................................... 0.0924\n",
      "eval_accuracy..................................... 0.9697\n",
      "eval_precision.................................... 0.8113\n",
      "eval_recall....................................... 0.9531\n",
      "eval_f1........................................... 0.8765\n",
      "eval_precision_class_0............................ 0.9939\n",
      "eval_precision_class_1............................ 0.8113\n",
      "eval_recall_class_0............................... 0.9718\n",
      "eval_recall_class_1............................... 0.9531\n",
      "eval_f1_class_0................................... 0.9827\n",
      "eval_f1_class_1................................... 0.8765\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.5159\n",
      "eval_samples_per_second........................... 366.3790\n",
      "eval_steps_per_second............................. 11.6310\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model)... Done. 0.0s\n",
      "âœ“ Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "âœ“ PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "âœ“ Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 30.61it/s]\n",
      "\n",
      "ğŸ“Š TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0667\n",
      "eval_model_preparation_time....................... 0.0022\n",
      "eval_accuracy..................................... 0.9746\n",
      "eval_precision.................................... 0.8415\n",
      "eval_recall....................................... 0.9498\n",
      "eval_f1........................................... 0.8924\n",
      "eval_precision_class_0............................ 0.9936\n",
      "eval_precision_class_1............................ 0.8415\n",
      "eval_recall_class_0............................... 0.9777\n",
      "eval_recall_class_1............................... 0.9498\n",
      "eval_f1_class_0................................... 0.9856\n",
      "eval_f1_class_1................................... 0.8924\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.6641\n",
      "eval_samples_per_second........................... 368.9040\n",
      "eval_steps_per_second............................. 24.0920\n",
      "100% 16/16 [00:00<00:00, 31.50it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9936    0.9777    0.9856     99651\n",
      "   EDU Start (1)     0.8415    0.9498    0.8924     12438\n",
      "\n",
      "        accuracy                         0.9746    112089\n",
      "       macro avg     0.9176    0.9638    0.9390    112089\n",
      "    weighted avg     0.9768    0.9746    0.9752    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "ğŸ“Š FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9697               0.9746              \n",
      "Precision                      0.8113               0.8415              \n",
      "Recall                         0.9531               0.9498              \n",
      "F1                             0.8765               0.8924              \n",
      "======================================================================\n",
      "\n",
      "âœ… Model saved at: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "âœ… Logs saved at: ./Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact edu-segmenter-w0f0p6yo (5.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-w0f0p6yo-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact edu-segmenter-w0f0p6yo (5.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-w0f0p6yo-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact edu-segmenter-w0f0p6yo (5.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-w0f0p6yo-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact edu-segmenter-w0f0p6yo (5.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-w0f0p6yo-test_confusion_matrix_table (2.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading output.log 33.5KB/33.5KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading wandb-summary.json 4.1KB/4.1KB (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (1.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading media/table/final_results_table_116_11191250ef371fc7ac6d.t...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading output.log 33.5KB/33.5KB (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading config.yaml 13.1KB/13.1KB (0.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£½\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.1s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¾\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£·\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£¯\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£Ÿ\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¡¿\u001b[0m uploading artifact run-w0f0p6yo-final_results_table (2.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ¢¿\u001b[0m uploading history steps 113-116, summary, console lines 648-721 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178mâ£»\u001b[0m uploading history steps 113-116, summary, console lines 648-721 (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy â–â–â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 â–â–â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 â–â–â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 â–â–â–†â–†â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss â–ˆâ–ˆâ–„â–„â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision â–â–â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 â–â–â–‚â–‚â–…â–…â–…â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 â–â–â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–‡â–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall â–â–â–â–â–…â–…â–…â–…â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡â–‡â–‡â–ˆâ–ˆâ–‡â–‡\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96972\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.87652\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98275\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.87652\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09239\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.81134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.9939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.81134\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95308\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ğŸš€ View run \u001b[33mLineardistilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/w0f0p6yo\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 14 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_220904-w0f0p6yo/logs\u001b[0m\n",
      "âœ… W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name distilbert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp6_Final_Models   \\\n",
    "  --wandb_run_name \"Lineardistilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\" \\\n",
    "  --class_1_weight_multiplier 0.5 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Yyp5nnyraB6",
    "outputId": "7601ee31-5381-4670-b58b-5cf6c0bb9e61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/vocab.txt (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_config.json (deflated 56%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/rng_state.pth (deflated 26%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scaler.pt (deflated 64%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/trainer_state.json (deflated 78%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/training_args.bin (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer.json (deflated 71%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scheduler.pt (deflated 61%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/optimizer.pt (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/README.md (deflated 66%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/vocab.txt (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_config.json (deflated 56%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/training_args.bin (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer.json (deflated 71%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/chunker_config.json (deflated 21%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/best_model/README.md (deflated 66%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/vocab.txt (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_config.json (deflated 56%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/rng_state.pth (deflated 26%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scaler.pt (deflated 64%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/trainer_state.json (deflated 80%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/training_args.bin (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer.json (deflated 71%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scheduler.pt (deflated 62%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/optimizer.pt (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/README.md (deflated 66%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/vocab.txt (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_config.json (deflated 56%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/rng_state.pth (deflated 26%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scaler.pt (deflated 64%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/trainer_state.json (deflated 80%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/training_args.bin (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer.json (deflated 71%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scheduler.pt (deflated 61%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/optimizer.pt (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/README.md (deflated 66%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/vocab.txt (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_config.json (deflated 56%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/rng_state.pth (deflated 26%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scaler.pt (deflated 64%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/trainer_state.json (deflated 79%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/training_args.bin (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer.json (deflated 71%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scheduler.pt (deflated 61%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/optimizer.pt (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/README.md (deflated 66%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/ (stored 0%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/vocab.txt (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_config.json (deflated 56%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/rng_state.pth (deflated 26%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scaler.pt (deflated 64%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/trainer_state.json (deflated 79%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/training_args.bin (deflated 53%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer.json (deflated 71%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scheduler.pt (deflated 61%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/optimizer.pt (deflated 8%)\n",
      "  adding: MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/README.md (deflated 66%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/vocab.txt (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/adapter_config.json (deflated 55%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/rng_state.pth (deflated 26%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/scaler.pt (deflated 64%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/trainer_state.json (deflated 79%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/training_args.bin (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/tokenizer.json (deflated 71%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/scheduler.pt (deflated 61%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/optimizer.pt (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/README.md (deflated 66%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/vocab.txt (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/adapter_config.json (deflated 55%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/training_args.bin (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/tokenizer.json (deflated 71%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/chunker_config.json (deflated 21%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/best_model/README.md (deflated 66%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/vocab.txt (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/adapter_config.json (deflated 55%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/rng_state.pth (deflated 26%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/scaler.pt (deflated 64%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/trainer_state.json (deflated 80%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/training_args.bin (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/tokenizer.json (deflated 71%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/scheduler.pt (deflated 62%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/optimizer.pt (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/README.md (deflated 66%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/vocab.txt (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/adapter_config.json (deflated 55%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/rng_state.pth (deflated 26%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/scaler.pt (deflated 64%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/trainer_state.json (deflated 80%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/training_args.bin (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/tokenizer.json (deflated 71%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/scheduler.pt (deflated 61%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/optimizer.pt (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/README.md (deflated 66%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/vocab.txt (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/adapter_config.json (deflated 55%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/rng_state.pth (deflated 26%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/scaler.pt (deflated 64%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/trainer_state.json (deflated 79%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/training_args.bin (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/tokenizer.json (deflated 71%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/scheduler.pt (deflated 61%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/optimizer.pt (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/README.md (deflated 66%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/ (stored 0%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/vocab.txt (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/adapter_config.json (deflated 55%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/rng_state.pth (deflated 26%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json (deflated 75%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/scaler.pt (deflated 64%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json (deflated 42%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/trainer_state.json (deflated 80%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/training_args.bin (deflated 53%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/tokenizer.json (deflated 71%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/scheduler.pt (deflated 61%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/optimizer.pt (deflated 8%)\n",
      "  adding: MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/README.md (deflated 66%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/vocab.txt (deflated 53%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_config.json (deflated 56%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/rng_state.pth (deflated 26%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json (deflated 75%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scaler.pt (deflated 64%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json (deflated 42%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/trainer_state.json (deflated 77%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/training_args.bin (deflated 54%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer.json (deflated 71%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scheduler.pt (deflated 61%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/optimizer.pt (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/README.md (deflated 66%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/vocab.txt (deflated 53%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_config.json (deflated 56%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/training_args.bin (deflated 54%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer.json (deflated 71%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/chunker_config.json (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/best_model/README.md (deflated 66%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/vocab.txt (deflated 53%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_config.json (deflated 56%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/rng_state.pth (deflated 26%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json (deflated 75%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scaler.pt (deflated 64%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json (deflated 42%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/trainer_state.json (deflated 78%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/training_args.bin (deflated 54%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer.json (deflated 71%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scheduler.pt (deflated 62%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/optimizer.pt (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/README.md (deflated 66%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/vocab.txt (deflated 53%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_config.json (deflated 56%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/rng_state.pth (deflated 26%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json (deflated 75%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scaler.pt (deflated 64%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json (deflated 42%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/trainer_state.json (deflated 78%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/training_args.bin (deflated 54%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer.json (deflated 71%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scheduler.pt (deflated 61%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/optimizer.pt (deflated 9%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/README.md (deflated 66%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/vocab.txt (deflated 53%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_config.json (deflated 56%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/rng_state.pth (deflated 26%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json (deflated 75%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scaler.pt (deflated 64%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json (deflated 42%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/trainer_state.json (deflated 77%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/training_args.bin (deflated 54%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer.json (deflated 71%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scheduler.pt (deflated 61%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/optimizer.pt (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/README.md (deflated 66%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/ (stored 0%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/vocab.txt (deflated 53%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_config.json (deflated 56%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/rng_state.pth (deflated 26%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json (deflated 75%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scaler.pt (deflated 64%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json (deflated 42%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/trainer_state.json (deflated 78%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/training_args.bin (deflated 54%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer.json (deflated 71%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scheduler.pt (deflated 61%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/optimizer.pt (deflated 8%)\n",
      "  adding: Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/README.md (deflated 66%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/vocab.txt (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/adapter_config.json (deflated 55%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/rng_state.pth (deflated 26%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors (deflated 7%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json (deflated 75%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/scaler.pt (deflated 64%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json (deflated 42%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/trainer_state.json (deflated 77%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/training_args.bin (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/tokenizer.json (deflated 71%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/scheduler.pt (deflated 61%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/optimizer.pt (deflated 8%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-468/README.md (deflated 66%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/vocab.txt (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/adapter_config.json (deflated 55%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/adapter_model.safetensors (deflated 7%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/training_args.bin (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/tokenizer.json (deflated 71%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/chunker_config.json (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/best_model/README.md (deflated 66%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/vocab.txt (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/adapter_config.json (deflated 55%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/rng_state.pth (deflated 26%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors (deflated 7%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json (deflated 75%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/scaler.pt (deflated 64%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json (deflated 42%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/trainer_state.json (deflated 79%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/training_args.bin (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/tokenizer.json (deflated 71%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/scheduler.pt (deflated 62%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/optimizer.pt (deflated 8%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-780/README.md (deflated 66%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/vocab.txt (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/adapter_config.json (deflated 55%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/rng_state.pth (deflated 26%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors (deflated 7%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json (deflated 75%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/scaler.pt (deflated 64%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json (deflated 42%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/trainer_state.json (deflated 78%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/training_args.bin (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/tokenizer.json (deflated 71%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/scheduler.pt (deflated 61%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/optimizer.pt (deflated 8%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-702/README.md (deflated 66%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/vocab.txt (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/adapter_config.json (deflated 55%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/rng_state.pth (deflated 26%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors (deflated 7%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json (deflated 75%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/scaler.pt (deflated 64%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json (deflated 42%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/trainer_state.json (deflated 77%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/training_args.bin (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/tokenizer.json (deflated 71%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/scheduler.pt (deflated 61%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/optimizer.pt (deflated 8%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-546/README.md (deflated 66%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/ (stored 0%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/vocab.txt (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/adapter_config.json (deflated 55%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/rng_state.pth (deflated 26%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors (deflated 7%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json (deflated 75%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/scaler.pt (deflated 64%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json (deflated 42%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/trainer_state.json (deflated 78%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/training_args.bin (deflated 53%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/tokenizer.json (deflated 71%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/scheduler.pt (deflated 61%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/optimizer.pt (deflated 8%)\n",
      "  adding: Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES/checkpoint-624/README.md (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DISRPT-Segmenter\n",
    "\n",
    "!zip -r MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip \\\n",
    "  MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES\n",
    "\n",
    "!zip -r MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES.zip \\\n",
    "  MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES\n",
    "\n",
    "!zip -r Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES.zip \\\n",
    "  Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES\n",
    "\n",
    "!zip -r Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES.zip \\\n",
    "  Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "7YuxP7-Tj3P1",
    "outputId": "79ce1ed5-8cdb-4a51-dbd8-85db5387bc61"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_443d28de-cbf6-4058-8ff5-57e2b6fa6d6c\", \"MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip\", 76486168)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_cc0c5ef9-8a94-43b8-84b7-46610329f305\", \"MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES.zip\", 73247861)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_145521b9-a0fd-47a4-aefe-3eab1032bff9\", \"Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES.zip\", 43504722)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_8ba9d599-8989-4f90-b0be-44643be17bf7\", \"Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES.zip\", 40247972)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"MLPbert_grp6_dim_512_256_128_cw0_5_rank_16_alpha_128_ep10_noES.zip\")\n",
    "files.download(\"MLPdistilbert_grp6_dim_512_256_128_cw0_5_rank_32_alpha_128_ep10_noES.zip\")\n",
    "files.download(\"Linearbert_grp6_cw0_5_rank_16_alpha_128_ep10_noES.zip\")\n",
    "files.download(\"Lineardistilbert_grp6_cw0_5_rank_32_alpha_128_ep10_noES.zip\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl5vrFrdvpWn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
