{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sDFcy5fhcnpi",
    "outputId": "566f7517-a1ea-4673-8f73-ee742c990b88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'DISRPT-Segmenter'...\n",
      "remote: Enumerating objects: 138, done.\u001b[K\n",
      "remote: Counting objects: 100% (138/138), done.\u001b[K\n",
      "remote: Compressing objects: 100% (88/88), done.\u001b[K\n",
      "remote: Total 138 (delta 66), reused 116 (delta 49), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (138/138), 51.39 KiB | 2.05 MiB/s, done.\n",
      "Resolving deltas: 100% (66/66), done.\n",
      "/content/DISRPT-Segmenter\n",
      "Collecting uv\n",
      "  Downloading uv-0.9.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Downloading uv-0.9.11-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.7/21.7 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: uv\n",
      "Successfully installed uv-0.9.11\n",
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1mThe `--system` flag has no effect, a system Python interpreter is always used in `uv venv`\u001b[0m\n",
      "Using CPython 3.12.12 interpreter at: \u001b[36m/usr/bin/python3\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Activate with: \u001b[32msource .venv/bin/activate\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "!git clone -b feature/resume-from-checkpoint --single-branch https://github.com/Arpnik/DISRPT-Segmenter.git\n",
    "%cd /content/DISRPT-Segmenter\n",
    "\n",
    "!pip install uv\n",
    "!uv venv .venv\n",
    "\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "93v6NoKpdYdX",
    "outputId": "746834dd-f754-4edc-b630-7e0098a47b04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "\u001b[2K\u001b[2mResolved \u001b[1m91 packages\u001b[0m \u001b[2min 706ms\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mPrepared \u001b[1m91 packages\u001b[0m \u001b[2min 43.07s\u001b[0m\u001b[0m\n",
      "\u001b[2K\u001b[2mInstalled \u001b[1m91 packages\u001b[0m \u001b[2min 252ms\u001b[0m\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maccelerate\u001b[0m\u001b[2m==1.12.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohappyeyeballs\u001b[0m\u001b[2m==2.6.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiohttp\u001b[0m\u001b[2m==3.13.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1maiosignal\u001b[0m\u001b[2m==1.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mannotated-types\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1manyio\u001b[0m\u001b[2m==4.11.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mattrs\u001b[0m\u001b[2m==25.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcertifi\u001b[0m\u001b[2m==2025.11.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcharset-normalizer\u001b[0m\u001b[2m==3.4.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mclick\u001b[0m\u001b[2m==8.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcontourpy\u001b[0m\u001b[2m==1.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mcycler\u001b[0m\u001b[2m==0.12.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdatasets\u001b[0m\u001b[2m==4.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdill\u001b[0m\u001b[2m==0.4.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mdisrpt-segmenter\u001b[0m\u001b[2m==0.1.0 (from file:///content/DISRPT-Segmenter)\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfilelock\u001b[0m\u001b[2m==3.20.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfonttools\u001b[0m\u001b[2m==4.60.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfrozenlist\u001b[0m\u001b[2m==1.8.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mfsspec\u001b[0m\u001b[2m==2025.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitdb\u001b[0m\u001b[2m==4.0.12\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mgitpython\u001b[0m\u001b[2m==3.1.45\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mh11\u001b[0m\u001b[2m==0.16.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhf-xet\u001b[0m\u001b[2m==1.2.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpcore\u001b[0m\u001b[2m==1.0.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhttpx\u001b[0m\u001b[2m==0.28.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mhuggingface-hub\u001b[0m\u001b[2m==0.36.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1midna\u001b[0m\u001b[2m==3.11\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjinja2\u001b[0m\u001b[2m==3.1.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mjoblib\u001b[0m\u001b[2m==1.5.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mkiwisolver\u001b[0m\u001b[2m==1.4.9\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmarkupsafe\u001b[0m\u001b[2m==3.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmatplotlib\u001b[0m\u001b[2m==3.9.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmpmath\u001b[0m\u001b[2m==1.3.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultidict\u001b[0m\u001b[2m==6.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mmultiprocess\u001b[0m\u001b[2m==0.70.16\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnetworkx\u001b[0m\u001b[2m==3.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnumpy\u001b[0m\u001b[2m==2.2.6\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cublas-cu12\u001b[0m\u001b[2m==12.8.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-cupti-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-nvrtc-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cuda-runtime-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cudnn-cu12\u001b[0m\u001b[2m==9.10.2.21\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufft-cu12\u001b[0m\u001b[2m==11.3.3.83\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cufile-cu12\u001b[0m\u001b[2m==1.13.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-curand-cu12\u001b[0m\u001b[2m==10.3.9.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusolver-cu12\u001b[0m\u001b[2m==11.7.3.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparse-cu12\u001b[0m\u001b[2m==12.5.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-cusparselt-cu12\u001b[0m\u001b[2m==0.7.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nccl-cu12\u001b[0m\u001b[2m==2.27.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvjitlink-cu12\u001b[0m\u001b[2m==12.8.93\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvshmem-cu12\u001b[0m\u001b[2m==3.3.20\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mnvidia-nvtx-cu12\u001b[0m\u001b[2m==12.8.90\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpackaging\u001b[0m\u001b[2m==25.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpandas\u001b[0m\u001b[2m==2.3.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpeft\u001b[0m\u001b[2m==0.17.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpillow\u001b[0m\u001b[2m==12.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mplatformdirs\u001b[0m\u001b[2m==4.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpropcache\u001b[0m\u001b[2m==0.4.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mprotobuf\u001b[0m\u001b[2m==6.33.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpsutil\u001b[0m\u001b[2m==7.1.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyarrow\u001b[0m\u001b[2m==22.0.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic\u001b[0m\u001b[2m==2.12.4\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpydantic-core\u001b[0m\u001b[2m==2.41.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyparsing\u001b[0m\u001b[2m==3.2.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpython-dateutil\u001b[0m\u001b[2m==2.9.0.post0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpytz\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mpyyaml\u001b[0m\u001b[2m==6.0.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mregex\u001b[0m\u001b[2m==2025.11.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mrequests\u001b[0m\u001b[2m==2.32.5\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msafetensors\u001b[0m\u001b[2m==0.7.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscikit-learn\u001b[0m\u001b[2m==1.7.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mscipy\u001b[0m\u001b[2m==1.16.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msentry-sdk\u001b[0m\u001b[2m==2.45.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msetuptools\u001b[0m\u001b[2m==80.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msix\u001b[0m\u001b[2m==1.17.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msmmap\u001b[0m\u001b[2m==5.0.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msniffio\u001b[0m\u001b[2m==1.3.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1msympy\u001b[0m\u001b[2m==1.14.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mthreadpoolctl\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtokenizers\u001b[0m\u001b[2m==0.22.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtorch\u001b[0m\u001b[2m==2.9.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtqdm\u001b[0m\u001b[2m==4.67.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtransformers\u001b[0m\u001b[2m==4.57.1\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtriton\u001b[0m\u001b[2m==3.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-extensions\u001b[0m\u001b[2m==4.15.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtyping-inspection\u001b[0m\u001b[2m==0.4.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mtzdata\u001b[0m\u001b[2m==2025.2\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1murllib3\u001b[0m\u001b[2m==2.5.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mwandb\u001b[0m\u001b[2m==0.22.3\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1mxxhash\u001b[0m\u001b[2m==3.6.0\u001b[0m\n",
      " \u001b[32m+\u001b[39m \u001b[1myarl\u001b[0m\u001b[2m==1.22.0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# make sure we're in the repo\n",
    "%cd /content/DISRPT-Segmenter\n",
    "\n",
    "# Install project deps into the .venv using uv\n",
    "!uv pip install -e . -p .venv/bin/python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bd3X1PG1vdYp",
    "outputId": "fe7ffc32-d860-4e71-f879-b894d405541f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DISRPT-Segmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BkkOBX75vgEu"
   },
   "outputs": [],
   "source": [
    "!rm com/disrpt/segmenter/utils/data_downloader.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lsz81rW7vgHB",
    "outputId": "62f98429-9def-4717-b609-40e8048f52ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_dataset.py   Helper.py    lora_config.py\n",
      "data_preprocesser.py  __init__.py  wandb_config.py\n"
     ]
    }
   ],
   "source": [
    "!ls com/disrpt/segmenter/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "ZJ77-IPDvgJv",
    "outputId": "d9cc6174-a50d-477d-a376-9d70c7b50d7b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-87f6cddb-a67c-4779-80f8-1b807bd56e7d\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-87f6cddb-a67c-4779-80f8-1b807bd56e7d\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data_downloader.py to data_downloader.py\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xHvoLFtvun8",
    "outputId": "4692ac43-4473-474d-9e85-c11614be6474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.txt  data_downloader.py\t     pyproject.toml\n",
      "com\t  disrpt_segmenter.egg-info  README.md\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHODf_qKvoKS"
   },
   "outputs": [],
   "source": [
    "!mv data_downloader.py com/disrpt/segmenter/utils/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "M84DIeSQvoNc",
    "outputId": "ef2eba75-e26d-4aa0-97c2-ace9d759c51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_dataset.py  data_preprocesser.py  __init__.py\t   wandb_config.py\n",
      "data_downloader.py   Helper.py\t\t   lora_config.py\n"
     ]
    }
   ],
   "source": [
    "!ls com/disrpt/segmenter/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hL9Xvc81dYf2",
    "outputId": "ac454439-a94d-4083-b5f4-6302db9bc5f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "\n",
      "===================================\n",
      "Multi-Dataset EDU Segmentation Loader\n",
      "===================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.erst.gum/eng.erst.gum_train.conllu\n",
      "‚úì Downloaded: gum/train.conllu\n",
      "‚úì gum: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.erst.gum/eng.erst.gum_dev.conllu\n",
      "‚úì Downloaded: gum/dev.conllu\n",
      "‚úì gum: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.erst.gum/eng.erst.gum_test.conllu\n",
      "‚úì Downloaded: gum/test.conllu\n",
      "‚úì gum: test split complete\n",
      "\n",
      "‚úÖ gum: Download complete! All splits ready for training.\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.dep.scidtb/eng.dep.scidtb_train.conllu\n",
      "‚úì Downloaded: scidtb/train.conllu\n",
      "‚úì scidtb: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.dep.scidtb/eng.dep.scidtb_dev.conllu\n",
      "‚úì Downloaded: scidtb/dev.conllu\n",
      "‚úì scidtb: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.dep.scidtb/eng.dep.scidtb_test.conllu\n",
      "‚úì Downloaded: scidtb/test.conllu\n",
      "‚úì scidtb: test split complete\n",
      "\n",
      "‚úÖ scidtb: Download complete! All splits ready for training.\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.oll/eng.rst.oll_train.conllu\n",
      "‚úì Downloaded: oll/train.conllu\n",
      "‚úì oll: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.oll/eng.rst.oll_dev.conllu\n",
      "‚úì Downloaded: oll/dev.conllu\n",
      "‚úì oll: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.oll/eng.rst.oll_test.conllu\n",
      "‚úì Downloaded: oll/test.conllu\n",
      "‚úì oll: test split complete\n",
      "\n",
      "‚úÖ oll: Download complete! All splits ready for training.\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.sts/eng.rst.sts_train.conllu\n",
      "‚úì Downloaded: sts/train.conllu\n",
      "‚úì sts: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.sts/eng.rst.sts_dev.conllu\n",
      "‚úì Downloaded: sts/dev.conllu\n",
      "‚úì sts: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.sts/eng.rst.sts_test.conllu\n",
      "‚úì Downloaded: sts/test.conllu\n",
      "‚úì sts: test split complete\n",
      "\n",
      "‚úÖ sts: Download complete! All splits ready for training.\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.umuc/eng.rst.umuc_train.conllu\n",
      "‚úì Downloaded: umuc/train.conllu\n",
      "‚úì umuc: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.umuc/eng.rst.umuc_dev.conllu\n",
      "‚úì Downloaded: umuc/dev.conllu\n",
      "‚úì umuc: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.rst.umuc/eng.rst.umuc_test.conllu\n",
      "‚úì Downloaded: umuc/test.conllu\n",
      "‚úì umuc: test split complete\n",
      "\n",
      "‚úÖ umuc: Download complete! All splits ready for training.\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.msdc/eng.sdrt.msdc_train.conllu\n",
      "‚úì Downloaded: msdc/train.conllu\n",
      "‚úì msdc: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.msdc/eng.sdrt.msdc_dev.conllu\n",
      "‚úì Downloaded: msdc/dev.conllu\n",
      "‚úì msdc: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.msdc/eng.sdrt.msdc_test.conllu\n",
      "‚úì Downloaded: msdc/test.conllu\n",
      "‚úì msdc: test split complete\n",
      "\n",
      "‚úÖ msdc: Download complete! All splits ready for training.\n",
      "\n",
      "üì• Downloading train split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.stac/eng.sdrt.stac_train.conllu\n",
      "‚úì Downloaded: stac/train.conllu\n",
      "‚úì stac: train split complete\n",
      "\n",
      "üì• Downloading dev split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.stac/eng.sdrt.stac_dev.conllu\n",
      "‚úì Downloaded: stac/dev.conllu\n",
      "‚úì stac: dev split complete\n",
      "\n",
      "üì• Downloading test split...\n",
      "https://raw.githubusercontent.com/disrpt/sharedtask2025/refs/heads/master/data/eng.sdrt.stac/eng.sdrt.stac_test.conllu\n",
      "‚úì Downloaded: stac/test.conllu\n",
      "‚úì stac: test split complete\n",
      "\n",
      "‚úÖ stac: Download complete! All splits ready for training.\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Tokenizer\n",
      "======================================================================\n",
      "Loading DistilBERT tokenizer...\n",
      "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 429kB/s]\n",
      "config.json: 100% 483/483 [00:00<00:00, 4.14MB/s]\n",
      "vocab.txt: 100% 232k/232k [00:00<00:00, 3.67MB/s]\n",
      "tokenizer.json: 100% 466k/466k [00:00<00:00, 12.2MB/s]\n",
      "‚úì Tokenizer loaded\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Load and Process Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Sample Data Point\n",
      "======================================================================\n",
      "Input IDs shape:      torch.Size([512])\n",
      "Attention mask shape: torch.Size([512])\n",
      "Labels shape:         torch.Size([512])\n",
      "\n",
      "First 20 tokens (IDs): [101, 12465, 12284, 1998, 3009, 2396, 1024, 20062, 2013, 3239, 1011, 9651, 6249, 8925, 1011, 5811, 6249, 1012, 8925, 1011]\n",
      "First 20 labels:       [-100, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, -100, -100, -100]\n",
      "\n",
      "Label meanings:\n",
      "  -100 = Ignore (special tokens or subword continuations)\n",
      "     0 = Token continues current EDU\n",
      "     1 = Token starts new EDU\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "Dataset ready for training!\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "VALIDATION: Detailed Token & Label Inspection\n",
      "======================================================================\n",
      "\n",
      "First 40 tokens with their labels:\n",
      "Idx   Token                Label      Meaning\n",
      "------------------------------------------------------------\n",
      "0     [CLS]                -100       IGNORE (special/subword)\n",
      "1     aesthetic            1          EDU START ‚Üê New segment begins\n",
      "2     appreciation         0          CONTINUE (within EDU)\n",
      "3     and                  0          CONTINUE (within EDU)\n",
      "4     spanish              0          CONTINUE (within EDU)\n",
      "5     art                  0          CONTINUE (within EDU)\n",
      "6     :                    0          CONTINUE (within EDU)\n",
      "7     insights             1          EDU START ‚Üê New segment begins\n",
      "8     from                 0          CONTINUE (within EDU)\n",
      "9     eye                  0          CONTINUE (within EDU)\n",
      "10    -                    0          CONTINUE (within EDU)\n",
      "11    tracking             0          CONTINUE (within EDU)\n",
      "12    claire               1          EDU START ‚Üê New segment begins\n",
      "13    bailey               0          CONTINUE (within EDU)\n",
      "14    -                    0          CONTINUE (within EDU)\n",
      "15    ross                 0          CONTINUE (within EDU)\n",
      "16    claire               0          CONTINUE (within EDU)\n",
      "17    .                    -100       IGNORE (special/subword)\n",
      "18    bailey               -100       IGNORE (special/subword)\n",
      "19    -                    -100       IGNORE (special/subword)\n",
      "20    ross                 -100       IGNORE (special/subword)\n",
      "21    @                    -100       IGNORE (special/subword)\n",
      "22    port                 -100       IGNORE (special/subword)\n",
      "23    .                    -100       IGNORE (special/subword)\n",
      "24    ac                   -100       IGNORE (special/subword)\n",
      "25    .                    -100       IGNORE (special/subword)\n",
      "26    uk                   -100       IGNORE (special/subword)\n",
      "27    university           0          CONTINUE (within EDU)\n",
      "28    of                   0          CONTINUE (within EDU)\n",
      "29    portsmouth           0          CONTINUE (within EDU)\n",
      "30    ,                    0          CONTINUE (within EDU)\n",
      "31    united               0          CONTINUE (within EDU)\n",
      "32    kingdom              0          CONTINUE (within EDU)\n",
      "33    andrew               1          EDU START ‚Üê New segment begins\n",
      "34    be                   0          CONTINUE (within EDU)\n",
      "35    ##res                -100       IGNORE (special/subword)\n",
      "36    ##ford               -100       IGNORE (special/subword)\n",
      "37    a                    0          CONTINUE (within EDU)\n",
      "38    .                    -100       IGNORE (special/subword)\n",
      "39    m                    -100       IGNORE (special/subword)\n",
      "\n",
      "======================================================================\n",
      "Label Distribution Statistics\n",
      "======================================================================\n",
      "Total tokens:           512\n",
      "Ignored (-100):            67 ( 13.1%) [Special tokens + subwords]\n",
      "EDU starts (1):            42 (  8.2%) [New segments]\n",
      "EDU continues (0):        403 ( 78.7%) [Within segments]\n",
      "\n",
      "Class Balance (excluding ignored):\n",
      "  Positive (EDU start):    9.4%\n",
      "  Negative (continue):     90.6%\n",
      "  Imbalance ratio:         9.6:1\n",
      "\n",
      "======================================================================\n",
      "Tensor Shape Validation\n",
      "======================================================================\n",
      "input_ids shape:      torch.Size([512])\n",
      "attention_mask shape: torch.Size([512])\n",
      "labels shape:         torch.Size([512])\n",
      "‚úì All shapes match correctly\n",
      "\n",
      "======================================================================\n",
      "DATASET-WIDE ANALYSIS (Per Dataset)\n",
      "======================================================================\n",
      "\n",
      "üìä GUM Dataset:\n",
      "  Examples:             390\n",
      "  Mean length:          509.7 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       257/512 tokens\n",
      "  Mean EDUs/sequence:   57.89\n",
      "  Total EDUs:           22578\n",
      "  Mean valid tokens:    443.4\n",
      "\n",
      "üìä MSDC Dataset:\n",
      "  Examples:             348\n",
      "  Mean length:          490.0 tokens\n",
      "  Median length:        504.0 tokens\n",
      "  Min/Max length:       136/512 tokens\n",
      "  Mean EDUs/sequence:   46.71\n",
      "  Total EDUs:           16255\n",
      "  Mean valid tokens:    478.3\n",
      "\n",
      "üìä OLL Dataset:\n",
      "  Examples:             76\n",
      "  Mean length:          508.0 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       372/512 tokens\n",
      "  Mean EDUs/sequence:   32.29\n",
      "  Total EDUs:           2454\n",
      "  Mean valid tokens:    473.5\n",
      "\n",
      "üìä SCIDTB Dataset:\n",
      "  Examples:             128\n",
      "  Mean length:          512.0 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       512/512 tokens\n",
      "  Mean EDUs/sequence:   46.81\n",
      "  Total EDUs:           5992\n",
      "  Mean valid tokens:    428.8\n",
      "\n",
      "üìä STAC Dataset:\n",
      "  Examples:             86\n",
      "  Mean length:          510.4 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       376/512 tokens\n",
      "  Mean EDUs/sequence:   106.35\n",
      "  Total EDUs:           9146\n",
      "  Mean valid tokens:    446.0\n",
      "\n",
      "üìä STS Dataset:\n",
      "  Examples:             117\n",
      "  Mean length:          509.7 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       447/512 tokens\n",
      "  Mean EDUs/sequence:   21.23\n",
      "  Total EDUs:           2484\n",
      "  Mean valid tokens:    458.6\n",
      "\n",
      "üìä UMUC Dataset:\n",
      "  Examples:             102\n",
      "  Mean length:          507.1 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       308/512 tokens\n",
      "  Mean EDUs/sequence:   41.43\n",
      "  Total EDUs:           4226\n",
      "  Mean valid tokens:    471.0\n",
      "\n",
      "üìä COMBINED Dataset:\n",
      "  Total examples:       1247\n",
      "  Mean length:          504.2 tokens\n",
      "  Median length:        512.0 tokens\n",
      "  Min/Max length:       136/512 tokens\n",
      "  Mean EDUs/sequence:   50.63\n",
      "  Total EDUs:           63135\n",
      "  Mean valid tokens:    457.3\n",
      "\n",
      "======================================================================\n",
      "Sample Sequences (from each dataset):\n",
      "======================================================================\n",
      "\n",
      "[GUM]\n",
      "  Length: 512 tokens | EDUs: 42\n",
      "  Text: aesthetic appreciation and spanish art : insights from eye - tracking claire bailey - ross claire. bailey - ross @ port. ac. uk university of portsmou...\n",
      "\n",
      "[SCIDTB]\n",
      "  Length: 512 tokens | EDUs: 38\n",
      "  Text: we propose a neural network approach to benefit from the non - linearity of corpus - wide statistics for part - of - speech ( pos ) tagging. we invest...\n",
      "\n",
      "[OLL]\n",
      "  Length: 512 tokens | EDUs: 34\n",
      "  Text: there is a popular hci term often floating around called \" intuitiveness \". we often read about products being rated for intuitiveness - - how well a ...\n",
      "\n",
      "[STAC]\n",
      "  Length: 512 tokens | EDUs: 74\n",
      "  Text: ello just got a connection reset hm, should n ' t happen ( and has n ' t before ). let me know if you have problems. got a socketexception error appea...\n",
      "\n",
      "[STS]\n",
      "  Length: 512 tokens | EDUs: 29\n",
      "  Text: relativism... and the enlightenment : response to marks et al. ' relativism ' is a term that has outlived its usefulness in science studies, and now d...\n",
      "\n",
      "[UMUC]\n",
      "  Length: 506 tokens | EDUs: 36\n",
      "  Text: many times, we have put forth in this chamber the essence of the russian position regarding the cause and development of the internal crisis in ukrain...\n",
      "\n",
      "[MSDC]\n",
      "  Length: 479 tokens | EDUs: 42\n",
      "  Text: mission has started. alright, start with a row of 5 orange ones on the ground any direction near the center preferably i was just about to ask place o...\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DISRPT-Segmenter\n",
    "!TOKENIZERS_PARALLELISM=true ./.venv/bin/python -m com.disrpt.segmenter.dataset_prep\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKR_RCqAdiue"
   },
   "outputs": [],
   "source": [
    "!rm com/disrpt/segmenter/fine_tune_with_linear_head.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "NO70EtWydiwv",
    "outputId": "76029225-b81e-43b6-aba2-19426821455c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-a563c285-2ca8-44b2-89ba-2fe2988505cf\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-a563c285-2ca8-44b2-89ba-2fe2988505cf\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine_tune_with_linear_head.py to fine_tune_with_linear_head.py\n"
     ]
    }
   ],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g63BmjxQdizH",
    "outputId": "eb4ce32b-6ee4-4ad0-d656-f6171ce234d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args.txt  dataset\t\t     fine_tune_with_linear_head.py  README.md\n",
      "com\t  disrpt_segmenter.egg-info  pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FjOy8e1d4DR"
   },
   "outputs": [],
   "source": [
    "!mv fine_tune_with_linear_head.py com/disrpt/segmenter/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihA7mXSGd4Fy",
    "outputId": "2b7f0426-2e7b-4993-b2cf-16c9f43e24de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_with_mlp_classifier.py  fine_tune_with_linear_head.py  __init__.py  utils\n",
      "dataset_prep.py\t\t     fine_tune_with_mlp.py\t    __pycache__\n"
     ]
    }
   ],
   "source": [
    "!ls com/disrpt/segmenter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yKN0lN9h63Q",
    "outputId": "30c1a4d8-4b9a-4b05-b025-b13060fa764b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
      "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"WANDB_API_KEY\"] = \"721741755abc53c01670a1316f5fcb1f112ec711\"\n",
    "\n",
    "import wandb\n",
    "wandb.login()   # optional, but nice to verify it's working\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z55nX9ERXbnz",
    "outputId": "6bc8c11b-9402-4aa3-90bb-b18eda5fd285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_023401-zyz5ieon\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r32_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/zyz5ieon\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r32_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    1.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 1,283,330 || all params: 110,074,372 || trainable%: 1.1659\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6604, 'grad_norm': 0.6897633075714111, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5853, 'grad_norm': 0.6177845001220703, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4395, 'grad_norm': 0.7975525856018066, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.55it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.21112099289894104, 'eval_accuracy': 0.9246655409793479, 'eval_precision': 0.6130943881193774, 'eval_recall': 0.8993506493506493, 'eval_f1': 0.7291330559565254, 'eval_precision_class_0': 0.9864040858480823, 'eval_precision_class_1': 0.6130943881193774, 'eval_recall_class_0': 0.9278822480403508, 'eval_recall_class_1': 0.8993506493506493, 'eval_f1_class_0': 0.9562486284836516, 'eval_f1_class_1': 0.7291330559565254, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.604, 'eval_samples_per_second': 312.922, 'eval_steps_per_second': 9.934, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.55it/s]\n",
      "100% 6/6 [00:00<00:00, 37.36it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2111\n",
      "  eval/accuracy             0.9247\n",
      "  eval/precision            0.6131\n",
      "  eval/recall               0.8994\n",
      "  eval/f1                   0.7291\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9562\n",
      "    eval/f1_class_1         0.7291\n",
      "    eval/precision_class_0  0.9864\n",
      "    eval/precision_class_1  0.6131\n",
      "    eval/recall_class_0     0.9279\n",
      "    eval/recall_class_1     0.8994\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2606, 'grad_norm': 0.3551922142505646, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1932, 'grad_norm': 0.31529802083969116, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1679, 'grad_norm': 0.2503828704357147, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1559, 'grad_norm': 0.46650633215904236, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:43, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13214027881622314, 'eval_accuracy': 0.9595815277072584, 'eval_precision': 0.769704975781594, 'eval_recall': 0.9153749476330122, 'eval_f1': 0.8362436013969287, 'eval_precision_class_0': 0.9889819183461969, 'eval_precision_class_1': 0.769704975781594, 'eval_recall_class_0': 0.9651987596651628, 'eval_recall_class_1': 0.9153749476330122, 'eval_f1_class_0': 0.9769456137396868, 'eval_f1_class_1': 0.8362436013969287, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6137, 'eval_samples_per_second': 307.961, 'eval_steps_per_second': 9.777, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:43, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.10it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1321\n",
      "  eval/accuracy             0.9596\n",
      "  eval/precision            0.7697\n",
      "  eval/recall               0.9154\n",
      "  eval/f1                   0.8362\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9769\n",
      "    eval/f1_class_1         0.8362\n",
      "    eval/precision_class_0  0.9890\n",
      "    eval/precision_class_1  0.7697\n",
      "    eval/recall_class_0     0.9652\n",
      "    eval/recall_class_1     0.9154\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1424, 'grad_norm': 0.29555433988571167, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.142, 'grad_norm': 0.128428652882576, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1351, 'grad_norm': 0.3013894259929657, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1292, 'grad_norm': 0.23036469519138336, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.56it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11613843590021133, 'eval_accuracy': 0.9581645786347696, 'eval_precision': 0.7493563657503529, 'eval_recall': 0.9450146627565983, 'eval_f1': 0.8358886469961555, 'eval_precision_class_0': 0.9927733729765444, 'eval_precision_class_1': 0.7493563657503529, 'eval_recall_class_0': 0.9598355092426238, 'eval_recall_class_1': 0.9450146627565983, 'eval_f1_class_0': 0.9760266325639932, 'eval_f1_class_1': 0.8358886469961555, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6003, 'eval_samples_per_second': 314.83, 'eval_steps_per_second': 9.995, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.56it/s]\n",
      "100% 6/6 [00:00<00:00, 38.34it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1161\n",
      "  eval/accuracy             0.9582\n",
      "  eval/precision            0.7494\n",
      "  eval/recall               0.9450\n",
      "  eval/f1                   0.8359\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9760\n",
      "    eval/f1_class_1         0.8359\n",
      "    eval/precision_class_0  0.9928\n",
      "    eval/precision_class_1  0.7494\n",
      "    eval/recall_class_0     0.9598\n",
      "    eval/recall_class_1     0.9450\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1213, 'grad_norm': 0.3737085163593292, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1219, 'grad_norm': 0.30473726987838745, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1147, 'grad_norm': 0.18947270512580872, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1208, 'grad_norm': 0.24287743866443634, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.55it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10615454614162445, 'eval_accuracy': 0.9631002845706054, 'eval_precision': 0.7765911635518044, 'eval_recall': 0.9443862589023879, 'eval_f1': 0.8523087102415048, 'eval_precision_class_0': 0.9927337912914967, 'eval_precision_class_1': 0.7765911635518044, 'eval_recall_class_0': 0.9654782342529378, 'eval_recall_class_1': 0.9443862589023879, 'eval_f1_class_0': 0.9789163332636167, 'eval_f1_class_1': 0.8523087102415048, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6193, 'eval_samples_per_second': 305.203, 'eval_steps_per_second': 9.689, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.55it/s]\n",
      "100% 6/6 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1062\n",
      "  eval/accuracy             0.9631\n",
      "  eval/precision            0.7766\n",
      "  eval/recall               0.9444\n",
      "  eval/f1                   0.8523\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9789\n",
      "    eval/f1_class_1         0.8523\n",
      "    eval/precision_class_0  0.9927\n",
      "    eval/precision_class_1  0.7766\n",
      "    eval/recall_class_0     0.9655\n",
      "    eval/recall_class_1     0.9444\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1107, 'grad_norm': 0.12021749466657639, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1104, 'grad_norm': 0.2813780605792999, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1064, 'grad_norm': 0.11932455003261566, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1095, 'grad_norm': 0.1860649734735489, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:32<00:26, 14.50it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10111937671899796, 'eval_accuracy': 0.9628641263918573, 'eval_precision': 0.7700548291859974, 'eval_recall': 0.9561164641809803, 'eval_f1': 0.8530579825258141, 'eval_precision_class_0': 0.9942471922453799, 'eval_precision_class_1': 0.7700548291859974, 'eval_recall_class_0': 0.9637215368440665, 'eval_recall_class_1': 0.9561164641809803, 'eval_f1_class_0': 0.9787464098665315, 'eval_f1_class_1': 0.8530579825258141, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6219, 'eval_samples_per_second': 303.907, 'eval_steps_per_second': 9.648, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.50it/s]\n",
      "100% 6/6 [00:00<00:00, 38.51it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1011\n",
      "  eval/accuracy             0.9629\n",
      "  eval/precision            0.7701\n",
      "  eval/recall               0.9561\n",
      "  eval/f1                   0.8531\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8531\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7701\n",
      "    eval/recall_class_0     0.9637\n",
      "    eval/recall_class_1     0.9561\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1097, 'grad_norm': 0.17080003023147583, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1033, 'grad_norm': 0.13332165777683258, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1029, 'grad_norm': 0.2079951912164688, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1012, 'grad_norm': 0.11626652628183365, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.25it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10180561244487762, 'eval_accuracy': 0.9635017534744772, 'eval_precision': 0.7735785102957377, 'eval_recall': 0.9561164641809803, 'eval_f1': 0.8552157009696004, 'eval_precision_class_0': 0.9942514542860279, 'eval_precision_class_1': 0.7735785102957377, 'eval_recall_class_0': 0.9644401857840593, 'eval_recall_class_1': 0.9561164641809803, 'eval_f1_class_0': 0.9791189564207013, 'eval_f1_class_1': 0.8552157009696004, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6052, 'eval_samples_per_second': 312.279, 'eval_steps_per_second': 9.914, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.25it/s]\n",
      "100% 6/6 [00:00<00:00, 38.36it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1018\n",
      "  eval/accuracy             0.9635\n",
      "  eval/precision            0.7736\n",
      "  eval/recall               0.9561\n",
      "  eval/f1                   0.8552\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9791\n",
      "    eval/f1_class_1         0.8552\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7736\n",
      "    eval/recall_class_0     0.9644\n",
      "    eval/recall_class_1     0.9561\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0983, 'grad_norm': 0.22384847700595856, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0968, 'grad_norm': 0.2580311894416809, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0999, 'grad_norm': 0.19378562271595, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0981, 'grad_norm': 0.1281597763299942, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:45<00:16, 14.39it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09851273894309998, 'eval_accuracy': 0.9653437872687126, 'eval_precision': 0.7828243948336328, 'eval_recall': 0.9585253456221198, 'eval_f1': 0.8618108197184425, 'eval_precision_class_0': 0.9945751938409272, 'eval_precision_class_1': 0.7828243948336328, 'eval_recall_class_0': 0.9662101915066342, 'eval_recall_class_1': 0.9585253456221198, 'eval_f1_class_0': 0.9801875265797663, 'eval_f1_class_1': 0.8618108197184425, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6068, 'eval_samples_per_second': 311.477, 'eval_steps_per_second': 9.888, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.39it/s]\n",
      "100% 6/6 [00:00<00:00, 38.50it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0985\n",
      "  eval/accuracy             0.9653\n",
      "  eval/precision            0.7828\n",
      "  eval/recall               0.9585\n",
      "  eval/f1                   0.8618\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9802\n",
      "    eval/f1_class_1         0.8618\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.7828\n",
      "    eval/recall_class_0     0.9662\n",
      "    eval/recall_class_1     0.9585\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1002, 'grad_norm': 0.1624298393726349, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0974, 'grad_norm': 0.16754911839962006, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0949, 'grad_norm': 0.12720368802547455, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0944, 'grad_norm': 0.19499284029006958, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09526703506708145, 'eval_accuracy': 0.9674692108774458, 'eval_precision': 0.7954248934504653, 'eval_recall': 0.9577922077922078, 'eval_f1': 0.8690900451413638, 'eval_precision_class_0': 0.9944939337632528, 'eval_precision_class_1': 0.7954248934504653, 'eval_recall_class_0': 0.9686988461692019, 'eval_recall_class_1': 0.9577922077922078, 'eval_f1_class_0': 0.9814269245548867, 'eval_f1_class_1': 0.8690900451413638, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.606, 'eval_samples_per_second': 311.88, 'eval_steps_per_second': 9.901, 'epoch': 8.0}\n",
      " 80% 624/780 [00:52<00:10, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.66it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0953\n",
      "  eval/accuracy             0.9675\n",
      "  eval/precision            0.7954\n",
      "  eval/recall               0.9578\n",
      "  eval/f1                   0.8691\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8691\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7954\n",
      "    eval/recall_class_0     0.9687\n",
      "    eval/recall_class_1     0.9578\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0907, 'grad_norm': 0.2460736632347107, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0909, 'grad_norm': 0.14714235067367554, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0961, 'grad_norm': 0.12270773202180862, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0937, 'grad_norm': 0.118299700319767, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:58<00:05, 14.40it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09330317378044128, 'eval_accuracy': 0.9682957645030642, 'eval_precision': 0.8009823699675467, 'eval_recall': 0.9564306661080855, 'eval_f1': 0.8718315910067306, 'eval_precision_class_0': 0.9943237637812465, 'eval_precision_class_1': 0.8009823699675467, 'eval_recall_class_0': 0.9698034362065983, 'eval_recall_class_1': 0.9564306661080855, 'eval_f1_class_0': 0.981910543087941, 'eval_f1_class_1': 0.8718315910067306, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.612, 'eval_samples_per_second': 308.824, 'eval_steps_per_second': 9.804, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.40it/s]\n",
      "100% 6/6 [00:00<00:00, 38.55it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0933\n",
      "  eval/accuracy             0.9683\n",
      "  eval/precision            0.8010\n",
      "  eval/recall               0.9564\n",
      "  eval/f1                   0.8718\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9819\n",
      "    eval/f1_class_1         0.8718\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.8010\n",
      "    eval/recall_class_0     0.9698\n",
      "    eval/recall_class_1     0.9564\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.091, 'grad_norm': 0.11833583563566208, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0862, 'grad_norm': 0.16487887501716614, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0932, 'grad_norm': 0.17489643394947052, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0969, 'grad_norm': 0.1555412858724594, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.46it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09382207691669464, 'eval_accuracy': 0.9681068379600657, 'eval_precision': 0.7995450170618602, 'eval_recall': 0.9570590699622957, 'eval_f1': 0.8712399294465367, 'eval_precision_class_0': 0.9944034944034944, 'eval_precision_class_1': 0.7995450170618602, 'eval_recall_class_0': 0.9695106533051198, 'eval_recall_class_1': 0.9570590699622957, 'eval_f1_class_0': 0.981799314020795, 'eval_f1_class_1': 0.8712399294465367, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6043, 'eval_samples_per_second': 312.769, 'eval_steps_per_second': 9.929, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 14.46it/s]\n",
      "100% 6/6 [00:00<00:00, 38.28it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0938\n",
      "  eval/accuracy             0.9681\n",
      "  eval/precision            0.7995\n",
      "  eval/recall               0.9571\n",
      "  eval/f1                   0.8712\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9818\n",
      "    eval/f1_class_1         0.8712\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7995\n",
      "    eval/recall_class_0     0.9695\n",
      "    eval/recall_class_1     0.9571\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.4408, 'train_samples_per_second': 190.554, 'train_steps_per_second': 11.919, 'train_loss': 0.15033363409531422, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.92it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.4408\n",
      "train_samples_per_second.......................... 190.5540\n",
      "train_steps_per_second............................ 11.9190\n",
      "total_flos........................................ 3303619240796160.0000\n",
      "train_loss........................................ 0.1503\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.29it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0933\n",
      "  eval/accuracy             0.9683\n",
      "  eval/precision            0.8010\n",
      "  eval/recall               0.9564\n",
      "  eval/f1                   0.8718\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9819\n",
      "    eval/f1_class_1         0.8718\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.8010\n",
      "    eval/recall_class_0     0.9698\n",
      "    eval/recall_class_1     0.9564\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.38it/s]\n",
      "eval_loss......................................... 0.0933\n",
      "eval_accuracy..................................... 0.9683\n",
      "eval_precision.................................... 0.8010\n",
      "eval_recall....................................... 0.9564\n",
      "eval_f1........................................... 0.8718\n",
      "eval_precision_class_0............................ 0.9943\n",
      "eval_precision_class_1............................ 0.8010\n",
      "eval_recall_class_0............................... 0.9698\n",
      "eval_recall_class_1............................... 0.9564\n",
      "eval_f1_class_0................................... 0.9819\n",
      "eval_f1_class_1................................... 0.8718\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6252\n",
      "eval_samples_per_second........................... 302.2910\n",
      "eval_steps_per_second............................. 9.5970\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.81it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0726\n",
      "eval_model_preparation_time....................... 0.0044\n",
      "eval_accuracy..................................... 0.9736\n",
      "eval_precision.................................... 0.8317\n",
      "eval_recall....................................... 0.9549\n",
      "eval_f1........................................... 0.8891\n",
      "eval_precision_class_0............................ 0.9943\n",
      "eval_precision_class_1............................ 0.8317\n",
      "eval_recall_class_0............................... 0.9759\n",
      "eval_recall_class_1............................... 0.9549\n",
      "eval_f1_class_0................................... 0.9850\n",
      "eval_f1_class_1................................... 0.8891\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.7977\n",
      "eval_samples_per_second........................... 307.1450\n",
      "eval_steps_per_second............................. 20.0580\n",
      "100% 16/16 [00:00<00:00, 25.11it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9943    0.9759    0.9850     99651\n",
      "   EDU Start (1)     0.8317    0.9549    0.8891     12438\n",
      "\n",
      "        accuracy                         0.9736    112089\n",
      "       macro avg     0.9130    0.9654    0.9370    112089\n",
      "    weighted avg     0.9762    0.9736    0.9743    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9683               0.9736              \n",
      "Precision                      0.8010               0.8317              \n",
      "Recall                         0.9564               0.9549              \n",
      "F1                             0.8718               0.8891              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-zyz5ieon-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-zyz5ieon-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-zyz5ieon-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-zyz5ieon-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-zyz5ieon-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Finishing up...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.9683\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.87183\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.87183\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.0933\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.80098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.80098\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95643\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r32_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/zyz5ieon\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_023401-zyz5ieon/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp1_class_weight \\\n",
    "  --class_1_weight_multiplier 0.5 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp1_cw0_5_rank_32_alpha_32_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xlQ48dp5Xh65",
    "outputId": "f917a09a-1d5d-4c27-d568-4682778b9f23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_023543-fefk801n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.7_r32_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/fefk801n\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.7_r32_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    1.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 1,283,330 || all params: 110,074,372 || trainable%: 1.1659\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.7):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    3.1615\n",
      "   Ratio (1/0):        5.6230x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    3.1615\n",
      "   Ratio (1/0):        5.6230x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6892, 'grad_norm': 0.7261000871658325, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6184, 'grad_norm': 0.6760108470916748, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4695, 'grad_norm': 0.6955209374427795, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.44it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.23798643052577972, 'eval_accuracy': 0.898451983138306, 'eval_precision': 0.5283018867924528, 'eval_recall': 0.9266862170087976, 'eval_f1': 0.6729540614542135, 'eval_precision_class_0': 0.9896969429357825, 'eval_precision_class_1': 0.5283018867924528, 'eval_recall_class_0': 0.8948643217417921, 'eval_recall_class_1': 0.9266862170087976, 'eval_f1_class_0': 0.9398946058903286, 'eval_f1_class_1': 0.6729540614542135, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6206, 'eval_samples_per_second': 304.531, 'eval_steps_per_second': 9.668, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.44it/s]\n",
      "100% 6/6 [00:00<00:00, 37.44it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2380\n",
      "  eval/accuracy             0.8985\n",
      "  eval/precision            0.5283\n",
      "  eval/recall               0.9267\n",
      "  eval/f1                   0.6730\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9399\n",
      "    eval/f1_class_1         0.6730\n",
      "    eval/precision_class_0  0.9897\n",
      "    eval/precision_class_1  0.5283\n",
      "    eval/recall_class_0     0.8949\n",
      "    eval/recall_class_1     0.9267\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2842, 'grad_norm': 0.595158040523529, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.211, 'grad_norm': 0.43223878741264343, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1762, 'grad_norm': 0.5314754843711853, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.162, 'grad_norm': 0.8208613395690918, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:43, 14.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.14107418060302734, 'eval_accuracy': 0.9547048613161095, 'eval_precision': 0.7385566321416639, 'eval_recall': 0.9260578131545874, 'eval_f1': 0.8217472118959108, 'eval_precision_class_0': 0.9902911286219178, 'eval_precision_class_1': 0.7385566321416639, 'eval_recall_class_0': 0.9583449781078239, 'eval_recall_class_1': 0.9260578131545874, 'eval_f1_class_0': 0.974056189046247, 'eval_f1_class_1': 0.8217472118959108, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6456, 'eval_samples_per_second': 292.755, 'eval_steps_per_second': 9.294, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:43, 14.49it/s]\n",
      "100% 6/6 [00:00<00:00, 37.56it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1411\n",
      "  eval/accuracy             0.9547\n",
      "  eval/precision            0.7386\n",
      "  eval/recall               0.9261\n",
      "  eval/f1                   0.8217\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9741\n",
      "    eval/f1_class_1         0.8217\n",
      "    eval/precision_class_0  0.9903\n",
      "    eval/precision_class_1  0.7386\n",
      "    eval/recall_class_0     0.9583\n",
      "    eval/recall_class_1     0.9261\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.148, 'grad_norm': 0.38810497522354126, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1456, 'grad_norm': 0.17418931424617767, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1396, 'grad_norm': 0.30445775389671326, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1367, 'grad_norm': 0.4015018343925476, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.43it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11969413608312607, 'eval_accuracy': 0.9539491551441155, 'eval_precision': 0.7245547073791349, 'eval_recall': 0.9543359865940511, 'eval_f1': 0.8237208461399386, 'eval_precision_class_0': 0.9939539334100648, 'eval_precision_class_1': 0.7245547073791349, 'eval_recall_class_0': 0.9539000013308314, 'eval_recall_class_1': 0.9543359865940511, 'eval_f1_class_0': 0.9735151506920016, 'eval_f1_class_1': 0.8237208461399386, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6139, 'eval_samples_per_second': 307.843, 'eval_steps_per_second': 9.773, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.43it/s]\n",
      "100% 6/6 [00:00<00:00, 38.26it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1197\n",
      "  eval/accuracy             0.9539\n",
      "  eval/precision            0.7246\n",
      "  eval/recall               0.9543\n",
      "  eval/f1                   0.8237\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9735\n",
      "    eval/f1_class_1         0.8237\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.7246\n",
      "    eval/recall_class_0     0.9539\n",
      "    eval/recall_class_1     0.9543\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1253, 'grad_norm': 0.34691551327705383, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1265, 'grad_norm': 0.408774197101593, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.121, 'grad_norm': 0.24034394323825836, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1254, 'grad_norm': 0.2762667238712311, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.50it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11011268943548203, 'eval_accuracy': 0.9575151436432122, 'eval_precision': 0.7414380782340529, 'eval_recall': 0.9568496020108923, 'eval_f1': 0.8354823959762231, 'eval_precision_class_0': 0.9943067972971106, 'eval_precision_class_1': 0.7414380782340529, 'eval_recall_class_0': 0.957599712540424, 'eval_recall_class_1': 0.9568496020108923, 'eval_f1_class_0': 0.9756081026113838, 'eval_f1_class_1': 0.8354823959762231, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6154, 'eval_samples_per_second': 307.122, 'eval_steps_per_second': 9.75, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.50it/s]\n",
      "100% 6/6 [00:00<00:00, 38.15it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1101\n",
      "  eval/accuracy             0.9575\n",
      "  eval/precision            0.7414\n",
      "  eval/recall               0.9568\n",
      "  eval/f1                   0.8355\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9756\n",
      "    eval/f1_class_1         0.8355\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7414\n",
      "    eval/recall_class_0     0.9576\n",
      "    eval/recall_class_1     0.9568\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1171, 'grad_norm': 0.15187565982341766, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.116, 'grad_norm': 0.2717702090740204, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.111, 'grad_norm': 0.11927038431167603, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1149, 'grad_norm': 0.1703551560640335, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:32<00:27, 14.15it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10595816373825073, 'eval_accuracy': 0.9550000590395447, 'eval_precision': 0.7253869725779838, 'eval_recall': 0.9669040636782572, 'eval_f1': 0.8289113355780022, 'eval_precision_class_0': 0.9956087935299186, 'eval_precision_class_1': 0.7253869725779838, 'eval_recall_class_0': 0.9534874436060207, 'eval_recall_class_1': 0.9669040636782572, 'eval_f1_class_0': 0.9740929824680666, 'eval_f1_class_1': 0.8289113355780022, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6093, 'eval_samples_per_second': 310.192, 'eval_steps_per_second': 9.847, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:27, 14.15it/s]\n",
      "100% 6/6 [00:00<00:00, 38.36it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1060\n",
      "  eval/accuracy             0.9550\n",
      "  eval/precision            0.7254\n",
      "  eval/recall               0.9669\n",
      "  eval/f1                   0.8289\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9741\n",
      "    eval/f1_class_1         0.8289\n",
      "    eval/precision_class_0  0.9956\n",
      "    eval/precision_class_1  0.7254\n",
      "    eval/recall_class_0     0.9535\n",
      "    eval/recall_class_1     0.9669\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1153, 'grad_norm': 0.18554922938346863, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1085, 'grad_norm': 0.14815816283226013, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.107, 'grad_norm': 0.33698928356170654, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1071, 'grad_norm': 0.11383860558271408, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.52it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10381858795881271, 'eval_accuracy': 0.9585660475386414, 'eval_precision': 0.7445533327933911, 'eval_recall': 0.9628194386258903, 'eval_f1': 0.8397350993377484, 'eval_precision_class_0': 0.9950927538635924, 'eval_precision_class_1': 0.7445533327933911, 'eval_recall_class_0': 0.9580255785789382, 'eval_recall_class_1': 0.9628194386258903, 'eval_f1_class_0': 0.9762074272967054, 'eval_f1_class_1': 0.8397350993377484, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6106, 'eval_samples_per_second': 309.54, 'eval_steps_per_second': 9.827, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.52it/s]\n",
      "100% 6/6 [00:00<00:00, 38.54it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1038\n",
      "  eval/accuracy             0.9586\n",
      "  eval/precision            0.7446\n",
      "  eval/recall               0.9628\n",
      "  eval/f1                   0.8397\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9762\n",
      "    eval/f1_class_1         0.8397\n",
      "    eval/precision_class_0  0.9951\n",
      "    eval/precision_class_1  0.7446\n",
      "    eval/recall_class_0     0.9580\n",
      "    eval/recall_class_1     0.9628\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1042, 'grad_norm': 0.2361242026090622, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.1009, 'grad_norm': 0.23878583312034607, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1042, 'grad_norm': 0.15492819249629974, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.1032, 'grad_norm': 0.1555740386247635, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:45<00:16, 14.36it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10161031782627106, 'eval_accuracy': 0.9614826010461808, 'eval_precision': 0.7592806467579607, 'eval_recall': 0.9639715123586091, 'eval_f1': 0.8494693124134749, 'eval_precision_class_0': 0.9952595532404537, 'eval_precision_class_1': 0.7592806467579607, 'eval_recall_class_0': 0.9611663406129809, 'eval_recall_class_1': 0.9639715123586091, 'eval_f1_class_0': 0.9779158881035557, 'eval_f1_class_1': 0.8494693124134749, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6064, 'eval_samples_per_second': 311.652, 'eval_steps_per_second': 9.894, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.36it/s]\n",
      "100% 6/6 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1016\n",
      "  eval/accuracy             0.9615\n",
      "  eval/precision            0.7593\n",
      "  eval/recall               0.9640\n",
      "  eval/f1                   0.8495\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9779\n",
      "    eval/f1_class_1         0.8495\n",
      "    eval/precision_class_0  0.9953\n",
      "    eval/precision_class_1  0.7593\n",
      "    eval/recall_class_0     0.9612\n",
      "    eval/recall_class_1     0.9640\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1045, 'grad_norm': 0.15831102430820465, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.1021, 'grad_norm': 0.2241806536912918, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.1011, 'grad_norm': 0.18035385012626648, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0991, 'grad_norm': 0.16191962361335754, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.45it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09830505400896072, 'eval_accuracy': 0.9618014145874907, 'eval_precision': 0.7606738789330251, 'eval_recall': 0.9647046501885211, 'eval_f1': 0.8506256637576765, 'eval_precision_class_0': 0.9953568476164233, 'eval_precision_class_1': 0.7606738789330251, 'eval_recall_class_0': 0.9614325068870524, 'eval_recall_class_1': 0.9647046501885211, 'eval_f1_class_0': 0.9781006085796874, 'eval_f1_class_1': 0.8506256637576765, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6111, 'eval_samples_per_second': 309.287, 'eval_steps_per_second': 9.819, 'epoch': 8.0}\n",
      " 80% 624/780 [00:52<00:10, 14.45it/s]\n",
      "100% 6/6 [00:00<00:00, 38.45it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0983\n",
      "  eval/accuracy             0.9618\n",
      "  eval/precision            0.7607\n",
      "  eval/recall               0.9647\n",
      "  eval/f1                   0.8506\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9781\n",
      "    eval/f1_class_1         0.8506\n",
      "    eval/precision_class_0  0.9954\n",
      "    eval/precision_class_1  0.7607\n",
      "    eval/recall_class_0     0.9614\n",
      "    eval/recall_class_1     0.9647\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0949, 'grad_norm': 0.3037440776824951, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0956, 'grad_norm': 0.15178900957107544, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0997, 'grad_norm': 0.13893599808216095, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0974, 'grad_norm': 0.12594729661941528, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.53it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09781844168901443, 'eval_accuracy': 0.9628405105739825, 'eval_precision': 0.766242409117378, 'eval_recall': 0.9647046501885211, 'eval_f1': 0.8540961565209328, 'eval_precision_class_0': 0.9953624704133869, 'eval_precision_class_1': 0.766242409117378, 'eval_recall_class_0': 0.9626036384929666, 'eval_recall_class_1': 0.9647046501885211, 'eval_f1_class_0': 0.9787090096002273, 'eval_f1_class_1': 0.8540961565209328, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6014, 'eval_samples_per_second': 314.279, 'eval_steps_per_second': 9.977, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.53it/s]\n",
      "100% 6/6 [00:00<00:00, 38.45it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0978\n",
      "  eval/accuracy             0.9628\n",
      "  eval/precision            0.7662\n",
      "  eval/recall               0.9647\n",
      "  eval/f1                   0.8541\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8541\n",
      "    eval/precision_class_0  0.9954\n",
      "    eval/precision_class_1  0.7662\n",
      "    eval/recall_class_0     0.9626\n",
      "    eval/recall_class_1     0.9647\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0962, 'grad_norm': 0.14198070764541626, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0914, 'grad_norm': 0.23177820444107056, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0974, 'grad_norm': 0.14519867300987244, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.1016, 'grad_norm': 0.14553935825824738, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09779459983110428, 'eval_accuracy': 0.9623445783986114, 'eval_precision': 0.7631818558066386, 'eval_recall': 0.9656472559698366, 'eval_f1': 0.8525590642193351, 'eval_precision_class_0': 0.995482591449978, 'eval_precision_class_1': 0.7631818558066386, 'eval_recall_class_0': 0.9619249144940845, 'eval_recall_class_1': 0.9656472559698366, 'eval_f1_class_0': 0.9784160975708803, 'eval_f1_class_1': 0.8525590642193351, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6067, 'eval_samples_per_second': 311.505, 'eval_steps_per_second': 9.889, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.62it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0978\n",
      "  eval/accuracy             0.9623\n",
      "  eval/precision            0.7632\n",
      "  eval/recall               0.9656\n",
      "  eval/f1                   0.8526\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9784\n",
      "    eval/f1_class_1         0.8526\n",
      "    eval/precision_class_0  0.9955\n",
      "    eval/precision_class_1  0.7632\n",
      "    eval/recall_class_0     0.9619\n",
      "    eval/recall_class_1     0.9656\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.3699, 'train_samples_per_second': 190.761, 'train_steps_per_second': 11.932, 'train_loss': 0.15817943215370178, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.93it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.3699\n",
      "train_samples_per_second.......................... 190.7610\n",
      "train_steps_per_second............................ 11.9320\n",
      "total_flos........................................ 3303619240796160.0000\n",
      "train_loss........................................ 0.1582\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.53it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0978\n",
      "  eval/accuracy             0.9628\n",
      "  eval/precision            0.7662\n",
      "  eval/recall               0.9647\n",
      "  eval/f1                   0.8541\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8541\n",
      "    eval/precision_class_0  0.9954\n",
      "    eval/precision_class_1  0.7662\n",
      "    eval/recall_class_0     0.9626\n",
      "    eval/recall_class_1     0.9647\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.57it/s]\n",
      "eval_loss......................................... 0.0978\n",
      "eval_accuracy..................................... 0.9628\n",
      "eval_precision.................................... 0.7662\n",
      "eval_recall....................................... 0.9647\n",
      "eval_f1........................................... 0.8541\n",
      "eval_precision_class_0............................ 0.9954\n",
      "eval_precision_class_1............................ 0.7662\n",
      "eval_recall_class_0............................... 0.9626\n",
      "eval_recall_class_1............................... 0.9647\n",
      "eval_f1_class_0................................... 0.9787\n",
      "eval_f1_class_1................................... 0.8541\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6087\n",
      "eval_samples_per_second........................... 310.4830\n",
      "eval_steps_per_second............................. 9.8570\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.11it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0818\n",
      "eval_model_preparation_time....................... 0.0046\n",
      "eval_accuracy..................................... 0.9693\n",
      "eval_precision.................................... 0.8008\n",
      "eval_recall....................................... 0.9629\n",
      "eval_f1........................................... 0.8744\n",
      "eval_precision_class_0............................ 0.9953\n",
      "eval_precision_class_1............................ 0.8008\n",
      "eval_recall_class_0............................... 0.9701\n",
      "eval_recall_class_1............................... 0.9629\n",
      "eval_f1_class_0................................... 0.9825\n",
      "eval_f1_class_1................................... 0.8744\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8213\n",
      "eval_samples_per_second........................... 298.3030\n",
      "eval_steps_per_second............................. 19.4810\n",
      "100% 16/16 [00:00<00:00, 24.34it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9953    0.9701    0.9825     99651\n",
      "   EDU Start (1)     0.8008    0.9629    0.8744     12438\n",
      "\n",
      "        accuracy                         0.9693    112089\n",
      "       macro avg     0.8980    0.9665    0.9285    112089\n",
      "    weighted avg     0.9737    0.9693    0.9705    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9628               0.9693              \n",
      "Precision                      0.7662               0.8008              \n",
      "Recall                         0.9647               0.9629              \n",
      "F1                             0.8541               0.8744              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-fefk801n-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-fefk801n-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-fefk801n-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-fefk801n-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-fefk801n-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-fefk801n-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-fefk801n-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-fefk801n-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-fefk801n-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-fefk801n-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-fefk801n-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.8541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.97871\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.8541\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09782\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.76624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99536\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.76624\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.9647\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.7_r32_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/fefk801n\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_023543-fefk801n/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp1_class_weight \\\n",
    "  --class_1_weight_multiplier 0.7 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp1_cw0_7_rank_32_alpha_32_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GAdhJFMjXj5N",
    "outputId": "ecd4dd11-93a9-492f-9738-c3f7dfbc2aec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_023724-aafxlf0l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw1.5_r32_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/aafxlf0l\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw1.5_r32_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    1.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 1,283,330 || all params: 110,074,372 || trainable%: 1.1659\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=1.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    6.7746\n",
      "   Ratio (1/0):        12.0492x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    6.7746\n",
      "   Ratio (1/0):        12.0492x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.699, 'grad_norm': 0.7661717534065247, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6353, 'grad_norm': 0.6843442916870117, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4939, 'grad_norm': 0.6530336141586304, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.61it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.250943124294281, 'eval_accuracy': 0.8274628346066195, 'eval_precision': 0.3917208347588094, 'eval_recall': 0.9593632174277336, 'eval_f1': 0.5562978258229078, 'eval_precision_class_0': 0.9936709893157165, 'eval_precision_class_1': 0.3917208347588094, 'eval_recall_class_0': 0.8107025458804115, 'eval_recall_class_1': 0.9593632174277336, 'eval_f1_class_0': 0.8929100156838603, 'eval_f1_class_1': 0.5562978258229078, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6189, 'eval_samples_per_second': 305.382, 'eval_steps_per_second': 9.695, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.61it/s]\n",
      "100% 6/6 [00:00<00:00, 37.70it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2509\n",
      "  eval/accuracy             0.8275\n",
      "  eval/precision            0.3917\n",
      "  eval/recall               0.9594\n",
      "  eval/f1                   0.5563\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.8929\n",
      "    eval/f1_class_1         0.5563\n",
      "    eval/precision_class_0  0.9937\n",
      "    eval/precision_class_1  0.3917\n",
      "    eval/recall_class_0     0.8107\n",
      "    eval/recall_class_1     0.9594\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2988, 'grad_norm': 1.360732078552246, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.2166, 'grad_norm': 0.4730820655822754, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1787, 'grad_norm': 0.42972543835639954, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1609, 'grad_norm': 0.16139104962348938, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.62it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13979440927505493, 'eval_accuracy': 0.9363081391916306, 'eval_precision': 0.6468052021487136, 'eval_recall': 0.9584206116464181, 'eval_f1': 0.7723666441593517, 'eval_precision_class_0': 0.9943720673083739, 'eval_precision_class_1': 0.6468052021487136, 'eval_recall_class_0': 0.9334983564232576, 'eval_recall_class_1': 0.9584206116464181, 'eval_f1_class_0': 0.9629741491742289, 'eval_f1_class_1': 0.7723666441593517, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6167, 'eval_samples_per_second': 306.463, 'eval_steps_per_second': 9.729, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.62it/s]\n",
      "100% 6/6 [00:00<00:00, 38.40it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1398\n",
      "  eval/accuracy             0.9363\n",
      "  eval/precision            0.6468\n",
      "  eval/recall               0.9584\n",
      "  eval/f1                   0.7724\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9630\n",
      "    eval/f1_class_1         0.7724\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.6468\n",
      "    eval/recall_class_0     0.9335\n",
      "    eval/recall_class_1     0.9584\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1473, 'grad_norm': 0.3624752163887024, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1461, 'grad_norm': 0.23543301224708557, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1389, 'grad_norm': 0.487960547208786, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1319, 'grad_norm': 0.17087292671203613, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:18<00:37, 14.64it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11744867265224457, 'eval_accuracy': 0.9394490429689806, 'eval_precision': 0.656007341521954, 'eval_recall': 0.973292836196062, 'eval_f1': 0.783756430800371, 'eval_precision_class_0': 0.9963841583596841, 'eval_precision_class_1': 0.656007341521954, 'eval_recall_class_0': 0.9351485873225004, 'eval_recall_class_1': 0.973292836196062, 'eval_f1_class_0': 0.9647956942003515, 'eval_f1_class_1': 0.783756430800371, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6006, 'eval_samples_per_second': 314.676, 'eval_steps_per_second': 9.99, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.64it/s]\n",
      "100% 6/6 [00:00<00:00, 38.46it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1174\n",
      "  eval/accuracy             0.9394\n",
      "  eval/precision            0.6560\n",
      "  eval/recall               0.9733\n",
      "  eval/f1                   0.7838\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9648\n",
      "    eval/f1_class_1         0.7838\n",
      "    eval/precision_class_0  0.9964\n",
      "    eval/precision_class_1  0.6560\n",
      "    eval/recall_class_0     0.9351\n",
      "    eval/recall_class_1     0.9733\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.124, 'grad_norm': 0.4339437484741211, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1244, 'grad_norm': 0.22358758747577667, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1188, 'grad_norm': 0.19123482704162598, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1241, 'grad_norm': 0.3888309895992279, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10952941328287125, 'eval_accuracy': 0.945388421164496, 'eval_precision': 0.6803164603325764, 'eval_recall': 0.9726644323418517, 'eval_f1': 0.8006379585326954, 'eval_precision_class_0': 0.9963259100762972, 'eval_precision_class_1': 0.6803164603325764, 'eval_recall_class_0': 0.9419225189976178, 'eval_recall_class_1': 0.9726644323418517, 'eval_f1_class_0': 0.9683607084464937, 'eval_f1_class_1': 0.8006379585326954, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6139, 'eval_samples_per_second': 307.879, 'eval_steps_per_second': 9.774, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.38it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1095\n",
      "  eval/accuracy             0.9454\n",
      "  eval/precision            0.6803\n",
      "  eval/recall               0.9727\n",
      "  eval/f1                   0.8006\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9684\n",
      "    eval/f1_class_1         0.8006\n",
      "    eval/precision_class_0  0.9963\n",
      "    eval/precision_class_1  0.6803\n",
      "    eval/recall_class_0     0.9419\n",
      "    eval/recall_class_1     0.9727\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1169, 'grad_norm': 0.18280668556690216, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1141, 'grad_norm': 0.2778947651386261, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1077, 'grad_norm': 0.1714838594198227, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1144, 'grad_norm': 0.20386922359466553, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:31<00:26, 14.61it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10479839146137238, 'eval_accuracy': 0.94236559647652, 'eval_precision': 0.6663103128786259, 'eval_recall': 0.9791579388353582, 'eval_f1': 0.792993765638916, 'eval_precision_class_0': 0.9971836168586713, 'eval_precision_class_1': 0.6663103128786259, 'eval_recall_class_0': 0.9376904752398824, 'eval_recall_class_1': 0.9791579388353582, 'eval_f1_class_0': 0.9665224041317155, 'eval_f1_class_1': 0.792993765638916, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6093, 'eval_samples_per_second': 310.203, 'eval_steps_per_second': 9.848, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.61it/s]\n",
      "100% 6/6 [00:00<00:00, 38.36it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1048\n",
      "  eval/accuracy             0.9424\n",
      "  eval/precision            0.6663\n",
      "  eval/recall               0.9792\n",
      "  eval/f1                   0.7930\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9665\n",
      "    eval/f1_class_1         0.7930\n",
      "    eval/precision_class_0  0.9972\n",
      "    eval/precision_class_1  0.6663\n",
      "    eval/recall_class_0     0.9377\n",
      "    eval/recall_class_1     0.9792\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1136, 'grad_norm': 0.24673980474472046, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1091, 'grad_norm': 0.33049020171165466, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1063, 'grad_norm': 0.478354811668396, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.106, 'grad_norm': 0.12442687898874283, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.35it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10338945686817169, 'eval_accuracy': 0.9447626019908134, 'eval_precision': 0.6758376660889659, 'eval_recall': 0.9802052785923754, 'eval_f1': 0.8000512908189434, 'eval_precision_class_0': 0.9973320534718595, 'eval_precision_class_1': 0.6758376660889659, 'eval_recall_class_0': 0.9402589797846714, 'eval_recall_class_1': 0.9802052785923754, 'eval_f1_class_0': 0.967954953350413, 'eval_f1_class_1': 0.8000512908189434, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.63, 'eval_samples_per_second': 300.006, 'eval_steps_per_second': 9.524, 'epoch': 6.0}\n",
      " 60% 468/780 [00:38<00:21, 14.35it/s]\n",
      "100% 6/6 [00:00<00:00, 38.45it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1034\n",
      "  eval/accuracy             0.9448\n",
      "  eval/precision            0.6758\n",
      "  eval/recall               0.9802\n",
      "  eval/f1                   0.8001\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9680\n",
      "    eval/f1_class_1         0.8001\n",
      "    eval/precision_class_0  0.9973\n",
      "    eval/precision_class_1  0.6758\n",
      "    eval/recall_class_0     0.9403\n",
      "    eval/recall_class_1     0.9802\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1031, 'grad_norm': 0.28418293595314026, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0994, 'grad_norm': 0.20016033947467804, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1042, 'grad_norm': 0.18803688883781433, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.1014, 'grad_norm': 0.17790696024894714, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.54it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.1019400954246521, 'eval_accuracy': 0.9473603419570428, 'eval_precision': 0.6867752825480699, 'eval_recall': 0.9801005446166736, 'eval_f1': 0.807629239665142, 'eval_precision_class_0': 0.9973263160857267, 'eval_precision_class_1': 0.6867752825480699, 'eval_recall_class_0': 0.9432001171131605, 'eval_recall_class_1': 0.9801005446166736, 'eval_f1_class_0': 0.9695083581844546, 'eval_f1_class_1': 0.807629239665142, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6126, 'eval_samples_per_second': 308.53, 'eval_steps_per_second': 9.795, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.54it/s]\n",
      "100% 6/6 [00:00<00:00, 38.36it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1019\n",
      "  eval/accuracy             0.9474\n",
      "  eval/precision            0.6868\n",
      "  eval/recall               0.9801\n",
      "  eval/f1                   0.8076\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9695\n",
      "    eval/f1_class_1         0.8076\n",
      "    eval/precision_class_0  0.9973\n",
      "    eval/precision_class_1  0.6868\n",
      "    eval/recall_class_0     0.9432\n",
      "    eval/recall_class_1     0.9801\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1019, 'grad_norm': 0.1268845647573471, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.1021, 'grad_norm': 0.18917179107666016, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0973, 'grad_norm': 0.11767955124378204, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0986, 'grad_norm': 0.13255934417247772, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09930044412612915, 'eval_accuracy': 0.9494975734747133, 'eval_precision': 0.6963713583190523, 'eval_recall': 0.978843736908253, 'eval_f1': 0.8137925029387435, 'eval_precision_class_0': 0.9971656283324914, 'eval_precision_class_1': 0.6963713583190523, 'eval_recall_class_0': 0.9457686216579497, 'eval_recall_class_1': 0.978843736908253, 'eval_f1_class_0': 0.9707873149874666, 'eval_f1_class_1': 0.8137925029387435, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6181, 'eval_samples_per_second': 305.757, 'eval_steps_per_second': 9.707, 'epoch': 8.0}\n",
      " 80% 624/780 [00:51<00:10, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.38it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0993\n",
      "  eval/accuracy             0.9495\n",
      "  eval/precision            0.6964\n",
      "  eval/recall               0.9788\n",
      "  eval/f1                   0.8138\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9708\n",
      "    eval/f1_class_1         0.8138\n",
      "    eval/precision_class_0  0.9972\n",
      "    eval/precision_class_1  0.6964\n",
      "    eval/recall_class_0     0.9458\n",
      "    eval/recall_class_1     0.9788\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0935, 'grad_norm': 0.24215564131736755, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.096, 'grad_norm': 0.26886048913002014, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0975, 'grad_norm': 0.13774235546588898, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.097, 'grad_norm': 0.12610800564289093, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.46it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09834104776382446, 'eval_accuracy': 0.9522606241660664, 'eval_precision': 0.7092360319270239, 'eval_recall': 0.9771679932970255, 'eval_f1': 0.821917808219178, 'eval_precision_class_0': 0.9969524981127855, 'eval_precision_class_1': 0.7092360319270239, 'eval_recall_class_0': 0.9490957000838424, 'eval_recall_class_1': 0.9771679932970255, 'eval_f1_class_0': 0.9724356570649395, 'eval_f1_class_1': 0.821917808219178, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6177, 'eval_samples_per_second': 305.969, 'eval_steps_per_second': 9.713, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.46it/s]\n",
      "100% 6/6 [00:00<00:00, 38.54it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0983\n",
      "  eval/accuracy             0.9523\n",
      "  eval/precision            0.7092\n",
      "  eval/recall               0.9772\n",
      "  eval/f1                   0.8219\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9724\n",
      "    eval/f1_class_1         0.8219\n",
      "    eval/precision_class_0  0.9970\n",
      "    eval/precision_class_1  0.7092\n",
      "    eval/recall_class_0     0.9491\n",
      "    eval/recall_class_1     0.9772\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0947, 'grad_norm': 0.13464471697807312, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0903, 'grad_norm': 0.20595400035381317, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0954, 'grad_norm': 0.15718761086463928, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.1012, 'grad_norm': 0.1450951099395752, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.58it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09796624630689621, 'eval_accuracy': 0.9498400028338981, 'eval_precision': 0.6977021784541928, 'eval_recall': 0.9794721407624634, 'eval_f1': 0.8149180899268038, 'eval_precision_class_0': 0.9972504734516378, 'eval_precision_class_1': 0.6977021784541928, 'eval_recall_class_0': 0.9460747128731318, 'eval_recall_class_1': 0.9794721407624634, 'eval_f1_class_0': 0.9709887588269843, 'eval_f1_class_1': 0.8149180899268038, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6149, 'eval_samples_per_second': 307.345, 'eval_steps_per_second': 9.757, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.58it/s]\n",
      "100% 6/6 [00:00<00:00, 38.58it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0980\n",
      "  eval/accuracy             0.9498\n",
      "  eval/precision            0.6977\n",
      "  eval/recall               0.9795\n",
      "  eval/f1                   0.8149\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9710\n",
      "    eval/f1_class_1         0.8149\n",
      "    eval/precision_class_0  0.9973\n",
      "    eval/precision_class_1  0.6977\n",
      "    eval/recall_class_0     0.9461\n",
      "    eval/recall_class_1     0.9795\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 64.946, 'train_samples_per_second': 192.006, 'train_steps_per_second': 12.01, 'train_loss': 0.1589810346945738, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 12.01it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 64.9460\n",
      "train_samples_per_second.......................... 192.0060\n",
      "train_steps_per_second............................ 12.0100\n",
      "total_flos........................................ 3303619240796160.0000\n",
      "train_loss........................................ 0.1590\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.35it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0983\n",
      "  eval/accuracy             0.9523\n",
      "  eval/precision            0.7092\n",
      "  eval/recall               0.9772\n",
      "  eval/f1                   0.8219\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9724\n",
      "    eval/f1_class_1         0.8219\n",
      "    eval/precision_class_0  0.9970\n",
      "    eval/precision_class_1  0.7092\n",
      "    eval/recall_class_0     0.9491\n",
      "    eval/recall_class_1     0.9772\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.43it/s]\n",
      "eval_loss......................................... 0.0983\n",
      "eval_accuracy..................................... 0.9523\n",
      "eval_precision.................................... 0.7092\n",
      "eval_recall....................................... 0.9772\n",
      "eval_f1........................................... 0.8219\n",
      "eval_precision_class_0............................ 0.9970\n",
      "eval_precision_class_1............................ 0.7092\n",
      "eval_recall_class_0............................... 0.9491\n",
      "eval_recall_class_1............................... 0.9772\n",
      "eval_f1_class_0................................... 0.9724\n",
      "eval_f1_class_1................................... 0.8219\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6167\n",
      "eval_samples_per_second........................... 306.4640\n",
      "eval_steps_per_second............................. 9.7290\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.61it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.1009\n",
      "eval_model_preparation_time....................... 0.0045\n",
      "eval_accuracy..................................... 0.9602\n",
      "eval_precision.................................... 0.7450\n",
      "eval_recall....................................... 0.9756\n",
      "eval_f1........................................... 0.8448\n",
      "eval_precision_class_0............................ 0.9968\n",
      "eval_precision_class_1............................ 0.7450\n",
      "eval_recall_class_0............................... 0.9583\n",
      "eval_recall_class_1............................... 0.9756\n",
      "eval_f1_class_0................................... 0.9772\n",
      "eval_f1_class_1................................... 0.8448\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8040\n",
      "eval_samples_per_second........................... 304.7360\n",
      "eval_steps_per_second............................. 19.9010\n",
      "100% 16/16 [00:00<00:00, 24.68it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9968    0.9583    0.9772     99651\n",
      "   EDU Start (1)     0.7450    0.9756    0.8448     12438\n",
      "\n",
      "        accuracy                         0.9602    112089\n",
      "       macro avg     0.8709    0.9669    0.9110    112089\n",
      "    weighted avg     0.9689    0.9602    0.9625    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9523               0.9602              \n",
      "Precision                      0.7092               0.7450              \n",
      "Recall                         0.9772               0.9756              \n",
      "F1                             0.8219               0.8448              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-aafxlf0l-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-aafxlf0l-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-aafxlf0l-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-aafxlf0l-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run-aafxlf0l-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.95226\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.82192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.97244\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.82192\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09834\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.70924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.70924\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.97717\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw1.5_r32_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/aafxlf0l\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_023724-aafxlf0l/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp1_class_weight \\\n",
    "  --class_1_weight_multiplier 1.5 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp1_cw1_5_rank_32_alpha_32_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "blJGwhtcYjYY",
    "outputId": "df74859e-9a3e-4eac-9df6-3c809fe589a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_024531-v6yc2jzz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r16_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/v6yc2jzz\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r16_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    2.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6477, 'grad_norm': 0.6983062028884888, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5787, 'grad_norm': 0.6056510210037231, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.436, 'grad_norm': 1.0717614889144897, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.61it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.20684927701950073, 'eval_accuracy': 0.9249961624295954, 'eval_precision': 0.6146341463414634, 'eval_recall': 0.8973607038123167, 'eval_f1': 0.7295640326975477, 'eval_precision_class_0': 0.9861482141090333, 'eval_precision_class_1': 0.6146341463414634, 'eval_recall_class_0': 0.9285077387844186, 'eval_recall_class_1': 0.8973607038123167, 'eval_f1_class_0': 0.9564603468366577, 'eval_f1_class_1': 0.7295640326975477, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6033, 'eval_samples_per_second': 313.288, 'eval_steps_per_second': 9.946, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.61it/s]\n",
      "100% 6/6 [00:00<00:00, 37.58it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2068\n",
      "  eval/accuracy             0.9250\n",
      "  eval/precision            0.6146\n",
      "  eval/recall               0.8974\n",
      "  eval/f1                   0.7296\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9565\n",
      "    eval/f1_class_1         0.7296\n",
      "    eval/precision_class_0  0.9861\n",
      "    eval/precision_class_1  0.6146\n",
      "    eval/recall_class_0     0.9285\n",
      "    eval/recall_class_1     0.8974\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2542, 'grad_norm': 0.5198289155960083, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1907, 'grad_norm': 0.45613715052604675, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1648, 'grad_norm': 0.6329065561294556, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1531, 'grad_norm': 0.881271243095398, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13798002898693085, 'eval_accuracy': 0.962072996493051, 'eval_precision': 0.792358803986711, 'eval_recall': 0.8992459153749476, 'eval_f1': 0.842425431711146, 'eval_precision_class_0': 0.9869741242738954, 'eval_precision_class_1': 0.792358803986711, 'eval_recall_class_0': 0.9700562941669661, 'eval_recall_class_1': 0.8992459153749476, 'eval_f1_class_0': 0.9784420849161711, 'eval_f1_class_1': 0.842425431711146, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.605, 'eval_samples_per_second': 312.397, 'eval_steps_per_second': 9.917, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 38.61it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1380\n",
      "  eval/accuracy             0.9621\n",
      "  eval/precision            0.7924\n",
      "  eval/recall               0.8992\n",
      "  eval/f1                   0.8424\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9784\n",
      "    eval/f1_class_1         0.8424\n",
      "    eval/precision_class_0  0.9870\n",
      "    eval/precision_class_1  0.7924\n",
      "    eval/recall_class_0     0.9701\n",
      "    eval/recall_class_1     0.8992\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.141, 'grad_norm': 0.31247392296791077, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1407, 'grad_norm': 0.15157060325145721, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1345, 'grad_norm': 0.2679063677787781, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1271, 'grad_norm': 0.23540784418582916, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:18<00:37, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11424463987350464, 'eval_accuracy': 0.9593099458016979, 'eval_precision': 0.7559134373427278, 'eval_recall': 0.9438625890238793, 'eval_f1': 0.839496972519795, 'eval_precision_class_0': 0.9926340236645732, 'eval_precision_class_1': 0.7559134373427278, 'eval_recall_class_0': 0.9612728071226095, 'eval_recall_class_1': 0.9438625890238793, 'eval_f1_class_0': 0.9767017335100198, 'eval_f1_class_1': 0.839496972519795, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5982, 'eval_samples_per_second': 315.956, 'eval_steps_per_second': 10.03, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1142\n",
      "  eval/accuracy             0.9593\n",
      "  eval/precision            0.7559\n",
      "  eval/recall               0.9439\n",
      "  eval/f1                   0.8395\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9767\n",
      "    eval/f1_class_1         0.8395\n",
      "    eval/precision_class_0  0.9926\n",
      "    eval/precision_class_1  0.7559\n",
      "    eval/recall_class_0     0.9613\n",
      "    eval/recall_class_1     0.9439\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.12, 'grad_norm': 0.4605804979801178, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1226, 'grad_norm': 0.34556078910827637, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.115, 'grad_norm': 0.167217418551445, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1198, 'grad_norm': 0.3382059931755066, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.35it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10620231926441193, 'eval_accuracy': 0.9619785332215518, 'eval_precision': 0.7698737632207437, 'eval_recall': 0.9453288646837034, 'eval_f1': 0.8486273034975554, 'eval_precision_class_0': 0.9928458850133626, 'eval_precision_class_1': 0.7698737632207437, 'eval_recall_class_0': 0.9640941696277665, 'eval_recall_class_1': 0.9453288646837034, 'eval_f1_class_0': 0.978258814632763, 'eval_f1_class_1': 0.8486273034975554, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6053, 'eval_samples_per_second': 312.233, 'eval_steps_per_second': 9.912, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.35it/s]\n",
      "100% 6/6 [00:00<00:00, 38.58it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1062\n",
      "  eval/accuracy             0.9620\n",
      "  eval/precision            0.7699\n",
      "  eval/recall               0.9453\n",
      "  eval/f1                   0.8486\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9783\n",
      "    eval/f1_class_1         0.8486\n",
      "    eval/precision_class_0  0.9928\n",
      "    eval/precision_class_1  0.7699\n",
      "    eval/recall_class_0     0.9641\n",
      "    eval/recall_class_1     0.9453\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1115, 'grad_norm': 0.15296567976474762, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1111, 'grad_norm': 0.2725920081138611, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1072, 'grad_norm': 0.16164857149124146, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1098, 'grad_norm': 0.20377591252326965, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:31<00:27, 14.45it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10427805036306381, 'eval_accuracy': 0.9590619797140124, 'eval_precision': 0.7478197082076778, 'eval_recall': 0.9609342270632594, 'eval_f1': 0.841087225558051, 'eval_precision_class_0': 0.9948494890914112, 'eval_precision_class_1': 0.7478197082076778, 'eval_recall_class_0': 0.9588240774011525, 'eval_recall_class_1': 0.9609342270632594, 'eval_f1_class_0': 0.9765046319827054, 'eval_f1_class_1': 0.841087225558051, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.608, 'eval_samples_per_second': 310.84, 'eval_steps_per_second': 9.868, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.45it/s]\n",
      "100% 6/6 [00:00<00:00, 38.72it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1043\n",
      "  eval/accuracy             0.9591\n",
      "  eval/precision            0.7478\n",
      "  eval/recall               0.9609\n",
      "  eval/f1                   0.8411\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9765\n",
      "    eval/f1_class_1         0.8411\n",
      "    eval/precision_class_0  0.9948\n",
      "    eval/precision_class_1  0.7478\n",
      "    eval/recall_class_0     0.9588\n",
      "    eval/recall_class_1     0.9609\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1104, 'grad_norm': 0.23085030913352966, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1055, 'grad_norm': 0.3084256649017334, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1033, 'grad_norm': 0.5119614005088806, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1034, 'grad_norm': 0.1758674532175064, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.51it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10197372734546661, 'eval_accuracy': 0.963466329747665, 'eval_precision': 0.7737065309584393, 'eval_recall': 0.9553833263510683, 'eval_f1': 0.8550004686474834, 'eval_precision_class_0': 0.9941562984403078, 'eval_precision_class_1': 0.7737065309584393, 'eval_recall_class_0': 0.9644934190388735, 'eval_recall_class_1': 0.9553833263510683, 'eval_f1_class_0': 0.9791002431775195, 'eval_f1_class_1': 0.8550004686474834, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6102, 'eval_samples_per_second': 309.737, 'eval_steps_per_second': 9.833, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.51it/s]\n",
      "100% 6/6 [00:00<00:00, 38.61it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1020\n",
      "  eval/accuracy             0.9635\n",
      "  eval/precision            0.7737\n",
      "  eval/recall               0.9554\n",
      "  eval/f1                   0.8550\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9791\n",
      "    eval/f1_class_1         0.8550\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7737\n",
      "    eval/recall_class_0     0.9645\n",
      "    eval/recall_class_1     0.9554\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0994, 'grad_norm': 0.23820656538009644, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0976, 'grad_norm': 0.2233343869447708, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1004, 'grad_norm': 0.17043684422969818, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0981, 'grad_norm': 0.1692165732383728, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.52it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09876324236392975, 'eval_accuracy': 0.9659577985334578, 'eval_precision': 0.7872597189897422, 'eval_recall': 0.9565354000837872, 'eval_f1': 0.8636814979431652, 'eval_precision_class_0': 0.9943219133099825, 'eval_precision_class_1': 0.7872597189897422, 'eval_recall_class_0': 0.9671550817795878, 'eval_recall_class_1': 0.9565354000837872, 'eval_f1_class_0': 0.9805503646384985, 'eval_f1_class_1': 0.8636814979431652, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6213, 'eval_samples_per_second': 304.211, 'eval_steps_per_second': 9.657, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.52it/s]\n",
      "100% 6/6 [00:00<00:00, 38.49it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0988\n",
      "  eval/accuracy             0.9660\n",
      "  eval/precision            0.7873\n",
      "  eval/recall               0.9565\n",
      "  eval/f1                   0.8637\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9806\n",
      "    eval/f1_class_1         0.8637\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7873\n",
      "    eval/recall_class_0     0.9672\n",
      "    eval/recall_class_1     0.9565\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1004, 'grad_norm': 0.23381522297859192, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0981, 'grad_norm': 0.1956336498260498, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0963, 'grad_norm': 0.1406327784061432, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0951, 'grad_norm': 0.1757872998714447, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.43it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09485414624214172, 'eval_accuracy': 0.9670677419735739, 'eval_precision': 0.7940485512920908, 'eval_recall': 0.9558022622538752, 'eval_f1': 0.8674492657193099, 'eval_precision_class_0': 0.9942346576315646, 'eval_precision_class_1': 0.7940485512920908, 'eval_recall_class_0': 0.9684992214636483, 'eval_recall_class_1': 0.9558022622538752, 'eval_f1_class_0': 0.9811982175721499, 'eval_f1_class_1': 0.8674492657193099, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6076, 'eval_samples_per_second': 311.063, 'eval_steps_per_second': 9.875, 'epoch': 8.0}\n",
      " 80% 624/780 [00:52<00:10, 14.43it/s]\n",
      "100% 6/6 [00:00<00:00, 38.51it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0949\n",
      "  eval/accuracy             0.9671\n",
      "  eval/precision            0.7940\n",
      "  eval/recall               0.9558\n",
      "  eval/f1                   0.8674\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9812\n",
      "    eval/f1_class_1         0.8674\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7940\n",
      "    eval/recall_class_0     0.9685\n",
      "    eval/recall_class_1     0.9558\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0917, 'grad_norm': 0.39230531454086304, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0915, 'grad_norm': 0.18325065076351166, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0959, 'grad_norm': 0.15746937692165375, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0941, 'grad_norm': 0.13550511002540588, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.39it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09339301288127899, 'eval_accuracy': 0.9681776854136901, 'eval_precision': 0.8010719620419998, 'eval_recall': 0.9548596564725597, 'eval_f1': 0.8712313058435663, 'eval_precision_class_0': 0.9941206962405195, 'eval_precision_class_1': 0.8010719620419998, 'eval_recall_class_0': 0.9698699777751161, 'eval_recall_class_1': 0.9548596564725597, 'eval_f1_class_0': 0.9818456170132503, 'eval_f1_class_1': 0.8712313058435663, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6167, 'eval_samples_per_second': 306.468, 'eval_steps_per_second': 9.729, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.39it/s]\n",
      "100% 6/6 [00:00<00:00, 38.62it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0934\n",
      "  eval/accuracy             0.9682\n",
      "  eval/precision            0.8011\n",
      "  eval/recall               0.9549\n",
      "  eval/f1                   0.8712\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9818\n",
      "    eval/f1_class_1         0.8712\n",
      "    eval/precision_class_0  0.9941\n",
      "    eval/precision_class_1  0.8011\n",
      "    eval/recall_class_0     0.9699\n",
      "    eval/recall_class_1     0.9549\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0923, 'grad_norm': 0.1579422652721405, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0878, 'grad_norm': 0.20088976621627808, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0942, 'grad_norm': 0.1791282594203949, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0969, 'grad_norm': 0.21051909029483795, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.70it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09386163204908371, 'eval_accuracy': 0.9675400583310702, 'eval_precision': 0.7960979008797143, 'eval_recall': 0.9572685379136993, 'eval_f1': 0.8692757620428931, 'eval_precision_class_0': 0.9944268385968746, 'eval_precision_class_1': 0.7960979008797143, 'eval_recall_class_0': 0.9688452376199411, 'eval_recall_class_1': 0.9572685379136993, 'eval_f1_class_0': 0.981469372897694, 'eval_f1_class_1': 0.8692757620428931, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6083, 'eval_samples_per_second': 310.719, 'eval_steps_per_second': 9.864, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.70it/s]\n",
      "100% 6/6 [00:00<00:00, 38.78it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0939\n",
      "  eval/accuracy             0.9675\n",
      "  eval/precision            0.7961\n",
      "  eval/recall               0.9573\n",
      "  eval/f1                   0.8693\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9815\n",
      "    eval/f1_class_1         0.8693\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7961\n",
      "    eval/recall_class_0     0.9688\n",
      "    eval/recall_class_1     0.9573\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.1862, 'train_samples_per_second': 191.298, 'train_steps_per_second': 11.966, 'train_loss': 0.1496914356182783, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.97it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.1862\n",
      "train_samples_per_second.......................... 191.2980\n",
      "train_steps_per_second............................ 11.9660\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1497\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.63it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0934\n",
      "  eval/accuracy             0.9682\n",
      "  eval/precision            0.8011\n",
      "  eval/recall               0.9549\n",
      "  eval/f1                   0.8712\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9818\n",
      "    eval/f1_class_1         0.8712\n",
      "    eval/precision_class_0  0.9941\n",
      "    eval/precision_class_1  0.8011\n",
      "    eval/recall_class_0     0.9699\n",
      "    eval/recall_class_1     0.9549\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.72it/s]\n",
      "eval_loss......................................... 0.0934\n",
      "eval_accuracy..................................... 0.9682\n",
      "eval_precision.................................... 0.8011\n",
      "eval_recall....................................... 0.9549\n",
      "eval_f1........................................... 0.8712\n",
      "eval_precision_class_0............................ 0.9941\n",
      "eval_precision_class_1............................ 0.8011\n",
      "eval_recall_class_0............................... 0.9699\n",
      "eval_recall_class_1............................... 0.9549\n",
      "eval_f1_class_0................................... 0.9818\n",
      "eval_f1_class_1................................... 0.8712\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.5991\n",
      "eval_samples_per_second........................... 315.4630\n",
      "eval_steps_per_second............................. 10.0150\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.92it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0711\n",
      "eval_model_preparation_time....................... 0.0044\n",
      "eval_accuracy..................................... 0.9735\n",
      "eval_precision.................................... 0.8321\n",
      "eval_recall....................................... 0.9536\n",
      "eval_f1........................................... 0.8887\n",
      "eval_precision_class_0............................ 0.9941\n",
      "eval_precision_class_1............................ 0.8321\n",
      "eval_recall_class_0............................... 0.9760\n",
      "eval_recall_class_1............................... 0.9536\n",
      "eval_f1_class_0................................... 0.9850\n",
      "eval_f1_class_1................................... 0.8887\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.7945\n",
      "eval_samples_per_second........................... 308.3540\n",
      "eval_steps_per_second............................. 20.1370\n",
      "100% 16/16 [00:00<00:00, 24.95it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9941    0.9760    0.9850     99651\n",
      "   EDU Start (1)     0.8321    0.9536    0.8887     12438\n",
      "\n",
      "        accuracy                         0.9735    112089\n",
      "       macro avg     0.9131    0.9648    0.9368    112089\n",
      "    weighted avg     0.9761    0.9735    0.9743    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9682               0.9735              \n",
      "Precision                      0.8011               0.8321              \n",
      "Recall                         0.9549               0.9536              \n",
      "F1                             0.8712               0.8887              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-v6yc2jzz-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-v6yc2jzz-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-v6yc2jzz-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-v6yc2jzz-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-v6yc2jzz-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96818\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.87123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98185\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.87123\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09339\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.80107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99412\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.80107\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95486\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r16_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/v6yc2jzz\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_024531-v6yc2jzz/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "BEST_CW=0.5\n",
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp2_classweight_rank \\\n",
    "  --class_1_weight_multiplier  $BEST_CW \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RKPD3wRYjj-",
    "outputId": "4c7c1963-a6d5-4e5d-fe87-5934b0be3718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_025134-xdqxzb97\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r8_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/xdqxzb97\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r8_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              8\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    4.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 398,594 || all params: 109,189,636 || trainable%: 0.3650\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6693, 'grad_norm': 0.7351037859916687, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5981, 'grad_norm': 0.6288608312606812, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4525, 'grad_norm': 0.934819221496582, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.64it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.2107541859149933, 'eval_accuracy': 0.915656106460107, 'eval_precision': 0.5791587123954973, 'eval_recall': 0.9214495182237118, 'eval_f1': 0.711265612999717, 'eval_precision_class_0': 0.9892083225416559, 'eval_precision_class_1': 0.5791587123954973, 'eval_recall_class_0': 0.9149199504930731, 'eval_recall_class_1': 0.9214495182237118, 'eval_f1_class_0': 0.9506149793624127, 'eval_f1_class_1': 0.711265612999717, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6043, 'eval_samples_per_second': 312.773, 'eval_steps_per_second': 9.929, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:47, 14.64it/s]\n",
      "100% 6/6 [00:00<00:00, 37.60it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2108\n",
      "  eval/accuracy             0.9157\n",
      "  eval/precision            0.5792\n",
      "  eval/recall               0.9214\n",
      "  eval/f1                   0.7113\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9506\n",
      "    eval/f1_class_1         0.7113\n",
      "    eval/precision_class_0  0.9892\n",
      "    eval/precision_class_1  0.5792\n",
      "    eval/recall_class_0     0.9149\n",
      "    eval/recall_class_1     0.9214\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2516, 'grad_norm': 1.3583375215530396, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1919, 'grad_norm': 0.5071184039115906, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1668, 'grad_norm': 0.700567901134491, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1531, 'grad_norm': 1.1733994483947754, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.58it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13176578283309937, 'eval_accuracy': 0.9598649175217561, 'eval_precision': 0.7715269804822044, 'eval_recall': 0.9149560117302052, 'eval_f1': 0.8371424464568061, 'eval_precision_class_0': 0.9889322029277867, 'eval_precision_class_1': 0.7715269804822044, 'eval_recall_class_0': 0.9655713924488628, 'eval_recall_class_1': 0.9149560117302052, 'eval_f1_class_0': 0.9771121899977779, 'eval_f1_class_1': 0.8371424464568061, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6085, 'eval_samples_per_second': 310.62, 'eval_steps_per_second': 9.861, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.58it/s]\n",
      "100% 6/6 [00:00<00:00, 38.65it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1318\n",
      "  eval/accuracy             0.9599\n",
      "  eval/precision            0.7715\n",
      "  eval/recall               0.9150\n",
      "  eval/f1                   0.8371\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9771\n",
      "    eval/f1_class_1         0.8371\n",
      "    eval/precision_class_0  0.9889\n",
      "    eval/precision_class_1  0.7715\n",
      "    eval/recall_class_0     0.9656\n",
      "    eval/recall_class_1     0.9150\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.14, 'grad_norm': 0.28867924213409424, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1394, 'grad_norm': 0.25847187638282776, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1336, 'grad_norm': 0.2432096302509308, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.13, 'grad_norm': 0.40925148129463196, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11312589794397354, 'eval_accuracy': 0.9608449739635608, 'eval_precision': 0.7639783124364622, 'eval_recall': 0.9444909928780897, 'eval_f1': 0.8446983889097041, 'eval_precision_class_0': 0.992728270563216, 'eval_precision_class_1': 0.7639783124364622, 'eval_recall_class_0': 0.9629230380218522, 'eval_recall_class_1': 0.9444909928780897, 'eval_f1_class_0': 0.9775985299879751, 'eval_f1_class_1': 0.8446983889097041, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5997, 'eval_samples_per_second': 315.172, 'eval_steps_per_second': 10.005, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 38.71it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1131\n",
      "  eval/accuracy             0.9608\n",
      "  eval/precision            0.7640\n",
      "  eval/recall               0.9445\n",
      "  eval/f1                   0.8447\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9776\n",
      "    eval/f1_class_1         0.8447\n",
      "    eval/precision_class_0  0.9927\n",
      "    eval/precision_class_1  0.7640\n",
      "    eval/recall_class_0     0.9629\n",
      "    eval/recall_class_1     0.9445\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1188, 'grad_norm': 0.6610664129257202, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1213, 'grad_norm': 0.29747119545936584, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.115, 'grad_norm': 0.2464645355939865, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1195, 'grad_norm': 0.351479709148407, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10548483580350876, 'eval_accuracy': 0.9618014145874907, 'eval_precision': 0.7663488313222513, 'eval_recall': 0.9511939673229995, 'eval_f1': 0.8488247114351138, 'eval_precision_class_0': 0.9936022405887037, 'eval_precision_class_1': 0.7663488313222513, 'eval_recall_class_0': 0.963149279354813, 'eval_recall_class_1': 0.9511939673229995, 'eval_f1_class_0': 0.9781387899634407, 'eval_f1_class_1': 0.8488247114351138, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6107, 'eval_samples_per_second': 309.474, 'eval_steps_per_second': 9.825, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.72it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1055\n",
      "  eval/accuracy             0.9618\n",
      "  eval/precision            0.7663\n",
      "  eval/recall               0.9512\n",
      "  eval/f1                   0.8488\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9781\n",
      "    eval/f1_class_1         0.8488\n",
      "    eval/precision_class_0  0.9936\n",
      "    eval/precision_class_1  0.7663\n",
      "    eval/recall_class_0     0.9631\n",
      "    eval/recall_class_1     0.9512\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1107, 'grad_norm': 0.21865077316761017, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1103, 'grad_norm': 0.4765234887599945, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1069, 'grad_norm': 0.21215391159057617, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1088, 'grad_norm': 0.21744361519813538, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:31<00:26, 14.65it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10136972367763519, 'eval_accuracy': 0.9619194936768648, 'eval_precision': 0.7642731756248433, 'eval_recall': 0.9575827398408043, 'eval_f1': 0.8500767049416578, 'eval_precision_class_0': 0.9944311525451696, 'eval_precision_class_1': 0.7642731756248433, 'eval_recall_class_0': 0.9624705553559308, 'eval_recall_class_1': 0.9575827398408043, 'eval_f1_class_0': 0.9781898598064477, 'eval_f1_class_1': 0.8500767049416578, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6059, 'eval_samples_per_second': 311.917, 'eval_steps_per_second': 9.902, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.65it/s]\n",
      "100% 6/6 [00:00<00:00, 38.51it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1014\n",
      "  eval/accuracy             0.9619\n",
      "  eval/precision            0.7643\n",
      "  eval/recall               0.9576\n",
      "  eval/f1                   0.8501\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9782\n",
      "    eval/f1_class_1         0.8501\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7643\n",
      "    eval/recall_class_0     0.9625\n",
      "    eval/recall_class_1     0.9576\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1101, 'grad_norm': 0.33791583776474, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1037, 'grad_norm': 0.29041358828544617, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1033, 'grad_norm': 0.3665896952152252, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1013, 'grad_norm': 0.18479537963867188, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.56it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10055231302976608, 'eval_accuracy': 0.9630176292080436, 'eval_precision': 0.7703067071115605, 'eval_recall': 0.9574780058651027, 'eval_f1': 0.8537542024654464, 'eval_precision_class_0': 0.9944246851869653, 'eval_precision_class_1': 0.7703067071115605, 'eval_recall_class_0': 0.9637215368440665, 'eval_recall_class_1': 0.9574780058651027, 'eval_f1_class_0': 0.9788324029142618, 'eval_f1_class_1': 0.8537542024654464, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.61, 'eval_samples_per_second': 309.836, 'eval_steps_per_second': 9.836, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.56it/s]\n",
      "100% 6/6 [00:00<00:00, 38.90it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1006\n",
      "  eval/accuracy             0.9630\n",
      "  eval/precision            0.7703\n",
      "  eval/recall               0.9575\n",
      "  eval/f1                   0.8538\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9788\n",
      "    eval/f1_class_1         0.8538\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7703\n",
      "    eval/recall_class_0     0.9637\n",
      "    eval/recall_class_1     0.9575\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0985, 'grad_norm': 0.3076312839984894, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0981, 'grad_norm': 0.23507176339626312, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1, 'grad_norm': 0.3005264103412628, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0978, 'grad_norm': 0.21306507289409637, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09776795655488968, 'eval_accuracy': 0.9647179680950301, 'eval_precision': 0.7791489361702127, 'eval_recall': 0.958839547549225, 'eval_f1': 0.8597051366325477, 'eval_precision_class_0': 0.9946119360013161, 'eval_precision_class_1': 0.7791489361702127, 'eval_recall_class_0': 0.9654649259392343, 'eval_recall_class_1': 0.958839547549225, 'eval_f1_class_0': 0.9798217179902755, 'eval_f1_class_1': 0.8597051366325477, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6094, 'eval_samples_per_second': 310.164, 'eval_steps_per_second': 9.846, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 38.54it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0978\n",
      "  eval/accuracy             0.9647\n",
      "  eval/precision            0.7791\n",
      "  eval/recall               0.9588\n",
      "  eval/f1                   0.8597\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9798\n",
      "    eval/f1_class_1         0.8597\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.7791\n",
      "    eval/recall_class_0     0.9655\n",
      "    eval/recall_class_1     0.9588\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1013, 'grad_norm': 0.2909484803676605, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0973, 'grad_norm': 0.23457053303718567, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0951, 'grad_norm': 0.2565326690673828, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0943, 'grad_norm': 0.37540584802627563, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.50it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09452122449874878, 'eval_accuracy': 0.9672684764255098, 'eval_precision': 0.7944038929440389, 'eval_recall': 0.9574780058651027, 'eval_f1': 0.8683510638297872, 'eval_precision_class_0': 0.9944521118869653, 'eval_precision_class_1': 0.7944038929440389, 'eval_recall_class_0': 0.9685125297773519, 'eval_recall_class_1': 0.9574780058651027, 'eval_f1_class_0': 0.9813109316217419, 'eval_f1_class_1': 0.8683510638297872, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6096, 'eval_samples_per_second': 310.015, 'eval_steps_per_second': 9.842, 'epoch': 8.0}\n",
      " 80% 624/780 [00:51<00:10, 14.50it/s]\n",
      "100% 6/6 [00:00<00:00, 38.80it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0945\n",
      "  eval/accuracy             0.9673\n",
      "  eval/precision            0.7944\n",
      "  eval/recall               0.9575\n",
      "  eval/f1                   0.8684\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9813\n",
      "    eval/f1_class_1         0.8684\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7944\n",
      "    eval/recall_class_0     0.9685\n",
      "    eval/recall_class_1     0.9575\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.09, 'grad_norm': 0.5107865333557129, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0916, 'grad_norm': 0.21552406251430511, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0958, 'grad_norm': 0.22816824913024902, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0931, 'grad_norm': 0.19411593675613403, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.33it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09362621605396271, 'eval_accuracy': 0.9669968945199494, 'eval_precision': 0.7921100441214638, 'eval_recall': 0.9589442815249267, 'eval_f1': 0.8675794760032217, 'eval_precision_class_0': 0.9946396827567345, 'eval_precision_class_1': 0.7921100441214638, 'eval_recall_class_0': 0.9680201221703199, 'eval_recall_class_1': 0.9589442815249267, 'eval_f1_class_0': 0.98114938187508, 'eval_f1_class_1': 0.8675794760032217, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6171, 'eval_samples_per_second': 306.276, 'eval_steps_per_second': 9.723, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.33it/s]\n",
      "100% 6/6 [00:00<00:00, 38.48it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0936\n",
      "  eval/accuracy             0.9670\n",
      "  eval/precision            0.7921\n",
      "  eval/recall               0.9589\n",
      "  eval/f1                   0.8676\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9811\n",
      "    eval/f1_class_1         0.8676\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.7921\n",
      "    eval/recall_class_0     0.9680\n",
      "    eval/recall_class_1     0.9589\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0923, 'grad_norm': 0.22823648154735565, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0871, 'grad_norm': 0.28125035762786865, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0938, 'grad_norm': 0.21808232367038727, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0967, 'grad_norm': 0.24536007642745972, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.52it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09386749565601349, 'eval_accuracy': 0.9673983634238212, 'eval_precision': 0.7951639558145603, 'eval_recall': 0.9574780058651027, 'eval_f1': 0.8688049417913993, 'eval_precision_class_0': 0.9944529456771232, 'eval_precision_class_1': 0.7951639558145603, 'eval_recall_class_0': 0.9686589212280912, 'eval_recall_class_1': 0.9574780058651027, 'eval_f1_class_0': 0.9813864750257866, 'eval_f1_class_1': 0.8688049417913993, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6136, 'eval_samples_per_second': 308.011, 'eval_steps_per_second': 9.778, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.52it/s]\n",
      "100% 6/6 [00:00<00:00, 38.38it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0939\n",
      "  eval/accuracy             0.9674\n",
      "  eval/precision            0.7952\n",
      "  eval/recall               0.9575\n",
      "  eval/f1                   0.8688\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8688\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7952\n",
      "    eval/recall_class_0     0.9687\n",
      "    eval/recall_class_1     0.9575\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 64.9247, 'train_samples_per_second': 192.069, 'train_steps_per_second': 12.014, 'train_loss': 0.15073752785340333, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 12.02it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 64.9247\n",
      "train_samples_per_second.......................... 192.0690\n",
      "train_steps_per_second............................ 12.0140\n",
      "total_flos........................................ 3269726915665920.0000\n",
      "train_loss........................................ 0.1507\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.64it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0939\n",
      "  eval/accuracy             0.9674\n",
      "  eval/precision            0.7952\n",
      "  eval/recall               0.9575\n",
      "  eval/f1                   0.8688\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8688\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7952\n",
      "    eval/recall_class_0     0.9687\n",
      "    eval/recall_class_1     0.9575\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.43it/s]\n",
      "eval_loss......................................... 0.0939\n",
      "eval_accuracy..................................... 0.9674\n",
      "eval_precision.................................... 0.7952\n",
      "eval_recall....................................... 0.9575\n",
      "eval_f1........................................... 0.8688\n",
      "eval_precision_class_0............................ 0.9945\n",
      "eval_precision_class_1............................ 0.7952\n",
      "eval_recall_class_0............................... 0.9687\n",
      "eval_recall_class_1............................... 0.9575\n",
      "eval_f1_class_0................................... 0.9814\n",
      "eval_f1_class_1................................... 0.8688\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6152\n",
      "eval_samples_per_second........................... 307.2240\n",
      "eval_steps_per_second............................. 9.7530\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.86it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0733\n",
      "eval_model_preparation_time....................... 0.0045\n",
      "eval_accuracy..................................... 0.9728\n",
      "eval_precision.................................... 0.8260\n",
      "eval_recall....................................... 0.9563\n",
      "eval_f1........................................... 0.8864\n",
      "eval_precision_class_0............................ 0.9944\n",
      "eval_precision_class_1............................ 0.8260\n",
      "eval_recall_class_0............................... 0.9749\n",
      "eval_recall_class_1............................... 0.9563\n",
      "eval_f1_class_0................................... 0.9846\n",
      "eval_f1_class_1................................... 0.8864\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8004\n",
      "eval_samples_per_second........................... 306.1040\n",
      "eval_steps_per_second............................. 19.9900\n",
      "100% 16/16 [00:00<00:00, 24.84it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9944    0.9749    0.9846     99651\n",
      "   EDU Start (1)     0.8260    0.9563    0.8864     12438\n",
      "\n",
      "        accuracy                         0.9728    112089\n",
      "       macro avg     0.9102    0.9656    0.9355    112089\n",
      "    weighted avg     0.9758    0.9728    0.9737    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9674               0.9728              \n",
      "Precision                      0.7952               0.8260              \n",
      "Recall                         0.9575               0.9563              \n",
      "F1                             0.8688               0.8864              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-xdqxzb97-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-xdqxzb97-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-xdqxzb97-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-xdqxzb97-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run-xdqxzb97-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.9674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.8688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98139\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.8688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09387\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.79516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99445\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.79516\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95748\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r8_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/xdqxzb97\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_025134-xdqxzb97/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp2_classweight_rank \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 8 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_8_alpha_32_ep10_noES\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rWwAT4-Yjmd",
    "outputId": "f1e17c32-2033-4537-f320-bb321e9edd61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_025632-nn4wcedb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r32_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/nn4wcedb\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r32_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    1.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 1,283,330 || all params: 110,074,372 || trainable%: 1.1659\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6761, 'grad_norm': 0.7236098647117615, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6061, 'grad_norm': 0.6446497440338135, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4617, 'grad_norm': 0.429455429315567, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.21018333733081818, 'eval_accuracy': 0.9275939023958247, 'eval_precision': 0.6265935369107619, 'eval_recall': 0.8854210305823209, 'eval_f1': 0.7338541666666667, 'eval_precision_class_0': 0.9846341840246078, 'eval_precision_class_1': 0.6265935369107619, 'eval_recall_class_0': 0.9329527155614112, 'eval_recall_class_1': 0.8854210305823209, 'eval_f1_class_0': 0.958097008295863, 'eval_f1_class_1': 0.7338541666666667, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6057, 'eval_samples_per_second': 312.04, 'eval_steps_per_second': 9.906, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 37.48it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2102\n",
      "  eval/accuracy             0.9276\n",
      "  eval/precision            0.6266\n",
      "  eval/recall               0.8854\n",
      "  eval/f1                   0.7339\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9581\n",
      "    eval/f1_class_1         0.7339\n",
      "    eval/precision_class_0  0.9846\n",
      "    eval/precision_class_1  0.6266\n",
      "    eval/recall_class_0     0.9330\n",
      "    eval/recall_class_1     0.8854\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2676, 'grad_norm': 0.2888621985912323, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1948, 'grad_norm': 0.26313236355781555, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1671, 'grad_norm': 0.7586347460746765, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1544, 'grad_norm': 0.6958410143852234, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.1370193511247635, 'eval_accuracy': 0.9596169514340706, 'eval_precision': 0.77465041233417, 'eval_recall': 0.9051110180142439, 'eval_f1': 0.8348145285935085, 'eval_precision_class_0': 0.9876790012647383, 'eval_precision_class_1': 0.77465041233417, 'eval_recall_class_0': 0.9665428993492234, 'eval_recall_class_1': 0.9051110180142439, 'eval_f1_class_0': 0.9769966503894427, 'eval_f1_class_1': 0.8348145285935085, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6147, 'eval_samples_per_second': 307.487, 'eval_steps_per_second': 9.761, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 38.30it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1370\n",
      "  eval/accuracy             0.9596\n",
      "  eval/precision            0.7747\n",
      "  eval/recall               0.9051\n",
      "  eval/f1                   0.8348\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9770\n",
      "    eval/f1_class_1         0.8348\n",
      "    eval/precision_class_0  0.9877\n",
      "    eval/precision_class_1  0.7747\n",
      "    eval/recall_class_0     0.9665\n",
      "    eval/recall_class_1     0.9051\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1424, 'grad_norm': 0.2814274728298187, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1385, 'grad_norm': 0.1161680817604065, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1344, 'grad_norm': 0.15624156594276428, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1309, 'grad_norm': 0.44492095708847046, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:18<00:36, 14.89it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11420569568872452, 'eval_accuracy': 0.9608095502367485, 'eval_precision': 0.7659011354904807, 'eval_recall': 0.939568496020109, 'eval_f1': 0.8438925732561968, 'eval_precision_class_0': 0.9920932909449682, 'eval_precision_class_1': 0.7659011354904807, 'eval_recall_class_0': 0.9635086038248094, 'eval_recall_class_1': 0.939568496020109, 'eval_f1_class_0': 0.9775920387261422, 'eval_f1_class_1': 0.8438925732561968, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5941, 'eval_samples_per_second': 318.138, 'eval_steps_per_second': 10.1, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:36, 14.89it/s]\n",
      "100% 6/6 [00:00<00:00, 38.80it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1142\n",
      "  eval/accuracy             0.9608\n",
      "  eval/precision            0.7659\n",
      "  eval/recall               0.9396\n",
      "  eval/f1                   0.8439\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9776\n",
      "    eval/f1_class_1         0.8439\n",
      "    eval/precision_class_0  0.9921\n",
      "    eval/precision_class_1  0.7659\n",
      "    eval/recall_class_0     0.9635\n",
      "    eval/recall_class_1     0.9396\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1215, 'grad_norm': 0.39426782727241516, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1223, 'grad_norm': 0.1988343596458435, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1146, 'grad_norm': 0.13536672294139862, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.121, 'grad_norm': 0.17515140771865845, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.65it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10595329850912094, 'eval_accuracy': 0.9624744653969228, 'eval_precision': 0.7725017111567419, 'eval_recall': 0.9456430666108085, 'eval_f1': 0.8503484648709738, 'eval_precision_class_0': 0.9928905083492007, 'eval_precision_class_1': 0.7725017111567419, 'eval_recall_class_0': 0.9646131938622057, 'eval_recall_class_1': 0.9456430666108085, 'eval_f1_class_0': 0.9785476097258036, 'eval_f1_class_1': 0.8503484648709738, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6124, 'eval_samples_per_second': 308.641, 'eval_steps_per_second': 9.798, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:31, 14.65it/s]\n",
      "100% 6/6 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1060\n",
      "  eval/accuracy             0.9625\n",
      "  eval/precision            0.7725\n",
      "  eval/recall               0.9456\n",
      "  eval/f1                   0.8503\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9785\n",
      "    eval/f1_class_1         0.8503\n",
      "    eval/precision_class_0  0.9929\n",
      "    eval/precision_class_1  0.7725\n",
      "    eval/recall_class_0     0.9646\n",
      "    eval/recall_class_1     0.9456\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1124, 'grad_norm': 0.1716660112142563, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1116, 'grad_norm': 0.18260790407657623, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1083, 'grad_norm': 0.1340211182832718, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1104, 'grad_norm': 0.12496789544820786, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:31<00:26, 14.52it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10272691398859024, 'eval_accuracy': 0.9605379683311882, 'eval_precision': 0.7558964209137391, 'eval_recall': 0.9599916212819438, 'eval_f1': 0.845806034880502, 'eval_precision_class_0': 0.9947356090569573, 'eval_precision_class_1': 0.7558964209137391, 'eval_recall_class_0': 0.960607391437431, 'eval_recall_class_1': 0.9599916212819438, 'eval_f1_class_0': 0.9773736662514217, 'eval_f1_class_1': 0.845806034880502, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6193, 'eval_samples_per_second': 305.194, 'eval_steps_per_second': 9.689, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.52it/s]\n",
      "100% 6/6 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1027\n",
      "  eval/accuracy             0.9605\n",
      "  eval/precision            0.7559\n",
      "  eval/recall               0.9600\n",
      "  eval/f1                   0.8458\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9774\n",
      "    eval/f1_class_1         0.8458\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.7559\n",
      "    eval/recall_class_0     0.9606\n",
      "    eval/recall_class_1     0.9600\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1104, 'grad_norm': 0.16086764633655548, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1048, 'grad_norm': 0.15206505358219147, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1038, 'grad_norm': 0.4329206943511963, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1023, 'grad_norm': 0.1241547092795372, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.64it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10126230865716934, 'eval_accuracy': 0.9635726009281016, 'eval_precision': 0.7733231836251374, 'eval_recall': 0.9575827398408043, 'eval_f1': 0.8556455009124515, 'eval_precision_class_0': 0.9944418521669914, 'eval_precision_class_1': 0.7733231836251374, 'eval_recall_class_0': 0.9643337192744308, 'eval_recall_class_1': 0.9575827398408043, 'eval_f1_class_0': 0.9791563912517651, 'eval_f1_class_1': 0.8556455009124515, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6087, 'eval_samples_per_second': 310.509, 'eval_steps_per_second': 9.857, 'epoch': 6.0}\n",
      " 60% 468/780 [00:38<00:21, 14.64it/s]\n",
      "100% 6/6 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1013\n",
      "  eval/accuracy             0.9636\n",
      "  eval/precision            0.7733\n",
      "  eval/recall               0.9576\n",
      "  eval/f1                   0.8556\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9792\n",
      "    eval/f1_class_1         0.8556\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7733\n",
      "    eval/recall_class_0     0.9643\n",
      "    eval/recall_class_1     0.9576\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0997, 'grad_norm': 0.20984111726284027, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0985, 'grad_norm': 0.15599852800369263, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1001, 'grad_norm': 0.14688333868980408, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0985, 'grad_norm': 0.13286608457565308, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.43it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09746523201465607, 'eval_accuracy': 0.9661703408943311, 'eval_precision': 0.7888322240470222, 'eval_recall': 0.9558022622538752, 'eval_f1': 0.8643273192214803, 'eval_precision_class_0': 0.9942286652078774, 'eval_precision_class_1': 0.7888322240470222, 'eval_recall_class_0': 0.967487789622177, 'eval_recall_class_1': 0.9558022622538752, 'eval_f1_class_0': 0.9806759700797917, 'eval_f1_class_1': 0.8643273192214803, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6082, 'eval_samples_per_second': 310.744, 'eval_steps_per_second': 9.865, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.43it/s]\n",
      "100% 6/6 [00:00<00:00, 38.57it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0975\n",
      "  eval/accuracy             0.9662\n",
      "  eval/precision            0.7888\n",
      "  eval/recall               0.9558\n",
      "  eval/f1                   0.8643\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9807\n",
      "    eval/f1_class_1         0.8643\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7888\n",
      "    eval/recall_class_0     0.9675\n",
      "    eval/recall_class_1     0.9558\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1007, 'grad_norm': 0.18916167318820953, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.098, 'grad_norm': 0.1362985074520111, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0965, 'grad_norm': 0.13759812712669373, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0946, 'grad_norm': 0.18391580879688263, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09499219805002213, 'eval_accuracy': 0.9666072335250151, 'eval_precision': 0.7903560318008988, 'eval_recall': 0.9578969417679095, 'eval_f1': 0.8660984848484848, 'eval_precision_class_0': 0.994501962607875, 'eval_precision_class_1': 0.7903560318008988, 'eval_recall_class_0': 0.9677140309551376, 'eval_recall_class_1': 0.9578969417679095, 'eval_f1_class_0': 0.9809251440057197, 'eval_f1_class_1': 0.8660984848484848, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6237, 'eval_samples_per_second': 303.023, 'eval_steps_per_second': 9.62, 'epoch': 8.0}\n",
      " 80% 624/780 [00:51<00:10, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.54it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0950\n",
      "  eval/accuracy             0.9666\n",
      "  eval/precision            0.7904\n",
      "  eval/recall               0.9579\n",
      "  eval/f1                   0.8661\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9809\n",
      "    eval/f1_class_1         0.8661\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7904\n",
      "    eval/recall_class_0     0.9677\n",
      "    eval/recall_class_1     0.9579\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0915, 'grad_norm': 0.22478581964969635, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0915, 'grad_norm': 0.1313031166791916, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0967, 'grad_norm': 0.12636138498783112, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0944, 'grad_norm': 0.14088118076324463, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.36it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09345196932554245, 'eval_accuracy': 0.9677171769651313, 'eval_precision': 0.7973987430167597, 'eval_recall': 0.9567448680351907, 'eval_f1': 0.8698343172729004, 'eval_precision_class_0': 0.9943604659101771, 'eval_precision_class_1': 0.7973987430167597, 'eval_recall_class_0': 0.9691114038940126, 'eval_recall_class_1': 0.9567448680351907, 'eval_f1_class_0': 0.9815735910604284, 'eval_f1_class_1': 0.8698343172729004, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6065, 'eval_samples_per_second': 311.616, 'eval_steps_per_second': 9.893, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.36it/s]\n",
      "100% 6/6 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0935\n",
      "  eval/accuracy             0.9677\n",
      "  eval/precision            0.7974\n",
      "  eval/recall               0.9567\n",
      "  eval/f1                   0.8698\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9816\n",
      "    eval/f1_class_1         0.8698\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7974\n",
      "    eval/recall_class_0     0.9691\n",
      "    eval/recall_class_1     0.9567\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0922, 'grad_norm': 0.13420860469341278, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0873, 'grad_norm': 0.18701253831386566, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0946, 'grad_norm': 0.13965058326721191, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0969, 'grad_norm': 0.1731528341770172, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.45it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09406599402427673, 'eval_accuracy': 0.9677053690561939, 'eval_precision': 0.7971216746620148, 'eval_recall': 0.9571638039379975, 'eval_f1': 0.8698424784657117, 'eval_precision_class_0': 0.9944143996503878, 'eval_precision_class_1': 0.7971216746620148, 'eval_recall_class_0': 0.9690448623254947, 'eval_recall_class_1': 0.9571638039379975, 'eval_f1_class_0': 0.9815657331580898, 'eval_f1_class_1': 0.8698424784657117, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.623, 'eval_samples_per_second': 303.37, 'eval_steps_per_second': 9.631, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.45it/s]\n",
      "100% 6/6 [00:00<00:00, 38.11it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0941\n",
      "  eval/accuracy             0.9677\n",
      "  eval/precision            0.7971\n",
      "  eval/recall               0.9572\n",
      "  eval/f1                   0.8698\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9816\n",
      "    eval/f1_class_1         0.8698\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7971\n",
      "    eval/recall_class_0     0.9690\n",
      "    eval/recall_class_1     0.9572\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 64.9158, 'train_samples_per_second': 192.095, 'train_steps_per_second': 12.016, 'train_loss': 0.1525440780016092, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 12.02it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 64.9158\n",
      "train_samples_per_second.......................... 192.0950\n",
      "train_steps_per_second............................ 12.0160\n",
      "total_flos........................................ 3303619240796160.0000\n",
      "train_loss........................................ 0.1525\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.37it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0935\n",
      "  eval/accuracy             0.9677\n",
      "  eval/precision            0.7974\n",
      "  eval/recall               0.9567\n",
      "  eval/f1                   0.8698\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9816\n",
      "    eval/f1_class_1         0.8698\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7974\n",
      "    eval/recall_class_0     0.9691\n",
      "    eval/recall_class_1     0.9567\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.31it/s]\n",
      "eval_loss......................................... 0.0935\n",
      "eval_accuracy..................................... 0.9677\n",
      "eval_precision.................................... 0.7974\n",
      "eval_recall....................................... 0.9567\n",
      "eval_f1........................................... 0.8698\n",
      "eval_precision_class_0............................ 0.9944\n",
      "eval_precision_class_1............................ 0.7974\n",
      "eval_recall_class_0............................... 0.9691\n",
      "eval_recall_class_1............................... 0.9567\n",
      "eval_f1_class_0................................... 0.9816\n",
      "eval_f1_class_1................................... 0.8698\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6286\n",
      "eval_samples_per_second........................... 300.6700\n",
      "eval_steps_per_second............................. 9.5450\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.35it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0730\n",
      "eval_model_preparation_time....................... 0.0046\n",
      "eval_accuracy..................................... 0.9726\n",
      "eval_precision.................................... 0.8257\n",
      "eval_recall....................................... 0.9540\n",
      "eval_f1........................................... 0.8853\n",
      "eval_precision_class_0............................ 0.9941\n",
      "eval_precision_class_1............................ 0.8257\n",
      "eval_recall_class_0............................... 0.9749\n",
      "eval_recall_class_1............................... 0.9540\n",
      "eval_f1_class_0................................... 0.9844\n",
      "eval_f1_class_1................................... 0.8853\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8115\n",
      "eval_samples_per_second........................... 301.9110\n",
      "eval_steps_per_second............................. 19.7170\n",
      "100% 16/16 [00:00<00:00, 24.82it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9941    0.9749    0.9844     99651\n",
      "   EDU Start (1)     0.8257    0.9540    0.8853     12438\n",
      "\n",
      "        accuracy                         0.9726    112089\n",
      "       macro avg     0.9099    0.9644    0.9348    112089\n",
      "    weighted avg     0.9755    0.9726    0.9734    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9677               0.9726              \n",
      "Precision                      0.7974               0.8257              \n",
      "Recall                         0.9567               0.9540              \n",
      "F1                             0.8698               0.8853              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-nn4wcedb-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-nn4wcedb-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-nn4wcedb-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-nn4wcedb-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-nn4wcedb-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96772\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.86983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98157\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.86983\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09345\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.7974\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99436\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.7974\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95674\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r32_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/nn4wcedb\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_025632-nn4wcedb/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp2_classweight_rank \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_32_alpha_32_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D3DMmkzKYjo-",
    "outputId": "276ecfa8-5234-47d8-d8e5-f56c39f885e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_025812-1vp4jarw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r64_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/1vp4jarw\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r64_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              64\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    0.50x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 2,462,978 || all params: 111,254,020 || trainable%: 2.2138\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6755, 'grad_norm': 0.9203456044197083, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5869, 'grad_norm': 0.6441732048988342, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4444, 'grad_norm': 0.379355251789093, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.22265596687793732, 'eval_accuracy': 0.9115233383320148, 'eval_precision': 0.5663031554494419, 'eval_recall': 0.919145370758274, 'eval_f1': 0.7008185266520264, 'eval_precision_class_0': 0.9888426407677188, 'eval_precision_class_1': 0.5663031554494419, 'eval_recall_class_0': 0.9105548235983019, 'eval_recall_class_1': 0.919145370758274, 'eval_f1_class_0': 0.9480853304511095, 'eval_f1_class_1': 0.7008185266520264, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6116, 'eval_samples_per_second': 309.008, 'eval_steps_per_second': 9.81, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 37.38it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2227\n",
      "  eval/accuracy             0.9115\n",
      "  eval/precision            0.5663\n",
      "  eval/recall               0.9191\n",
      "  eval/f1                   0.7008\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9481\n",
      "    eval/f1_class_1         0.7008\n",
      "    eval/precision_class_0  0.9888\n",
      "    eval/precision_class_1  0.5663\n",
      "    eval/recall_class_0     0.9106\n",
      "    eval/recall_class_1     0.9191\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2665, 'grad_norm': 0.8851436972618103, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1972, 'grad_norm': 0.20402498543262482, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1675, 'grad_norm': 0.18268398940563202, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1575, 'grad_norm': 0.18751555681228638, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.63it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13230368494987488, 'eval_accuracy': 0.9575505673700244, 'eval_precision': 0.7561752302263534, 'eval_recall': 0.9201927105152912, 'eval_f1': 0.8301601549581896, 'eval_precision_class_0': 0.9895716436293964, 'eval_precision_class_1': 0.7561752302263534, 'eval_recall_class_0': 0.9622975472777844, 'eval_recall_class_1': 0.9201927105152912, 'eval_f1_class_0': 0.9757440405907793, 'eval_f1_class_1': 0.8301601549581896, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5964, 'eval_samples_per_second': 316.923, 'eval_steps_per_second': 10.061, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.63it/s]\n",
      "100% 6/6 [00:00<00:00, 38.57it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1323\n",
      "  eval/accuracy             0.9576\n",
      "  eval/precision            0.7562\n",
      "  eval/recall               0.9202\n",
      "  eval/f1                   0.8302\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9757\n",
      "    eval/f1_class_1         0.8302\n",
      "    eval/precision_class_0  0.9896\n",
      "    eval/precision_class_1  0.7562\n",
      "    eval/recall_class_0     0.9623\n",
      "    eval/recall_class_1     0.9202\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1406, 'grad_norm': 0.2351319044828415, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.142, 'grad_norm': 0.17357490956783295, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1379, 'grad_norm': 0.4121125340461731, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1281, 'grad_norm': 0.1296268105506897, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.53it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11630810052156448, 'eval_accuracy': 0.9581881944526444, 'eval_precision': 0.7493979905339201, 'eval_recall': 0.9452241307080017, 'eval_f1': 0.835996480014821, 'eval_precision_class_0': 0.9928007047876002, 'eval_precision_class_1': 0.7493979905339201, 'eval_recall_class_0': 0.9598355092426238, 'eval_recall_class_1': 0.9452241307080017, 'eval_f1_class_0': 0.9760398411226968, 'eval_f1_class_1': 0.835996480014821, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6168, 'eval_samples_per_second': 306.428, 'eval_steps_per_second': 9.728, 'epoch': 3.0}\n",
      " 30% 234/780 [00:20<00:37, 14.53it/s]\n",
      "100% 6/6 [00:00<00:00, 38.32it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1163\n",
      "  eval/accuracy             0.9582\n",
      "  eval/precision            0.7494\n",
      "  eval/recall               0.9452\n",
      "  eval/f1                   0.8360\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9760\n",
      "    eval/f1_class_1         0.8360\n",
      "    eval/precision_class_0  0.9928\n",
      "    eval/precision_class_1  0.7494\n",
      "    eval/recall_class_0     0.9598\n",
      "    eval/recall_class_1     0.9452\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1205, 'grad_norm': 0.33550015091896057, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1224, 'grad_norm': 0.1756913661956787, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1152, 'grad_norm': 0.10026215016841888, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1212, 'grad_norm': 0.2555372416973114, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.50it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10696352273225784, 'eval_accuracy': 0.962072996493051, 'eval_precision': 0.7698007153806847, 'eval_recall': 0.9466904063678258, 'eval_f1': 0.8491310474401127, 'eval_precision_class_0': 0.9930223312816154, 'eval_precision_class_1': 0.7698007153806847, 'eval_recall_class_0': 0.9640276280592486, 'eval_recall_class_1': 0.9466904063678258, 'eval_f1_class_0': 0.9783101939387391, 'eval_f1_class_1': 0.8491310474401127, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6223, 'eval_samples_per_second': 303.71, 'eval_steps_per_second': 9.642, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.50it/s]\n",
      "100% 6/6 [00:00<00:00, 38.35it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1070\n",
      "  eval/accuracy             0.9621\n",
      "  eval/precision            0.7698\n",
      "  eval/recall               0.9467\n",
      "  eval/f1                   0.8491\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9783\n",
      "    eval/f1_class_1         0.8491\n",
      "    eval/precision_class_0  0.9930\n",
      "    eval/precision_class_1  0.7698\n",
      "    eval/recall_class_0     0.9640\n",
      "    eval/recall_class_1     0.9467\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1115, 'grad_norm': 0.10756818950176239, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1119, 'grad_norm': 0.30981409549713135, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1073, 'grad_norm': 0.12436572462320328, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1099, 'grad_norm': 0.11168479919433594, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:32<00:26, 14.59it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10248342901468277, 'eval_accuracy': 0.9613054824121197, 'eval_precision': 0.7616186900292031, 'eval_recall': 0.9560117302052786, 'eval_f1': 0.8478149816560628, 'eval_precision_class_0': 0.9942231514084507, 'eval_precision_class_1': 0.7616186900292031, 'eval_recall_class_0': 0.9619781477488988, 'eval_recall_class_1': 0.9560117302052786, 'eval_f1_class_0': 0.9778348946531841, 'eval_f1_class_1': 0.8478149816560628, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6066, 'eval_samples_per_second': 311.551, 'eval_steps_per_second': 9.891, 'epoch': 5.0}\n",
      " 50% 390/780 [00:33<00:26, 14.59it/s]\n",
      "100% 6/6 [00:00<00:00, 38.60it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1025\n",
      "  eval/accuracy             0.9613\n",
      "  eval/precision            0.7616\n",
      "  eval/recall               0.9560\n",
      "  eval/f1                   0.8478\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9778\n",
      "    eval/f1_class_1         0.8478\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7616\n",
      "    eval/recall_class_0     0.9620\n",
      "    eval/recall_class_1     0.9560\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.11, 'grad_norm': 0.15397392213344574, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1051, 'grad_norm': 0.10710409283638, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1029, 'grad_norm': 0.19130757451057434, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1019, 'grad_norm': 0.09755151718854904, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10124870389699936, 'eval_accuracy': 0.9633482506582909, 'eval_precision': 0.7727272727272727, 'eval_recall': 0.9561164641809803, 'eval_f1': 0.8546952532534406, 'eval_precision_class_0': 0.9942504288164665, 'eval_precision_class_1': 0.7727272727272727, 'eval_recall_class_0': 0.9642671777059129, 'eval_recall_class_1': 0.9561164641809803, 'eval_f1_class_0': 0.9790292941303643, 'eval_f1_class_1': 0.8546952532534406, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6248, 'eval_samples_per_second': 302.509, 'eval_steps_per_second': 9.603, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.59it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1012\n",
      "  eval/accuracy             0.9633\n",
      "  eval/precision            0.7727\n",
      "  eval/recall               0.9561\n",
      "  eval/f1                   0.8547\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9790\n",
      "    eval/f1_class_1         0.8547\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7727\n",
      "    eval/recall_class_0     0.9643\n",
      "    eval/recall_class_1     0.9561\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0993, 'grad_norm': 0.12150225788354874, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0972, 'grad_norm': 0.1860155314207077, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0996, 'grad_norm': 0.11766140908002853, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0979, 'grad_norm': 0.10188639909029007, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:45<00:16, 14.42it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09914517402648926, 'eval_accuracy': 0.9655209059027737, 'eval_precision': 0.785197934595525, 'eval_recall': 0.9555927943024717, 'eval_f1': 0.8620559334845049, 'eval_precision_class_0': 0.9941972655982702, 'eval_precision_class_1': 0.785197934595525, 'eval_recall_class_0': 0.9667824489958877, 'eval_recall_class_1': 0.9555927943024717, 'eval_f1_class_0': 0.9802982254908575, 'eval_f1_class_1': 0.8620559334845049, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6268, 'eval_samples_per_second': 301.546, 'eval_steps_per_second': 9.573, 'epoch': 7.0}\n",
      " 70% 546/780 [00:46<00:16, 14.42it/s]\n",
      "100% 6/6 [00:00<00:00, 38.29it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0991\n",
      "  eval/accuracy             0.9655\n",
      "  eval/precision            0.7852\n",
      "  eval/recall               0.9556\n",
      "  eval/f1                   0.8621\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9803\n",
      "    eval/f1_class_1         0.8621\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7852\n",
      "    eval/recall_class_0     0.9668\n",
      "    eval/recall_class_1     0.9556\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1002, 'grad_norm': 0.13550466299057007, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0974, 'grad_norm': 0.14849285781383514, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0964, 'grad_norm': 0.12440244853496552, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0948, 'grad_norm': 0.14482173323631287, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.47it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09510914981365204, 'eval_accuracy': 0.9662411883479555, 'eval_precision': 0.7887919868750539, 'eval_recall': 0.9567448680351907, 'eval_f1': 0.8646883430356382, 'eval_precision_class_0': 0.9943508234392953, 'eval_precision_class_1': 0.7887919868750539, 'eval_recall_class_0': 0.9674478646810662, 'eval_recall_class_1': 0.9567448680351907, 'eval_f1_class_0': 0.9807148783465656, 'eval_f1_class_1': 0.8646883430356382, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6167, 'eval_samples_per_second': 306.451, 'eval_steps_per_second': 9.729, 'epoch': 8.0}\n",
      " 80% 624/780 [00:52<00:10, 14.47it/s]\n",
      "100% 6/6 [00:00<00:00, 38.40it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0951\n",
      "  eval/accuracy             0.9662\n",
      "  eval/precision            0.7888\n",
      "  eval/recall               0.9567\n",
      "  eval/f1                   0.8647\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9807\n",
      "    eval/f1_class_1         0.8647\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7888\n",
      "    eval/recall_class_0     0.9674\n",
      "    eval/recall_class_1     0.9567\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0914, 'grad_norm': 0.18003201484680176, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0915, 'grad_norm': 0.1182214543223381, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0956, 'grad_norm': 0.09669310599565506, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0937, 'grad_norm': 0.09343131631612778, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:58<00:05, 14.43it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09462804347276688, 'eval_accuracy': 0.9675518662400076, 'eval_precision': 0.7976711609175275, 'eval_recall': 0.9542312526183494, 'eval_f1': 0.8689556509298999, 'eval_precision_class_0': 0.9940355139421567, 'eval_precision_class_1': 0.7976711609175275, 'eval_recall_class_0': 0.9692444870310483, 'eval_recall_class_1': 0.9542312526183494, 'eval_f1_class_0': 0.9814834779796238, 'eval_f1_class_1': 0.8689556509298999, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6258, 'eval_samples_per_second': 302.014, 'eval_steps_per_second': 9.588, 'epoch': 9.0}\n",
      " 90% 702/780 [00:59<00:05, 14.43it/s]\n",
      "100% 6/6 [00:00<00:00, 38.38it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0946\n",
      "  eval/accuracy             0.9676\n",
      "  eval/precision            0.7977\n",
      "  eval/recall               0.9542\n",
      "  eval/f1                   0.8690\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9815\n",
      "    eval/f1_class_1         0.8690\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.7977\n",
      "    eval/recall_class_0     0.9692\n",
      "    eval/recall_class_1     0.9542\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.092, 'grad_norm': 0.09974345564842224, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0864, 'grad_norm': 0.14239910244941711, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0941, 'grad_norm': 0.13333933055400848, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0975, 'grad_norm': 0.10610225796699524, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.51it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09420469403266907, 'eval_accuracy': 0.9673511317880716, 'eval_precision': 0.7960715844609341, 'eval_recall': 0.9550691244239631, 'eval_f1': 0.8683521401704518, 'eval_precision_class_0': 0.9941420651609908, 'eval_precision_class_1': 0.7960715844609341, 'eval_recall_class_0': 0.968911779188459, 'eval_recall_class_1': 0.9550691244239631, 'eval_f1_class_0': 0.9813647851727043, 'eval_f1_class_1': 0.8683521401704518, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.615, 'eval_samples_per_second': 307.318, 'eval_steps_per_second': 9.756, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 14.51it/s]\n",
      "100% 6/6 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0942\n",
      "  eval/accuracy             0.9674\n",
      "  eval/precision            0.7961\n",
      "  eval/recall               0.9551\n",
      "  eval/f1                   0.8684\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8684\n",
      "    eval/precision_class_0  0.9941\n",
      "    eval/precision_class_1  0.7961\n",
      "    eval/recall_class_0     0.9689\n",
      "    eval/recall_class_1     0.9551\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.8434, 'train_samples_per_second': 189.389, 'train_steps_per_second': 11.846, 'train_loss': 0.15150428123963186, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.85it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.8434\n",
      "train_samples_per_second.......................... 189.3890\n",
      "train_steps_per_second............................ 11.8460\n",
      "total_flos........................................ 3348809007636480.0000\n",
      "train_loss........................................ 0.1515\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.41it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0946\n",
      "  eval/accuracy             0.9676\n",
      "  eval/precision            0.7977\n",
      "  eval/recall               0.9542\n",
      "  eval/f1                   0.8690\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9815\n",
      "    eval/f1_class_1         0.8690\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.7977\n",
      "    eval/recall_class_0     0.9692\n",
      "    eval/recall_class_1     0.9542\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.29it/s]\n",
      "eval_loss......................................... 0.0946\n",
      "eval_accuracy..................................... 0.9676\n",
      "eval_precision.................................... 0.7977\n",
      "eval_recall....................................... 0.9542\n",
      "eval_f1........................................... 0.8690\n",
      "eval_precision_class_0............................ 0.9940\n",
      "eval_precision_class_1............................ 0.7977\n",
      "eval_recall_class_0............................... 0.9692\n",
      "eval_recall_class_1............................... 0.9542\n",
      "eval_f1_class_0................................... 0.9815\n",
      "eval_f1_class_1................................... 0.8690\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6207\n",
      "eval_samples_per_second........................... 304.5120\n",
      "eval_steps_per_second............................. 9.6670\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.28it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0724\n",
      "eval_model_preparation_time....................... 0.0045\n",
      "eval_accuracy..................................... 0.9729\n",
      "eval_precision.................................... 0.8288\n",
      "eval_recall....................................... 0.9528\n",
      "eval_f1........................................... 0.8865\n",
      "eval_precision_class_0............................ 0.9940\n",
      "eval_precision_class_1............................ 0.8288\n",
      "eval_recall_class_0............................... 0.9754\n",
      "eval_recall_class_1............................... 0.9528\n",
      "eval_f1_class_0................................... 0.9846\n",
      "eval_f1_class_1................................... 0.8865\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8167\n",
      "eval_samples_per_second........................... 299.9730\n",
      "eval_steps_per_second............................. 19.5900\n",
      "100% 16/16 [00:00<00:00, 24.87it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9940    0.9754    0.9846     99651\n",
      "   EDU Start (1)     0.8288    0.9528    0.8865     12438\n",
      "\n",
      "        accuracy                         0.9729    112089\n",
      "       macro avg     0.9114    0.9641    0.9356    112089\n",
      "    weighted avg     0.9757    0.9729    0.9737    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9676               0.9729              \n",
      "Precision                      0.7977               0.8288              \n",
      "Recall                         0.9542               0.9528              \n",
      "F1                             0.8690               0.8865              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-1vp4jarw-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-1vp4jarw-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-1vp4jarw-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.5s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-1vp4jarw-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run-1vp4jarw-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96755\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.86896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98148\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.86896\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09463\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.79767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99404\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.79767\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r64_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/1vp4jarw\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_025812-1vp4jarw/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp2_classweight_rank \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 64 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_64_alpha_32_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AiGcipEcYjrz",
    "outputId": "5d422510-6bbd-4b11-fa66-d3371d86e28e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_030512-61m6zdrj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r16_a32_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/61m6zdrj\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r16_a32_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 32\n",
      "  Effective LR scale:    2.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6685, 'grad_norm': 0.7500047087669373, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5969, 'grad_norm': 0.6461816430091858, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4485, 'grad_norm': 0.6053774356842041, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:47, 14.73it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.21126367151737213, 'eval_accuracy': 0.9263894956842093, 'eval_precision': 0.6198293317905699, 'eval_recall': 0.8976749057394219, 'eval_f1': 0.7333162217659137, 'eval_precision_class_0': 0.986212444080665, 'eval_precision_class_1': 0.6198293317905699, 'eval_recall_class_0': 0.9300381948603292, 'eval_recall_class_1': 0.8976749057394219, 'eval_f1_class_0': 0.9573019547677429, 'eval_f1_class_1': 0.7333162217659137, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6129, 'eval_samples_per_second': 308.346, 'eval_steps_per_second': 9.789, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:47, 14.73it/s]\n",
      "100% 6/6 [00:00<00:00, 37.61it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2113\n",
      "  eval/accuracy             0.9264\n",
      "  eval/precision            0.6198\n",
      "  eval/recall               0.8977\n",
      "  eval/f1                   0.7333\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9573\n",
      "    eval/f1_class_1         0.7333\n",
      "    eval/precision_class_0  0.9862\n",
      "    eval/precision_class_1  0.6198\n",
      "    eval/recall_class_0     0.9300\n",
      "    eval/recall_class_1     0.8977\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.258, 'grad_norm': 0.29561087489128113, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1928, 'grad_norm': 0.39448028802871704, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1652, 'grad_norm': 0.7489322423934937, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1527, 'grad_norm': 1.0439934730529785, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:43, 14.52it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13364891707897186, 'eval_accuracy': 0.960821358145686, 'eval_precision': 0.7803275737940965, 'eval_recall': 0.9081483033095936, 'eval_f1': 0.8393998063891578, 'eval_precision_class_0': 0.9880805142911507, 'eval_precision_class_1': 0.7803275737940965, 'eval_recall_class_0': 0.9675144062495841, 'eval_recall_class_1': 0.9081483033095936, 'eval_f1_class_0': 0.9776893180381662, 'eval_f1_class_1': 0.8393998063891578, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5998, 'eval_samples_per_second': 315.088, 'eval_steps_per_second': 10.003, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.52it/s]\n",
      "100% 6/6 [00:00<00:00, 38.56it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1336\n",
      "  eval/accuracy             0.9608\n",
      "  eval/precision            0.7803\n",
      "  eval/recall               0.9081\n",
      "  eval/f1                   0.8394\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9777\n",
      "    eval/f1_class_1         0.8394\n",
      "    eval/precision_class_0  0.9881\n",
      "    eval/precision_class_1  0.7803\n",
      "    eval/recall_class_0     0.9675\n",
      "    eval/recall_class_1     0.9081\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1396, 'grad_norm': 0.20941148698329926, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.139, 'grad_norm': 0.2153237909078598, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1339, 'grad_norm': 0.18737192451953888, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1298, 'grad_norm': 0.5201109051704407, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:38, 14.15it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11546476930379868, 'eval_accuracy': 0.9608567818724982, 'eval_precision': 0.7661173255913244, 'eval_recall': 0.9396732299958106, 'eval_f1': 0.8440660426172445, 'eval_precision_class_0': 0.992107210392173, 'eval_precision_class_1': 0.7661173255913244, 'eval_recall_class_0': 0.9635485287659201, 'eval_recall_class_1': 0.9396732299958106, 'eval_f1_class_0': 0.9776193466064448, 'eval_f1_class_1': 0.8440660426172445, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.62, 'eval_samples_per_second': 304.818, 'eval_steps_per_second': 9.677, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:38, 14.15it/s]\n",
      "100% 6/6 [00:00<00:00, 38.44it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1155\n",
      "  eval/accuracy             0.9609\n",
      "  eval/precision            0.7661\n",
      "  eval/recall               0.9397\n",
      "  eval/f1                   0.8441\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9776\n",
      "    eval/f1_class_1         0.8441\n",
      "    eval/precision_class_0  0.9921\n",
      "    eval/precision_class_1  0.7661\n",
      "    eval/recall_class_0     0.9635\n",
      "    eval/recall_class_1     0.9397\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1203, 'grad_norm': 0.49958083033561707, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1225, 'grad_norm': 0.4793667793273926, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1156, 'grad_norm': 0.22362953424453735, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1207, 'grad_norm': 0.27281880378723145, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.53it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.1064959466457367, 'eval_accuracy': 0.9637969511979123, 'eval_precision': 0.781973203410475, 'eval_recall': 0.9413489736070382, 'eval_f1': 0.8542914171656687, 'eval_precision_class_0': 0.9923492041806135, 'eval_precision_class_1': 0.781973203410475, 'eval_recall_class_0': 0.966649365858852, 'eval_recall_class_1': 0.9413489736070382, 'eval_f1_class_0': 0.9793307086614174, 'eval_f1_class_1': 0.8542914171656687, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6104, 'eval_samples_per_second': 309.614, 'eval_steps_per_second': 9.829, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.53it/s]\n",
      "100% 6/6 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1065\n",
      "  eval/accuracy             0.9638\n",
      "  eval/precision            0.7820\n",
      "  eval/recall               0.9413\n",
      "  eval/f1                   0.8543\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9793\n",
      "    eval/f1_class_1         0.8543\n",
      "    eval/precision_class_0  0.9923\n",
      "    eval/precision_class_1  0.7820\n",
      "    eval/recall_class_0     0.9666\n",
      "    eval/recall_class_1     0.9413\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1137, 'grad_norm': 0.19396916031837463, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1117, 'grad_norm': 0.41681042313575745, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.107, 'grad_norm': 0.17014892399311066, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1105, 'grad_norm': 0.1680092066526413, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:32<00:27, 14.35it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10209926217794418, 'eval_accuracy': 0.9614471773193685, 'eval_precision': 0.7616826322365681, 'eval_recall': 0.9576874738165061, 'eval_f1': 0.8485129680322925, 'eval_precision_class_0': 0.9944416928072203, 'eval_precision_class_1': 0.7616826322365681, 'eval_recall_class_0': 0.9619249144940845, 'eval_recall_class_1': 0.9576874738165061, 'eval_f1_class_0': 0.9779130728902419, 'eval_f1_class_1': 0.8485129680322925, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6134, 'eval_samples_per_second': 308.115, 'eval_steps_per_second': 9.781, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:27, 14.35it/s]\n",
      "100% 6/6 [00:00<00:00, 38.62it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1021\n",
      "  eval/accuracy             0.9614\n",
      "  eval/precision            0.7617\n",
      "  eval/recall               0.9577\n",
      "  eval/f1                   0.8485\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9779\n",
      "    eval/f1_class_1         0.8485\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7617\n",
      "    eval/recall_class_0     0.9619\n",
      "    eval/recall_class_1     0.9577\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1108, 'grad_norm': 0.28094881772994995, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1049, 'grad_norm': 0.2478669434785843, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1031, 'grad_norm': 0.3959870934486389, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1029, 'grad_norm': 0.13903868198394775, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.46it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10094177722930908, 'eval_accuracy': 0.9633600585672283, 'eval_precision': 0.7729775518847946, 'eval_recall': 0.9556975282781734, 'eval_f1': 0.8546808410996113, 'eval_precision_class_0': 0.9941962570660228, 'eval_precision_class_1': 0.7729775518847946, 'eval_recall_class_0': 0.9643337192744308, 'eval_recall_class_1': 0.9556975282781734, 'eval_f1_class_0': 0.9790373247762202, 'eval_f1_class_1': 0.8546808410996113, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6154, 'eval_samples_per_second': 307.098, 'eval_steps_per_second': 9.749, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.46it/s]\n",
      "100% 6/6 [00:00<00:00, 38.55it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1009\n",
      "  eval/accuracy             0.9634\n",
      "  eval/precision            0.7730\n",
      "  eval/recall               0.9557\n",
      "  eval/f1                   0.8547\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9790\n",
      "    eval/f1_class_1         0.8547\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7730\n",
      "    eval/recall_class_0     0.9643\n",
      "    eval/recall_class_1     0.9557\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0996, 'grad_norm': 0.2046879082918167, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0974, 'grad_norm': 0.19908791780471802, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1004, 'grad_norm': 0.27345359325408936, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0978, 'grad_norm': 0.22147199511528015, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:45<00:16, 14.42it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09841565787792206, 'eval_accuracy': 0.9641393805570971, 'eval_precision': 0.7762878723584825, 'eval_recall': 0.9580016757436113, 'eval_f1': 0.8576250527401434, 'eval_precision_class_0': 0.9944997668230324, 'eval_precision_class_1': 0.7762878723584825, 'eval_recall_class_0': 0.9649192850773879, 'eval_recall_class_1': 0.9580016757436113, 'eval_f1_class_0': 0.9794862442332503, 'eval_f1_class_1': 0.8576250527401434, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6053, 'eval_samples_per_second': 312.238, 'eval_steps_per_second': 9.912, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.42it/s]\n",
      "100% 6/6 [00:00<00:00, 38.60it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0984\n",
      "  eval/accuracy             0.9641\n",
      "  eval/precision            0.7763\n",
      "  eval/recall               0.9580\n",
      "  eval/f1                   0.8576\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9795\n",
      "    eval/f1_class_1         0.8576\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7763\n",
      "    eval/recall_class_0     0.9649\n",
      "    eval/recall_class_1     0.9580\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1014, 'grad_norm': 0.30786600708961487, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0986, 'grad_norm': 0.2551977336406708, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0959, 'grad_norm': 0.15649579465389252, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0951, 'grad_norm': 0.2243884801864624, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.55it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09390506893396378, 'eval_accuracy': 0.9669024312484502, 'eval_precision': 0.7922696940809429, 'eval_recall': 0.9574780058651027, 'eval_f1': 0.8670745008773177, 'eval_precision_class_0': 0.9944497607655503, 'eval_precision_class_1': 0.7922696940809429, 'eval_recall_class_0': 0.9680999720525412, 'eval_recall_class_1': 0.9574780058651027, 'eval_f1_class_0': 0.9810979762763755, 'eval_f1_class_1': 0.8670745008773177, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6015, 'eval_samples_per_second': 314.195, 'eval_steps_per_second': 9.974, 'epoch': 8.0}\n",
      " 80% 624/780 [00:52<00:10, 14.55it/s]\n",
      "100% 6/6 [00:00<00:00, 38.75it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0939\n",
      "  eval/accuracy             0.9669\n",
      "  eval/precision            0.7923\n",
      "  eval/recall               0.9575\n",
      "  eval/f1                   0.8671\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9811\n",
      "    eval/f1_class_1         0.8671\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7923\n",
      "    eval/recall_class_0     0.9681\n",
      "    eval/recall_class_1     0.9575\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0917, 'grad_norm': 0.2760993242263794, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0924, 'grad_norm': 0.22418121993541718, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0969, 'grad_norm': 0.1561153382062912, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0943, 'grad_norm': 0.1708831787109375, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:58<00:05, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09347681701183319, 'eval_accuracy': 0.9674337871506334, 'eval_precision': 0.7958863517517867, 'eval_recall': 0.9564306661080855, 'eval_f1': 0.8688041099800209, 'eval_precision_class_0': 0.9943181042136174, 'eval_precision_class_1': 0.7958863517517867, 'eval_recall_class_0': 0.9688319293062376, 'eval_recall_class_1': 0.9564306661080855, 'eval_f1_class_0': 0.9814095823559545, 'eval_f1_class_1': 0.8688041099800209, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6157, 'eval_samples_per_second': 306.945, 'eval_steps_per_second': 9.744, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.28it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0935\n",
      "  eval/accuracy             0.9674\n",
      "  eval/precision            0.7959\n",
      "  eval/recall               0.9564\n",
      "  eval/f1                   0.8688\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8688\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7959\n",
      "    eval/recall_class_0     0.9688\n",
      "    eval/recall_class_1     0.9564\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.092, 'grad_norm': 0.15356184542179108, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0874, 'grad_norm': 0.23842497169971466, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0947, 'grad_norm': 0.2158527970314026, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.098, 'grad_norm': 0.2231752723455429, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.39it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09340457618236542, 'eval_accuracy': 0.967421979241696, 'eval_precision': 0.795405099643199, 'eval_recall': 0.9572685379136993, 'eval_f1': 0.8688625885260707, 'eval_precision_class_0': 0.9944260772152245, 'eval_precision_class_1': 0.795405099643199, 'eval_recall_class_0': 0.9687121544829055, 'eval_recall_class_1': 0.9572685379136993, 'eval_f1_class_0': 0.9814007105346537, 'eval_f1_class_1': 0.8688625885260707, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6163, 'eval_samples_per_second': 306.647, 'eval_steps_per_second': 9.735, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 14.39it/s]\n",
      "100% 6/6 [00:00<00:00, 38.74it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0934\n",
      "  eval/accuracy             0.9674\n",
      "  eval/precision            0.7954\n",
      "  eval/recall               0.9573\n",
      "  eval/f1                   0.8689\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8689\n",
      "    eval/precision_class_0  0.9944\n",
      "    eval/precision_class_1  0.7954\n",
      "    eval/recall_class_0     0.9687\n",
      "    eval/recall_class_1     0.9573\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.5681, 'train_samples_per_second': 190.184, 'train_steps_per_second': 11.896, 'train_loss': 0.15132965369102283, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.90it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.5681\n",
      "train_samples_per_second.......................... 190.1840\n",
      "train_steps_per_second............................ 11.8960\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1513\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.21it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0935\n",
      "  eval/accuracy             0.9674\n",
      "  eval/precision            0.7959\n",
      "  eval/recall               0.9564\n",
      "  eval/f1                   0.8688\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9814\n",
      "    eval/f1_class_1         0.8688\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7959\n",
      "    eval/recall_class_0     0.9688\n",
      "    eval/recall_class_1     0.9564\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.41it/s]\n",
      "eval_loss......................................... 0.0935\n",
      "eval_accuracy..................................... 0.9674\n",
      "eval_precision.................................... 0.7959\n",
      "eval_recall....................................... 0.9564\n",
      "eval_f1........................................... 0.8688\n",
      "eval_precision_class_0............................ 0.9943\n",
      "eval_precision_class_1............................ 0.7959\n",
      "eval_recall_class_0............................... 0.9688\n",
      "eval_recall_class_1............................... 0.9564\n",
      "eval_f1_class_0................................... 0.9814\n",
      "eval_f1_class_1................................... 0.8688\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6148\n",
      "eval_samples_per_second........................... 307.4250\n",
      "eval_steps_per_second............................. 9.7600\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.46it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0724\n",
      "eval_model_preparation_time....................... 0.0047\n",
      "eval_accuracy..................................... 0.9731\n",
      "eval_precision.................................... 0.8296\n",
      "eval_recall....................................... 0.9535\n",
      "eval_f1........................................... 0.8873\n",
      "eval_precision_class_0............................ 0.9941\n",
      "eval_precision_class_1............................ 0.8296\n",
      "eval_recall_class_0............................... 0.9756\n",
      "eval_recall_class_1............................... 0.9535\n",
      "eval_f1_class_0................................... 0.9847\n",
      "eval_f1_class_1................................... 0.8873\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8179\n",
      "eval_samples_per_second........................... 299.5590\n",
      "eval_steps_per_second............................. 19.5630\n",
      "100% 16/16 [00:00<00:00, 24.81it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9941    0.9756    0.9847     99651\n",
      "   EDU Start (1)     0.8296    0.9535    0.8873     12438\n",
      "\n",
      "        accuracy                         0.9731    112089\n",
      "       macro avg     0.9118    0.9645    0.9360    112089\n",
      "    weighted avg     0.9758    0.9731    0.9739    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9674               0.9731              \n",
      "Precision                      0.7959               0.8296              \n",
      "Recall                         0.9564               0.9535              \n",
      "F1                             0.8688               0.8873              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-61m6zdrj-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-61m6zdrj-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-61m6zdrj-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-61m6zdrj-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-61m6zdrj-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.8688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98141\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.8688\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09348\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.79589\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99432\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.79589\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95643\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r16_a32_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/61m6zdrj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_030512-61m6zdrj/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 32 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_32_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rg8LabqvYkge",
    "outputId": "e6d785c8-7f65-453e-ad46-5740cd7167d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_030654-kscjmqif\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r16_a16_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/kscjmqif\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r16_a16_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 16\n",
      "  Effective LR scale:    1.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6852, 'grad_norm': 0.7798186540603638, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6205, 'grad_norm': 0.6842682957649231, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.5168, 'grad_norm': 0.6117835640907288, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:47, 14.73it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.2435457408428192, 'eval_accuracy': 0.9041552031550733, 'eval_precision': 0.5459213144214107, 'eval_recall': 0.8908671973188103, 'eval_f1': 0.6769867483783676, 'eval_precision_class_0': 0.984922150836372, 'eval_precision_class_1': 0.5459213144214107, 'eval_recall_class_0': 0.9058436805472379, 'eval_recall_class_1': 0.8908671973188103, 'eval_f1_class_0': 0.9437292459566444, 'eval_f1_class_1': 0.6769867483783676, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6207, 'eval_samples_per_second': 304.516, 'eval_steps_per_second': 9.667, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:47, 14.73it/s]\n",
      "100% 6/6 [00:00<00:00, 36.97it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.2435\n",
      "  eval/accuracy             0.9042\n",
      "  eval/precision            0.5459\n",
      "  eval/recall               0.8909\n",
      "  eval/f1                   0.6770\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9437\n",
      "    eval/f1_class_1         0.6770\n",
      "    eval/precision_class_0  0.9849\n",
      "    eval/precision_class_1  0.5459\n",
      "    eval/recall_class_0     0.9058\n",
      "    eval/recall_class_1     0.8909\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.3158, 'grad_norm': 1.1011555194854736, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.2143, 'grad_norm': 0.3016815185546875, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1771, 'grad_norm': 0.17610055208206177, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1675, 'grad_norm': 0.2198469340801239, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.63it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.13821779191493988, 'eval_accuracy': 0.9526148614341886, 'eval_precision': 0.7293825113966017, 'eval_recall': 0.9216589861751152, 'eval_f1': 0.8143247119789015, 'eval_precision_class_0': 0.9897003745318352, 'eval_precision_class_1': 0.7293825113966017, 'eval_recall_class_0': 0.9565483557578419, 'eval_recall_class_1': 0.9216589861751152, 'eval_f1_class_0': 0.9728420126552296, 'eval_f1_class_1': 0.8143247119789015, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6058, 'eval_samples_per_second': 311.978, 'eval_steps_per_second': 9.904, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.63it/s]\n",
      "100% 6/6 [00:00<00:00, 38.64it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1382\n",
      "  eval/accuracy             0.9526\n",
      "  eval/precision            0.7294\n",
      "  eval/recall               0.9217\n",
      "  eval/f1                   0.8143\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9728\n",
      "    eval/f1_class_1         0.8143\n",
      "    eval/precision_class_0  0.9897\n",
      "    eval/precision_class_1  0.7294\n",
      "    eval/recall_class_0     0.9565\n",
      "    eval/recall_class_1     0.9217\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1477, 'grad_norm': 0.4650686979293823, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1511, 'grad_norm': 0.31241631507873535, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1455, 'grad_norm': 0.4086573123931885, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1342, 'grad_norm': 0.16533242166042328, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.62it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.12048525363206863, 'eval_accuracy': 0.9573498329180885, 'eval_precision': 0.7480361022898212, 'eval_recall': 0.9374738165060745, 'eval_f1': 0.8321093241610115, 'eval_precision_class_0': 0.9917907677076028, 'eval_precision_class_1': 0.7480361022898212, 'eval_recall_class_0': 0.9598754341837346, 'eval_recall_class_1': 0.9374738165060745, 'eval_f1_class_0': 0.9755721473786723, 'eval_f1_class_1': 0.8321093241610115, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6123, 'eval_samples_per_second': 308.651, 'eval_steps_per_second': 9.798, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.62it/s]\n",
      "100% 6/6 [00:00<00:00, 38.79it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1205\n",
      "  eval/accuracy             0.9573\n",
      "  eval/precision            0.7480\n",
      "  eval/recall               0.9375\n",
      "  eval/f1                   0.8321\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9756\n",
      "    eval/f1_class_1         0.8321\n",
      "    eval/precision_class_0  0.9918\n",
      "    eval/precision_class_1  0.7480\n",
      "    eval/recall_class_0     0.9599\n",
      "    eval/recall_class_1     0.9375\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1287, 'grad_norm': 0.42064836621284485, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1292, 'grad_norm': 0.14686666429042816, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.124, 'grad_norm': 0.1584596335887909, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1283, 'grad_norm': 0.34591177105903625, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.39it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11393658816814423, 'eval_accuracy': 0.9617069513159915, 'eval_precision': 0.7742496737712049, 'eval_recall': 0.932132383745287, 'eval_f1': 0.8458869932994345, 'eval_precision_class_0': 0.9911468153127305, 'eval_precision_class_1': 0.7742496737712049, 'eval_recall_class_0': 0.9654649259392343, 'eval_recall_class_1': 0.932132383745287, 'eval_f1_class_0': 0.978137324299727, 'eval_f1_class_1': 0.8458869932994345, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6078, 'eval_samples_per_second': 310.939, 'eval_steps_per_second': 9.871, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.39it/s]\n",
      "100% 6/6 [00:00<00:00, 38.43it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1139\n",
      "  eval/accuracy             0.9617\n",
      "  eval/precision            0.7742\n",
      "  eval/recall               0.9321\n",
      "  eval/f1                   0.8459\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9781\n",
      "    eval/f1_class_1         0.8459\n",
      "    eval/precision_class_0  0.9911\n",
      "    eval/precision_class_1  0.7742\n",
      "    eval/recall_class_0     0.9655\n",
      "    eval/recall_class_1     0.9321\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1218, 'grad_norm': 0.35186198353767395, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1209, 'grad_norm': 0.3543473482131958, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1154, 'grad_norm': 0.11606840789318085, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1187, 'grad_norm': 0.25612789392471313, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:31<00:26, 14.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10748997330665588, 'eval_accuracy': 0.9599357649753805, 'eval_precision': 0.7573806138663545, 'eval_recall': 0.9484708839547549, 'eval_f1': 0.8422227388979308, 'eval_precision_class_0': 0.9932354396964197, 'eval_precision_class_1': 0.7573806138663545, 'eval_recall_class_0': 0.9613925819459417, 'eval_recall_class_1': 0.9484708839547549, 'eval_f1_class_0': 0.9770546347203344, 'eval_f1_class_1': 0.8422227388979308, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6136, 'eval_samples_per_second': 308.036, 'eval_steps_per_second': 9.779, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.49it/s]\n",
      "100% 6/6 [00:00<00:00, 38.49it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1075\n",
      "  eval/accuracy             0.9599\n",
      "  eval/precision            0.7574\n",
      "  eval/recall               0.9485\n",
      "  eval/f1                   0.8422\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9771\n",
      "    eval/f1_class_1         0.8422\n",
      "    eval/precision_class_0  0.9932\n",
      "    eval/precision_class_1  0.7574\n",
      "    eval/recall_class_0     0.9614\n",
      "    eval/recall_class_1     0.9485\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1171, 'grad_norm': 0.33178964257240295, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.1136, 'grad_norm': 0.3822126090526581, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.1134, 'grad_norm': 0.25986480712890625, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.1106, 'grad_norm': 0.11707127839326859, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.37it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10598557442426682, 'eval_accuracy': 0.9606442395116249, 'eval_precision': 0.7597592577112764, 'eval_recall': 0.9519271051529116, 'eval_f1': 0.8450560178513319, 'eval_precision_class_0': 0.9936886395511921, 'eval_precision_class_1': 0.7597592577112764, 'eval_recall_class_0': 0.961751906415938, 'eval_recall_class_1': 0.9519271051529116, 'eval_f1_class_0': 0.977459473716245, 'eval_f1_class_1': 0.8450560178513319, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6317, 'eval_samples_per_second': 299.188, 'eval_steps_per_second': 9.498, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.37it/s]\n",
      "100% 6/6 [00:00<00:00, 38.39it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1060\n",
      "  eval/accuracy             0.9606\n",
      "  eval/precision            0.7598\n",
      "  eval/recall               0.9519\n",
      "  eval/f1                   0.8451\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9775\n",
      "    eval/f1_class_1         0.8451\n",
      "    eval/precision_class_0  0.9937\n",
      "    eval/precision_class_1  0.7598\n",
      "    eval/recall_class_0     0.9618\n",
      "    eval/recall_class_1     0.9519\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.108, 'grad_norm': 0.17081333696842194, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.1062, 'grad_norm': 0.19195911288261414, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.1097, 'grad_norm': 0.1632816046476364, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.1063, 'grad_norm': 0.22440211474895477, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.62it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.104490727186203, 'eval_accuracy': 0.9606914711473745, 'eval_precision': 0.7590602349412647, 'eval_recall': 0.9542312526183494, 'eval_f1': 0.8455292097814486, 'eval_precision_class_0': 0.9939878380981206, 'eval_precision_class_1': 0.7590602349412647, 'eval_recall_class_0': 0.9615123567692737, 'eval_recall_class_1': 0.9542312526183494, 'eval_f1_class_0': 0.977480433209089, 'eval_f1_class_1': 0.8455292097814486, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6078, 'eval_samples_per_second': 310.936, 'eval_steps_per_second': 9.871, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.62it/s]\n",
      "100% 6/6 [00:00<00:00, 38.47it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1045\n",
      "  eval/accuracy             0.9607\n",
      "  eval/precision            0.7591\n",
      "  eval/recall               0.9542\n",
      "  eval/f1                   0.8455\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9775\n",
      "    eval/f1_class_1         0.8455\n",
      "    eval/precision_class_0  0.9940\n",
      "    eval/precision_class_1  0.7591\n",
      "    eval/recall_class_0     0.9615\n",
      "    eval/recall_class_1     0.9542\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.109, 'grad_norm': 0.19288839399814606, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.1074, 'grad_norm': 0.16660350561141968, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.104, 'grad_norm': 0.11200576275587082, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.1029, 'grad_norm': 0.10639102756977081, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.43it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10089939832687378, 'eval_accuracy': 0.9627342393935457, 'eval_precision': 0.7709393014581214, 'eval_recall': 0.9524507750314202, 'eval_f1': 0.8521364317841079, 'eval_precision_class_0': 0.9937716927551342, 'eval_precision_class_1': 0.7709393014581214, 'eval_recall_class_0': 0.9640409363729522, 'eval_recall_class_1': 0.9524507750314202, 'eval_f1_class_0': 0.9786805733817907, 'eval_f1_class_1': 0.8521364317841079, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6145, 'eval_samples_per_second': 307.549, 'eval_steps_per_second': 9.763, 'epoch': 8.0}\n",
      " 80% 624/780 [00:51<00:10, 14.43it/s]\n",
      "100% 6/6 [00:00<00:00, 38.44it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1009\n",
      "  eval/accuracy             0.9627\n",
      "  eval/precision            0.7709\n",
      "  eval/recall               0.9525\n",
      "  eval/f1                   0.8521\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8521\n",
      "    eval/precision_class_0  0.9938\n",
      "    eval/precision_class_1  0.7709\n",
      "    eval/recall_class_0     0.9640\n",
      "    eval/recall_class_1     0.9525\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0999, 'grad_norm': 0.24991102516651154, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.1005, 'grad_norm': 0.14308173954486847, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.1056, 'grad_norm': 0.13721665740013123, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.1024, 'grad_norm': 0.1548510044813156, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.53it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09946905076503754, 'eval_accuracy': 0.9647179680950301, 'eval_precision': 0.7828073805828591, 'eval_recall': 0.9508797653958945, 'eval_f1': 0.8586966802232101, 'eval_precision_class_0': 0.9935833413142521, 'eval_precision_class_1': 0.7828073805828591, 'eval_recall_class_0': 0.9664763577807056, 'eval_recall_class_1': 0.9508797653958945, 'eval_f1_class_0': 0.9798424091963949, 'eval_f1_class_1': 0.8586966802232101, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6217, 'eval_samples_per_second': 303.993, 'eval_steps_per_second': 9.651, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.53it/s]\n",
      "100% 6/6 [00:00<00:00, 38.75it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0995\n",
      "  eval/accuracy             0.9647\n",
      "  eval/precision            0.7828\n",
      "  eval/recall               0.9509\n",
      "  eval/f1                   0.8587\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9798\n",
      "    eval/f1_class_1         0.8587\n",
      "    eval/precision_class_0  0.9936\n",
      "    eval/precision_class_1  0.7828\n",
      "    eval/recall_class_0     0.9665\n",
      "    eval/recall_class_1     0.9509\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1003, 'grad_norm': 0.11149445921182632, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0962, 'grad_norm': 0.3081912100315094, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.1022, 'grad_norm': 0.13440081477165222, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.1061, 'grad_norm': 0.13421471416950226, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.61it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10025787353515625, 'eval_accuracy': 0.963525369292352, 'eval_precision': 0.7751086123179146, 'eval_recall': 0.9529744449099288, 'eval_f1': 0.8548879597876639, 'eval_precision_class_0': 0.9938450993831391, 'eval_precision_class_1': 0.7751086123179146, 'eval_recall_class_0': 0.9648660518225736, 'eval_recall_class_1': 0.9529744449099288, 'eval_f1_class_0': 0.9791412037193348, 'eval_f1_class_1': 0.8548879597876639, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6157, 'eval_samples_per_second': 306.956, 'eval_steps_per_second': 9.745, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.61it/s]\n",
      "100% 6/6 [00:00<00:00, 38.66it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1003\n",
      "  eval/accuracy             0.9635\n",
      "  eval/precision            0.7751\n",
      "  eval/recall               0.9530\n",
      "  eval/f1                   0.8549\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9791\n",
      "    eval/f1_class_1         0.8549\n",
      "    eval/precision_class_0  0.9938\n",
      "    eval/precision_class_1  0.7751\n",
      "    eval/recall_class_0     0.9649\n",
      "    eval/recall_class_1     0.9530\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.0253, 'train_samples_per_second': 191.771, 'train_steps_per_second': 11.995, 'train_loss': 0.16369549311124362, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 12.00it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.0253\n",
      "train_samples_per_second.......................... 191.7710\n",
      "train_steps_per_second............................ 11.9950\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1637\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.26it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0995\n",
      "  eval/accuracy             0.9647\n",
      "  eval/precision            0.7828\n",
      "  eval/recall               0.9509\n",
      "  eval/f1                   0.8587\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9798\n",
      "    eval/f1_class_1         0.8587\n",
      "    eval/precision_class_0  0.9936\n",
      "    eval/precision_class_1  0.7828\n",
      "    eval/recall_class_0     0.9665\n",
      "    eval/recall_class_1     0.9509\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.30it/s]\n",
      "eval_loss......................................... 0.0995\n",
      "eval_accuracy..................................... 0.9647\n",
      "eval_precision.................................... 0.7828\n",
      "eval_recall....................................... 0.9509\n",
      "eval_f1........................................... 0.8587\n",
      "eval_precision_class_0............................ 0.9936\n",
      "eval_precision_class_1............................ 0.7828\n",
      "eval_recall_class_0............................... 0.9665\n",
      "eval_recall_class_1............................... 0.9509\n",
      "eval_f1_class_0................................... 0.9798\n",
      "eval_f1_class_1................................... 0.8587\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6167\n",
      "eval_samples_per_second........................... 306.4870\n",
      "eval_steps_per_second............................. 9.7300\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 25.12it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0755\n",
      "eval_model_preparation_time....................... 0.0046\n",
      "eval_accuracy..................................... 0.9707\n",
      "eval_precision.................................... 0.8144\n",
      "eval_recall....................................... 0.9530\n",
      "eval_f1........................................... 0.8783\n",
      "eval_precision_class_0............................ 0.9940\n",
      "eval_precision_class_1............................ 0.8144\n",
      "eval_recall_class_0............................... 0.9729\n",
      "eval_recall_class_1............................... 0.9530\n",
      "eval_f1_class_0................................... 0.9833\n",
      "eval_f1_class_1................................... 0.8783\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.7936\n",
      "eval_samples_per_second........................... 308.7060\n",
      "eval_steps_per_second............................. 20.1600\n",
      "100% 16/16 [00:00<00:00, 24.84it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9940    0.9729    0.9833     99651\n",
      "   EDU Start (1)     0.8144    0.9530    0.8783     12438\n",
      "\n",
      "        accuracy                         0.9707    112089\n",
      "       macro avg     0.9042    0.9630    0.9308    112089\n",
      "    weighted avg     0.9741    0.9707    0.9717    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9647               0.9707              \n",
      "Precision                      0.7828               0.8144              \n",
      "Recall                         0.9509               0.9530              \n",
      "F1                             0.8587               0.8783              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-kscjmqif-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-kscjmqif-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-kscjmqif-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-kscjmqif-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚°ø\u001b[0m uploading artifact run-kscjmqif-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96472\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.8587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.97984\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.8587\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09947\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.78281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99358\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.78281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95088\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r16_a16_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/kscjmqif\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_030654-kscjmqif/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 16 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_16_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ill2mL5hV73",
    "outputId": "c0521217-dfc1-45a9-c4d7-d58dfa9e8cfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_031549-28s6c44k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r16_a64_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/28s6c44k\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r16_a64_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 64\n",
      "  Effective LR scale:    4.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6989, 'grad_norm': 0.9025867581367493, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5888, 'grad_norm': 0.5921308994293213, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4009, 'grad_norm': 0.5174015760421753, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.18655185401439667, 'eval_accuracy': 0.9387759921595484, 'eval_precision': 0.6706029561273168, 'eval_recall': 0.8980938416422287, 'eval_f1': 0.7678531452876651, 'eval_precision_class_0': 0.9864676921365192, 'eval_precision_class_1': 0.6706029561273168, 'eval_recall_class_0': 0.9439453826805605, 'eval_recall_class_1': 0.8980938416422287, 'eval_f1_class_0': 0.9647382058309474, 'eval_f1_class_1': 0.7678531452876651, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6108, 'eval_samples_per_second': 309.432, 'eval_steps_per_second': 9.823, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 37.60it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1866\n",
      "  eval/accuracy             0.9388\n",
      "  eval/precision            0.6706\n",
      "  eval/recall               0.8981\n",
      "  eval/f1                   0.7679\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9647\n",
      "    eval/f1_class_1         0.7679\n",
      "    eval/precision_class_0  0.9865\n",
      "    eval/precision_class_1  0.6706\n",
      "    eval/recall_class_0     0.9439\n",
      "    eval/recall_class_1     0.8981\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2261, 'grad_norm': 0.7238978743553162, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1793, 'grad_norm': 0.4696234464645386, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1589, 'grad_norm': 0.5722448229789734, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1454, 'grad_norm': 0.54361891746521, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.57it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.12468431144952774, 'eval_accuracy': 0.9576568385504611, 'eval_precision': 0.7517311264989022, 'eval_recall': 0.9323418516966904, 'eval_f1': 0.8323515661524077, 'eval_precision_class_0': 0.9911320987823795, 'eval_precision_class_1': 0.7517311264989022, 'eval_recall_class_0': 0.9608735577115024, 'eval_recall_class_1': 0.9323418516966904, 'eval_f1_class_0': 0.9757683055382869, 'eval_f1_class_1': 0.8323515661524077, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6083, 'eval_samples_per_second': 310.706, 'eval_steps_per_second': 9.864, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.57it/s]\n",
      "100% 6/6 [00:00<00:00, 38.59it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1247\n",
      "  eval/accuracy             0.9577\n",
      "  eval/precision            0.7517\n",
      "  eval/recall               0.9323\n",
      "  eval/f1                   0.8324\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9758\n",
      "    eval/f1_class_1         0.8324\n",
      "    eval/precision_class_0  0.9911\n",
      "    eval/precision_class_1  0.7517\n",
      "    eval/recall_class_0     0.9609\n",
      "    eval/recall_class_1     0.9323\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1313, 'grad_norm': 1.6691144704818726, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1319, 'grad_norm': 0.27354758977890015, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1266, 'grad_norm': 0.31112322211265564, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1247, 'grad_norm': 0.6970521211624146, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.46it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11110434681177139, 'eval_accuracy': 0.9626279682131091, 'eval_precision': 0.7763921364856673, 'eval_recall': 0.9389400921658986, 'eval_f1': 0.8499644465513154, 'eval_precision_class_0': 0.9920292034672281, 'eval_precision_class_1': 0.7763921364856673, 'eval_recall_class_0': 0.9656379340173806, 'eval_recall_class_1': 0.9389400921658986, 'eval_f1_class_0': 0.978655678668492, 'eval_f1_class_1': 0.8499644465513154, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6205, 'eval_samples_per_second': 304.601, 'eval_steps_per_second': 9.67, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.46it/s]\n",
      "100% 6/6 [00:00<00:00, 38.57it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1111\n",
      "  eval/accuracy             0.9626\n",
      "  eval/precision            0.7764\n",
      "  eval/recall               0.9389\n",
      "  eval/f1                   0.8500\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8500\n",
      "    eval/precision_class_0  0.9920\n",
      "    eval/precision_class_1  0.7764\n",
      "    eval/recall_class_0     0.9656\n",
      "    eval/recall_class_1     0.9389\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1157, 'grad_norm': 0.5306685566902161, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1159, 'grad_norm': 0.46087774634361267, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1091, 'grad_norm': 0.24654079973697662, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1138, 'grad_norm': 0.27980729937553406, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.54it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10295463353395462, 'eval_accuracy': 0.9612936745031823, 'eval_precision': 0.7613371123707903, 'eval_recall': 0.9565354000837872, 'eval_f1': 0.847846268102488, 'eval_precision_class_0': 0.9942910596618657, 'eval_precision_class_1': 0.7613371123707903, 'eval_recall_class_0': 0.9618982978666774, 'eval_recall_class_1': 0.9565354000837872, 'eval_f1_class_0': 0.977826481053073, 'eval_f1_class_1': 0.847846268102488, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6208, 'eval_samples_per_second': 304.439, 'eval_steps_per_second': 9.665, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.54it/s]\n",
      "100% 6/6 [00:00<00:00, 38.67it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1030\n",
      "  eval/accuracy             0.9613\n",
      "  eval/precision            0.7613\n",
      "  eval/recall               0.9565\n",
      "  eval/f1                   0.8478\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9778\n",
      "    eval/f1_class_1         0.8478\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7613\n",
      "    eval/recall_class_0     0.9619\n",
      "    eval/recall_class_1     0.9565\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1072, 'grad_norm': 0.2233676165342331, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.1047, 'grad_norm': 0.39319539070129395, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.1012, 'grad_norm': 0.27921822667121887, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1051, 'grad_norm': 0.24816499650478363, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:32<00:27, 14.42it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09885545819997787, 'eval_accuracy': 0.9624862733058602, 'eval_precision': 0.766859344894027, 'eval_recall': 0.9587348135735233, 'eval_f1': 0.8521293925994881, 'eval_precision_class_0': 0.9945843413239499, 'eval_precision_class_1': 0.766859344894027, 'eval_recall_class_0': 0.9629629629629629, 'eval_recall_class_1': 0.9587348135735233, 'eval_f1_class_0': 0.9785182530613349, 'eval_f1_class_1': 0.8521293925994881, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6099, 'eval_samples_per_second': 309.883, 'eval_steps_per_second': 9.838, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:27, 14.42it/s]\n",
      "100% 6/6 [00:00<00:00, 38.58it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0989\n",
      "  eval/accuracy             0.9625\n",
      "  eval/precision            0.7669\n",
      "  eval/recall               0.9587\n",
      "  eval/f1                   0.8521\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9785\n",
      "    eval/f1_class_1         0.8521\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.7669\n",
      "    eval/recall_class_0     0.9630\n",
      "    eval/recall_class_1     0.9587\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.105, 'grad_norm': 0.37530043721199036, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0983, 'grad_norm': 0.33167243003845215, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.0986, 'grad_norm': 0.6810822486877441, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0982, 'grad_norm': 0.17448116838932037, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.63it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10001877695322037, 'eval_accuracy': 0.9628405105739825, 'eval_precision': 0.7676674751191771, 'eval_recall': 0.9613531629660662, 'eval_f1': 0.853661939083934, 'eval_precision_class_0': 0.9949265797723148, 'eval_precision_class_1': 0.7676674751191771, 'eval_recall_class_0': 0.9630295045314808, 'eval_recall_class_1': 0.9613531629660662, 'eval_f1_class_0': 0.9787182244223084, 'eval_f1_class_1': 0.853661939083934, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6103, 'eval_samples_per_second': 309.674, 'eval_steps_per_second': 9.831, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.63it/s]\n",
      "100% 6/6 [00:00<00:00, 38.68it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1000\n",
      "  eval/accuracy             0.9628\n",
      "  eval/precision            0.7677\n",
      "  eval/recall               0.9614\n",
      "  eval/f1                   0.8537\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9787\n",
      "    eval/f1_class_1         0.8537\n",
      "    eval/precision_class_0  0.9949\n",
      "    eval/precision_class_1  0.7677\n",
      "    eval/recall_class_0     0.9630\n",
      "    eval/recall_class_1     0.9614\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0929, 'grad_norm': 0.2656490206718445, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0902, 'grad_norm': 0.2195712774991989, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.094, 'grad_norm': 0.23778139054775238, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.091, 'grad_norm': 0.19699063897132874, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.45it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09653450548648834, 'eval_accuracy': 0.9653319793597752, 'eval_precision': 0.7820337826309504, 'eval_recall': 0.9600963552576456, 'eval_f1': 0.8619652092148566, 'eval_precision_class_0': 0.9947784614962929, 'eval_precision_class_1': 0.7820337826309504, 'eval_recall_class_0': 0.965997258487377, 'eval_recall_class_1': 0.9600963552576456, 'eval_f1_class_0': 0.9801766278661517, 'eval_f1_class_1': 0.8619652092148566, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.619, 'eval_samples_per_second': 305.329, 'eval_steps_per_second': 9.693, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.45it/s]\n",
      "100% 6/6 [00:00<00:00, 38.50it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0965\n",
      "  eval/accuracy             0.9653\n",
      "  eval/precision            0.7820\n",
      "  eval/recall               0.9601\n",
      "  eval/f1                   0.8620\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9802\n",
      "    eval/f1_class_1         0.8620\n",
      "    eval/precision_class_0  0.9948\n",
      "    eval/precision_class_1  0.7820\n",
      "    eval/recall_class_0     0.9660\n",
      "    eval/recall_class_1     0.9601\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0945, 'grad_norm': 0.3396679759025574, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.091, 'grad_norm': 0.2146950662136078, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0895, 'grad_norm': 0.1723840981721878, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.087, 'grad_norm': 0.3519667387008667, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09179314225912094, 'eval_accuracy': 0.9671858210629479, 'eval_precision': 0.7923974082073434, 'eval_recall': 0.9606200251361542, 'eval_f1': 0.8684372484968991, 'eval_precision_class_0': 0.9948573460623137, 'eval_precision_class_1': 0.7923974082073434, 'eval_recall_class_0': 0.9680201221703199, 'eval_recall_class_1': 0.9606200251361542, 'eval_f1_class_0': 0.9812552696367745, 'eval_f1_class_1': 0.8684372484968991, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6046, 'eval_samples_per_second': 312.605, 'eval_steps_per_second': 9.924, 'epoch': 8.0}\n",
      " 80% 624/780 [00:52<00:10, 14.49it/s]\n",
      "100% 6/6 [00:00<00:00, 38.65it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0918\n",
      "  eval/accuracy             0.9672\n",
      "  eval/precision            0.7924\n",
      "  eval/recall               0.9606\n",
      "  eval/f1                   0.8684\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9813\n",
      "    eval/f1_class_1         0.8684\n",
      "    eval/precision_class_0  0.9949\n",
      "    eval/precision_class_1  0.7924\n",
      "    eval/recall_class_0     0.9680\n",
      "    eval/recall_class_1     0.9606\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.083, 'grad_norm': 0.327291876077652, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0855, 'grad_norm': 0.18982093036174774, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0899, 'grad_norm': 0.25038716197013855, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0867, 'grad_norm': 0.2490445077419281, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.30it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09054582566022873, 'eval_accuracy': 0.969075086492933, 'eval_precision': 0.8048394192696876, 'eval_recall': 0.9580016757436113, 'eval_f1': 0.8747668914072586, 'eval_precision_class_0': 0.9945311221428181, 'eval_precision_class_1': 0.8048394192696876, 'eval_recall_class_0': 0.9704821602054804, 'eval_recall_class_1': 0.9580016757436113, 'eval_f1_class_0': 0.9823594786650052, 'eval_f1_class_1': 0.8747668914072586, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6103, 'eval_samples_per_second': 309.673, 'eval_steps_per_second': 9.831, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.30it/s]\n",
      "100% 6/6 [00:00<00:00, 38.75it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0905\n",
      "  eval/accuracy             0.9691\n",
      "  eval/precision            0.8048\n",
      "  eval/recall               0.9580\n",
      "  eval/f1                   0.8748\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9824\n",
      "    eval/f1_class_1         0.8748\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.8048\n",
      "    eval/recall_class_0     0.9705\n",
      "    eval/recall_class_1     0.9580\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0847, 'grad_norm': 0.1711321920156479, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0799, 'grad_norm': 0.34009256958961487, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.087, 'grad_norm': 0.2097448855638504, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0891, 'grad_norm': 0.27703920006752014, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09069392085075378, 'eval_accuracy': 0.9687326571337482, 'eval_precision': 0.8019957983193278, 'eval_recall': 0.959572685379137, 'eval_f1': 0.8737364104520313, 'eval_precision_class_0': 0.9947314543096977, 'eval_precision_class_1': 0.8019957983193278, 'eval_recall_class_0': 0.9698965944025233, 'eval_recall_class_1': 0.959572685379137, 'eval_f1_class_0': 0.9821570556446505, 'eval_f1_class_1': 0.8737364104520313, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6119, 'eval_samples_per_second': 308.857, 'eval_steps_per_second': 9.805, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.41it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0907\n",
      "  eval/accuracy             0.9687\n",
      "  eval/precision            0.8020\n",
      "  eval/recall               0.9596\n",
      "  eval/f1                   0.8737\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9822\n",
      "    eval/f1_class_1         0.8737\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8020\n",
      "    eval/recall_class_0     0.9699\n",
      "    eval/recall_class_1     0.9596\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.2389, 'train_samples_per_second': 191.144, 'train_steps_per_second': 11.956, 'train_loss': 0.1436337186739995, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.96it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.2389\n",
      "train_samples_per_second.......................... 191.1440\n",
      "train_steps_per_second............................ 11.9560\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1436\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.66it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0905\n",
      "  eval/accuracy             0.9691\n",
      "  eval/precision            0.8048\n",
      "  eval/recall               0.9580\n",
      "  eval/f1                   0.8748\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9824\n",
      "    eval/f1_class_1         0.8748\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.8048\n",
      "    eval/recall_class_0     0.9705\n",
      "    eval/recall_class_1     0.9580\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.63it/s]\n",
      "eval_loss......................................... 0.0905\n",
      "eval_accuracy..................................... 0.9691\n",
      "eval_precision.................................... 0.8048\n",
      "eval_recall....................................... 0.9580\n",
      "eval_f1........................................... 0.8748\n",
      "eval_precision_class_0............................ 0.9945\n",
      "eval_precision_class_1............................ 0.8048\n",
      "eval_recall_class_0............................... 0.9705\n",
      "eval_recall_class_1............................... 0.9580\n",
      "eval_f1_class_0................................... 0.9824\n",
      "eval_f1_class_1................................... 0.8748\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6152\n",
      "eval_samples_per_second........................... 307.2070\n",
      "eval_steps_per_second............................. 9.7530\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.73it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0686\n",
      "eval_model_preparation_time....................... 0.0045\n",
      "eval_accuracy..................................... 0.9751\n",
      "eval_precision.................................... 0.8417\n",
      "eval_recall....................................... 0.9555\n",
      "eval_f1........................................... 0.8950\n",
      "eval_precision_class_0............................ 0.9944\n",
      "eval_precision_class_1............................ 0.8417\n",
      "eval_recall_class_0............................... 0.9776\n",
      "eval_recall_class_1............................... 0.9555\n",
      "eval_f1_class_0................................... 0.9859\n",
      "eval_f1_class_1................................... 0.8950\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8123\n",
      "eval_samples_per_second........................... 301.6170\n",
      "eval_steps_per_second............................. 19.6970\n",
      "100% 16/16 [00:00<00:00, 24.95it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9944    0.9776    0.9859     99651\n",
      "   EDU Start (1)     0.8417    0.9555    0.8950     12438\n",
      "\n",
      "        accuracy                         0.9751    112089\n",
      "       macro avg     0.9180    0.9666    0.9405    112089\n",
      "    weighted avg     0.9774    0.9751    0.9758    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9691               0.9751              \n",
      "Precision                      0.8048               0.8417              \n",
      "Recall                         0.9580               0.9555              \n",
      "F1                             0.8748               0.8950              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-28s6c44k-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-28s6c44k-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-28s6c44k-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.4s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-28s6c44k-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-28s6c44k-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96908\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.87477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98236\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.87477\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09055\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.80484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.80484\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.958\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r16_a64_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/28s6c44k\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_031549-28s6c44k/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 64 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_64_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MOdx6fP4hV-u",
    "outputId": "4baacc58-1ee3-43eb-df1f-28687e15e13d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_031730-zuax3yii\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/zuax3yii\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 128\n",
      "  Effective LR scale:    8.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.683, 'grad_norm': 0.7655841112136841, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.5558, 'grad_norm': 0.7643767595291138, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.317, 'grad_norm': 1.1595630645751953, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:48, 14.61it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.1675213873386383, 'eval_accuracy': 0.9438888167294454, 'eval_precision': 0.6915947587088527, 'eval_recall': 0.9065772936740679, 'eval_f1': 0.7846265409717187, 'eval_precision_class_0': 0.9876408075041914, 'eval_precision_class_1': 0.6915947587088527, 'eval_recall_class_0': 0.9486299091042174, 'eval_recall_class_1': 0.9065772936740679, 'eval_f1_class_0': 0.967742373433618, 'eval_f1_class_1': 0.7846265409717187, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6138, 'eval_samples_per_second': 307.907, 'eval_steps_per_second': 9.775, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:48, 14.61it/s]\n",
      "100% 6/6 [00:00<00:00, 37.49it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1675\n",
      "  eval/accuracy             0.9439\n",
      "  eval/precision            0.6916\n",
      "  eval/recall               0.9066\n",
      "  eval/f1                   0.7846\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9677\n",
      "    eval/f1_class_1         0.7846\n",
      "    eval/precision_class_0  0.9876\n",
      "    eval/precision_class_1  0.6916\n",
      "    eval/recall_class_0     0.9486\n",
      "    eval/recall_class_1     0.9066\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2015, 'grad_norm': 1.3771294355392456, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1606, 'grad_norm': 1.2981486320495605, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1471, 'grad_norm': 0.8118991851806641, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1362, 'grad_norm': 0.6783084273338318, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.63it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11701247096061707, 'eval_accuracy': 0.957538759461087, 'eval_precision': 0.7442146725750861, 'eval_recall': 0.9498324256388773, 'eval_f1': 0.8345449526088157, 'eval_precision_class_0': 0.9933933768257865, 'eval_precision_class_1': 0.7442146725750861, 'eval_recall_class_0': 0.9585179861859704, 'eval_recall_class_1': 0.9498324256388773, 'eval_f1_class_0': 0.9756441169299125, 'eval_f1_class_1': 0.8345449526088157, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6109, 'eval_samples_per_second': 309.403, 'eval_steps_per_second': 9.822, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.63it/s]\n",
      "100% 6/6 [00:00<00:00, 38.48it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1170\n",
      "  eval/accuracy             0.9575\n",
      "  eval/precision            0.7442\n",
      "  eval/recall               0.9498\n",
      "  eval/f1                   0.8345\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9756\n",
      "    eval/f1_class_1         0.8345\n",
      "    eval/precision_class_0  0.9934\n",
      "    eval/precision_class_1  0.7442\n",
      "    eval/recall_class_0     0.9585\n",
      "    eval/recall_class_1     0.9498\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1209, 'grad_norm': 0.33311185240745544, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1178, 'grad_norm': 0.25538039207458496, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1165, 'grad_norm': 0.25437459349632263, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1097, 'grad_norm': 0.6644843220710754, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:19<00:37, 14.43it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10186465084552765, 'eval_accuracy': 0.9621438439466755, 'eval_precision': 0.7649565508021391, 'eval_recall': 0.958839547549225, 'eval_f1': 0.8509946086633203, 'eval_precision_class_0': 0.9945957838863602, 'eval_precision_class_1': 0.7649565508021391, 'eval_recall_class_0': 0.9625637135518559, 'eval_recall_class_1': 0.958839547549225, 'eval_f1_class_0': 0.9783176204839648, 'eval_f1_class_1': 0.8509946086633203, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6027, 'eval_samples_per_second': 313.59, 'eval_steps_per_second': 9.955, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:37, 14.43it/s]\n",
      "100% 6/6 [00:00<00:00, 38.63it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1019\n",
      "  eval/accuracy             0.9621\n",
      "  eval/precision            0.7650\n",
      "  eval/recall               0.9588\n",
      "  eval/f1                   0.8510\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9783\n",
      "    eval/f1_class_1         0.8510\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.7650\n",
      "    eval/recall_class_0     0.9626\n",
      "    eval/recall_class_1     0.9588\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1026, 'grad_norm': 0.3853118419647217, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1036, 'grad_norm': 0.4802241325378418, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.0969, 'grad_norm': 0.3325744569301605, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1021, 'grad_norm': 0.3526912033557892, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:32, 14.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09163996577262878, 'eval_accuracy': 0.9709879677407928, 'eval_precision': 0.8229935319303999, 'eval_recall': 0.9461667364893172, 'eval_f1': 0.8802923264311815, 'eval_precision_class_0': 0.9930269155632733, 'eval_precision_class_1': 0.8229935319303999, 'eval_recall_class_0': 0.9741419464739622, 'eval_recall_class_1': 0.9461667364893172, 'eval_f1_class_0': 0.9834937824565175, 'eval_f1_class_1': 0.8802923264311815, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6063, 'eval_samples_per_second': 311.716, 'eval_steps_per_second': 9.896, 'epoch': 4.0}\n",
      " 40% 312/780 [00:26<00:32, 14.49it/s]\n",
      "100% 6/6 [00:00<00:00, 38.63it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0916\n",
      "  eval/accuracy             0.9710\n",
      "  eval/precision            0.8230\n",
      "  eval/recall               0.9462\n",
      "  eval/f1                   0.8803\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9835\n",
      "    eval/f1_class_1         0.8803\n",
      "    eval/precision_class_0  0.9930\n",
      "    eval/precision_class_1  0.8230\n",
      "    eval/recall_class_0     0.9741\n",
      "    eval/recall_class_1     0.9462\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0935, 'grad_norm': 0.25913742184638977, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.0914, 'grad_norm': 0.3763195872306824, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.0886, 'grad_norm': 0.24184589087963104, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.091, 'grad_norm': 0.2900379002094269, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:32<00:27, 14.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.0903843492269516, 'eval_accuracy': 0.9708816965603562, 'eval_precision': 0.8172370542913456, 'eval_recall': 0.9553833263510683, 'eval_f1': 0.8809270883631096, 'eval_precision_class_0': 0.9942062099636868, 'eval_precision_class_1': 0.8172370542913456, 'eval_recall_class_0': 0.9728510400447159, 'eval_recall_class_1': 0.9553833263510683, 'eval_f1_class_0': 0.9834127048187908, 'eval_f1_class_1': 0.8809270883631096, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6166, 'eval_samples_per_second': 306.502, 'eval_steps_per_second': 9.73, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.48it/s]\n",
      "100% 6/6 [00:00<00:00, 38.64it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0904\n",
      "  eval/accuracy             0.9709\n",
      "  eval/precision            0.8172\n",
      "  eval/recall               0.9554\n",
      "  eval/f1                   0.8809\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9834\n",
      "    eval/f1_class_1         0.8809\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.8172\n",
      "    eval/recall_class_0     0.9729\n",
      "    eval/recall_class_1     0.9554\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0894, 'grad_norm': 0.26741617918014526, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0838, 'grad_norm': 0.262501984834671, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.0848, 'grad_norm': 0.34206217527389526, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0825, 'grad_norm': 0.2126334011554718, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:38<00:21, 14.63it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08976680785417557, 'eval_accuracy': 0.9709761598318554, 'eval_precision': 0.8177092668937086, 'eval_recall': 0.9555927943024717, 'eval_f1': 0.8812904472133681, 'eval_precision_class_0': 0.9942337245515497, 'eval_precision_class_1': 0.8177092668937086, 'eval_recall_class_0': 0.9729308899269373, 'eval_recall_class_1': 0.9555927943024717, 'eval_f1_class_0': 0.9834669608265174, 'eval_f1_class_1': 0.8812904472133681, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6288, 'eval_samples_per_second': 300.578, 'eval_steps_per_second': 9.542, 'epoch': 6.0}\n",
      " 60% 468/780 [00:39<00:21, 14.63it/s]\n",
      "100% 6/6 [00:00<00:00, 38.61it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0898\n",
      "  eval/accuracy             0.9710\n",
      "  eval/precision            0.8177\n",
      "  eval/recall               0.9556\n",
      "  eval/f1                   0.8813\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9835\n",
      "    eval/f1_class_1         0.8813\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.8177\n",
      "    eval/recall_class_0     0.9729\n",
      "    eval/recall_class_1     0.9556\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0782, 'grad_norm': 0.32771262526512146, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0776, 'grad_norm': 0.2563493549823761, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0812, 'grad_norm': 0.25691109895706177, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0788, 'grad_norm': 0.3260762095451355, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:16, 14.56it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08790790289640427, 'eval_accuracy': 0.9716964422770371, 'eval_precision': 0.8197835613988015, 'eval_recall': 0.9599916212819438, 'eval_f1': 0.8843648994162767, 'eval_precision_class_0': 0.9948032867170921, 'eval_precision_class_1': 0.8197835613988015, 'eval_recall_class_0': 0.9731837478873052, 'eval_recall_class_1': 0.9599916212819438, 'eval_f1_class_0': 0.9838747653869182, 'eval_f1_class_1': 0.8843648994162767, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6189, 'eval_samples_per_second': 305.373, 'eval_steps_per_second': 9.694, 'epoch': 7.0}\n",
      " 70% 546/780 [00:45<00:16, 14.56it/s]\n",
      "100% 6/6 [00:00<00:00, 38.33it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0879\n",
      "  eval/accuracy             0.9717\n",
      "  eval/precision            0.8198\n",
      "  eval/recall               0.9600\n",
      "  eval/f1                   0.8844\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9839\n",
      "    eval/f1_class_1         0.8844\n",
      "    eval/precision_class_0  0.9948\n",
      "    eval/precision_class_1  0.8198\n",
      "    eval/recall_class_0     0.9732\n",
      "    eval/recall_class_1     0.9600\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.081, 'grad_norm': 0.23726989328861237, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0764, 'grad_norm': 0.24517633020877838, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.0758, 'grad_norm': 0.2339096963405609, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0748, 'grad_norm': 0.30228686332702637, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:51<00:10, 14.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08695156127214432, 'eval_accuracy': 0.9738927133393948, 'eval_precision': 0.8363436325295681, 'eval_recall': 0.9553833263510683, 'eval_f1': 0.8919090686873625, 'eval_precision_class_0': 0.994226234040823, 'eval_precision_class_1': 0.8363436325295681, 'eval_recall_class_0': 0.9762446600391265, 'eval_recall_class_1': 0.9553833263510683, 'eval_f1_class_0': 0.9851534014222115, 'eval_f1_class_1': 0.8919090686873625, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.614, 'eval_samples_per_second': 307.835, 'eval_steps_per_second': 9.773, 'epoch': 8.0}\n",
      " 80% 624/780 [00:51<00:10, 14.49it/s]\n",
      "100% 6/6 [00:00<00:00, 38.52it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0870\n",
      "  eval/accuracy             0.9739\n",
      "  eval/precision            0.8363\n",
      "  eval/recall               0.9554\n",
      "  eval/f1                   0.8919\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9852\n",
      "    eval/f1_class_1         0.8919\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.8363\n",
      "    eval/recall_class_0     0.9762\n",
      "    eval/recall_class_1     0.9554\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.071, 'grad_norm': 0.38906076550483704, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0716, 'grad_norm': 0.2762344181537628, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0754, 'grad_norm': 0.25913429260253906, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0743, 'grad_norm': 0.2531014084815979, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:57<00:05, 14.56it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08397173136472702, 'eval_accuracy': 0.972794577808216, 'eval_precision': 0.8266005410279531, 'eval_recall': 0.9600963552576456, 'eval_f1': 0.8883612753173757, 'eval_precision_class_0': 0.9948232992296091, 'eval_precision_class_1': 0.8266005410279531, 'eval_recall_class_0': 0.9744081127480337, 'eval_recall_class_1': 0.9600963552576456, 'eval_f1_class_0': 0.9845098830173457, 'eval_f1_class_1': 0.8883612753173757, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.613, 'eval_samples_per_second': 308.296, 'eval_steps_per_second': 9.787, 'epoch': 9.0}\n",
      " 90% 702/780 [00:58<00:05, 14.56it/s]\n",
      "100% 6/6 [00:00<00:00, 38.46it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0840\n",
      "  eval/accuracy             0.9728\n",
      "  eval/precision            0.8266\n",
      "  eval/recall               0.9601\n",
      "  eval/f1                   0.8884\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9845\n",
      "    eval/f1_class_1         0.8884\n",
      "    eval/precision_class_0  0.9948\n",
      "    eval/precision_class_1  0.8266\n",
      "    eval/recall_class_0     0.9744\n",
      "    eval/recall_class_1     0.9601\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.071, 'grad_norm': 0.23297394812107086, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0679, 'grad_norm': 0.2727842330932617, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0734, 'grad_norm': 0.24946357309818268, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0761, 'grad_norm': 0.3397115468978882, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.34it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08477012068033218, 'eval_accuracy': 0.9736329393427718, 'eval_precision': 0.8330146590184832, 'eval_recall': 0.9582111436950147, 'eval_f1': 0.8912376406409819, 'eval_precision_class_0': 0.9945866008194719, 'eval_precision_class_1': 0.8330146590184832, 'eval_recall_class_0': 0.9755925526676514, 'eval_recall_class_1': 0.9582111436950147, 'eval_f1_class_0': 0.9849980180991219, 'eval_f1_class_1': 0.8912376406409819, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6281, 'eval_samples_per_second': 300.93, 'eval_steps_per_second': 9.553, 'epoch': 10.0}\n",
      "100% 780/780 [01:04<00:00, 14.34it/s]\n",
      "100% 6/6 [00:00<00:00, 38.33it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0848\n",
      "  eval/accuracy             0.9736\n",
      "  eval/precision            0.8330\n",
      "  eval/recall               0.9582\n",
      "  eval/f1                   0.8912\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9850\n",
      "    eval/f1_class_1         0.8912\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.8330\n",
      "    eval/recall_class_0     0.9756\n",
      "    eval/recall_class_1     0.9582\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 65.1014, 'train_samples_per_second': 191.547, 'train_steps_per_second': 11.981, 'train_loss': 0.12771341418608642, 'epoch': 10.0}\n",
      "100% 780/780 [01:05<00:00, 11.98it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 65.1014\n",
      "train_samples_per_second.......................... 191.5470\n",
      "train_steps_per_second............................ 11.9810\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1277\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.39it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0870\n",
      "  eval/accuracy             0.9739\n",
      "  eval/precision            0.8363\n",
      "  eval/recall               0.9554\n",
      "  eval/f1                   0.8919\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9852\n",
      "    eval/f1_class_1         0.8919\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.8363\n",
      "    eval/recall_class_0     0.9762\n",
      "    eval/recall_class_1     0.9554\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.58it/s]\n",
      "eval_loss......................................... 0.0870\n",
      "eval_accuracy..................................... 0.9739\n",
      "eval_precision.................................... 0.8363\n",
      "eval_recall....................................... 0.9554\n",
      "eval_f1........................................... 0.8919\n",
      "eval_precision_class_0............................ 0.9942\n",
      "eval_precision_class_1............................ 0.8363\n",
      "eval_recall_class_0............................... 0.9762\n",
      "eval_recall_class_1............................... 0.9554\n",
      "eval_f1_class_0................................... 0.9852\n",
      "eval_f1_class_1................................... 0.8919\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6161\n",
      "eval_samples_per_second........................... 306.7910\n",
      "eval_steps_per_second............................. 9.7390\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 24.67it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0621\n",
      "eval_model_preparation_time....................... 0.0044\n",
      "eval_accuracy..................................... 0.9781\n",
      "eval_precision.................................... 0.8638\n",
      "eval_recall....................................... 0.9530\n",
      "eval_f1........................................... 0.9062\n",
      "eval_precision_class_0............................ 0.9941\n",
      "eval_precision_class_1............................ 0.8638\n",
      "eval_recall_class_0............................... 0.9812\n",
      "eval_recall_class_1............................... 0.9530\n",
      "eval_f1_class_0................................... 0.9876\n",
      "eval_f1_class_1................................... 0.9062\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.8102\n",
      "eval_samples_per_second........................... 302.4040\n",
      "eval_steps_per_second............................. 19.7490\n",
      "100% 16/16 [00:00<00:00, 24.62it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9941    0.9812    0.9876     99651\n",
      "   EDU Start (1)     0.8638    0.9530    0.9062     12438\n",
      "\n",
      "        accuracy                         0.9781    112089\n",
      "       macro avg     0.9289    0.9671    0.9469    112089\n",
      "    weighted avg     0.9796    0.9781    0.9786    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9739               0.9781              \n",
      "Precision                      0.8363               0.8638              \n",
      "Recall                         0.9554               0.9530              \n",
      "F1                             0.8919               0.9062              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-zuax3yii-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-zuax3yii-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-zuax3yii-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-zuax3yii-test_confusion_matrix_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-zuax3yii-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñá‚ñá‚ñà‚ñà‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.97389\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.89191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98515\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.89191\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.08695\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.83634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.83634\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95538\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r16_a128_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/zuax3yii\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_031730-zuax3yii/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXwbmQsNhWBE",
    "outputId": "c80a9040-983b-4382-9f0a-88c01d3923cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_032126-sweccoje\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbert-base-uncased_cw0.5_r16_a256_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/sweccoje\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/bert-base-uncased_cw0.5_r16_a256_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing bert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['query', 'value']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              16\n",
      "  Alpha:                 256\n",
      "  Effective LR scale:    16.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        query, value\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 693,506 || all params: 109,484,548 || trainable%: 0.6334\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    bert-base-uncased\n",
      "Output directory:         ./bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.6575, 'grad_norm': 1.0470292568206787, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.4958, 'grad_norm': 2.4036524295806885, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.2694, 'grad_norm': 0.8372663855552673, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 77/780 [00:06<00:46, 14.99it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.1578771471977234, 'eval_accuracy': 0.939283732243857, 'eval_precision': 0.6631368483412322, 'eval_recall': 0.9378927524088815, 'eval_f1': 0.7769390942217596, 'eval_precision_class_0': 0.991669593313198, 'eval_precision_class_1': 0.6631368483412322, 'eval_recall_class_0': 0.9394604809624573, 'eval_recall_class_1': 0.9378927524088815, 'eval_f1_class_0': 0.9648592867979716, 'eval_f1_class_1': 0.7769390942217596, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5998, 'eval_samples_per_second': 315.086, 'eval_steps_per_second': 10.003, 'epoch': 1.0}\n",
      " 10% 78/780 [00:06<00:46, 14.99it/s]\n",
      "100% 6/6 [00:00<00:00, 37.82it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1579\n",
      "  eval/accuracy             0.9393\n",
      "  eval/precision            0.6631\n",
      "  eval/recall               0.9379\n",
      "  eval/f1                   0.7769\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9649\n",
      "    eval/f1_class_1         0.7769\n",
      "    eval/precision_class_0  0.9917\n",
      "    eval/precision_class_1  0.6631\n",
      "    eval/recall_class_0     0.9395\n",
      "    eval/recall_class_1     0.9379\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1842, 'grad_norm': 0.6900140047073364, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1555, 'grad_norm': 1.5171931982040405, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1367, 'grad_norm': 1.1247482299804688, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1322, 'grad_norm': 0.3892115652561188, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 155/780 [00:12<00:42, 14.75it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11246605217456818, 'eval_accuracy': 0.9578929967292092, 'eval_precision': 0.7427365687388411, 'eval_recall': 0.9585253456221198, 'eval_f1': 0.8369455875628715, 'eval_precision_class_0': 0.9945278925477082, 'eval_precision_class_1': 0.7427365687388411, 'eval_recall_class_0': 0.9578126455596812, 'eval_recall_class_1': 0.9585253456221198, 'eval_f1_class_0': 0.9758250399978307, 'eval_f1_class_1': 0.8369455875628715, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5981, 'eval_samples_per_second': 316.016, 'eval_steps_per_second': 10.032, 'epoch': 2.0}\n",
      " 20% 156/780 [00:13<00:42, 14.75it/s]\n",
      "100% 6/6 [00:00<00:00, 38.77it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1125\n",
      "  eval/accuracy             0.9579\n",
      "  eval/precision            0.7427\n",
      "  eval/recall               0.9585\n",
      "  eval/f1                   0.8369\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9758\n",
      "    eval/f1_class_1         0.8369\n",
      "    eval/precision_class_0  0.9945\n",
      "    eval/precision_class_1  0.7427\n",
      "    eval/recall_class_0     0.9578\n",
      "    eval/recall_class_1     0.9585\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1157, 'grad_norm': 0.6980617046356201, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1113, 'grad_norm': 0.4050045609474182, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1078, 'grad_norm': 0.7544416189193726, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1021, 'grad_norm': 0.6452640891075134, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 233/780 [00:18<00:36, 14.86it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09338334202766418, 'eval_accuracy': 0.9668906233395128, 'eval_precision': 0.7912923289564616, 'eval_recall': 0.9593632174277336, 'eval_f1': 0.8672599886385154, 'eval_precision_class_0': 0.9946931462257054, 'eval_precision_class_1': 0.7912923289564616, 'eval_recall_class_0': 0.9678471140921734, 'eval_recall_class_1': 0.9593632174277336, 'eval_f1_class_0': 0.9810865136859713, 'eval_f1_class_1': 0.8672599886385154, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5994, 'eval_samples_per_second': 315.302, 'eval_steps_per_second': 10.01, 'epoch': 3.0}\n",
      " 30% 234/780 [00:19<00:36, 14.86it/s]\n",
      "100% 6/6 [00:00<00:00, 38.79it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0934\n",
      "  eval/accuracy             0.9669\n",
      "  eval/precision            0.7913\n",
      "  eval/recall               0.9594\n",
      "  eval/f1                   0.8673\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9811\n",
      "    eval/f1_class_1         0.8673\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.7913\n",
      "    eval/recall_class_0     0.9678\n",
      "    eval/recall_class_1     0.9594\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0925, 'grad_norm': 0.48836734890937805, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.0935, 'grad_norm': 0.5779935121536255, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.0853, 'grad_norm': 0.3891408443450928, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.0907, 'grad_norm': 0.3416025638580322, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 311/780 [00:25<00:31, 14.77it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08645395934581757, 'eval_accuracy': 0.9719680241825975, 'eval_precision': 0.8225139363423845, 'eval_recall': 0.9581064097193129, 'eval_f1': 0.88514755684567, 'eval_precision_class_0': 0.9945627795071159, 'eval_precision_class_1': 0.8225139363423845, 'eval_recall_class_0': 0.9737293887491516, 'eval_recall_class_1': 0.9581064097193129, 'eval_f1_class_0': 0.9840358286037066, 'eval_f1_class_1': 0.88514755684567, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6052, 'eval_samples_per_second': 312.294, 'eval_steps_per_second': 9.914, 'epoch': 4.0}\n",
      " 40% 312/780 [00:25<00:31, 14.77it/s]\n",
      "100% 6/6 [00:00<00:00, 38.61it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0865\n",
      "  eval/accuracy             0.9720\n",
      "  eval/precision            0.8225\n",
      "  eval/recall               0.9581\n",
      "  eval/f1                   0.8851\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9840\n",
      "    eval/f1_class_1         0.8851\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.8225\n",
      "    eval/recall_class_0     0.9737\n",
      "    eval/recall_class_1     0.9581\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0835, 'grad_norm': 0.6338822245597839, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.081, 'grad_norm': 0.3607620298862457, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.0786, 'grad_norm': 0.36594119668006897, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.0827, 'grad_norm': 0.4643038511276245, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 389/780 [00:31<00:26, 14.90it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08230118453502655, 'eval_accuracy': 0.9731370071674007, 'eval_precision': 0.8289461781999096, 'eval_recall': 0.9597821533305404, 'eval_f1': 0.8895791874969664, 'eval_precision_class_0': 0.9947850177906945, 'eval_precision_class_1': 0.8289461781999096, 'eval_recall_class_0': 0.974833978786548, 'eval_recall_class_1': 0.9597821533305404, 'eval_f1_class_0': 0.9847084523609477, 'eval_f1_class_1': 0.8895791874969664, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5933, 'eval_samples_per_second': 318.537, 'eval_steps_per_second': 10.112, 'epoch': 5.0}\n",
      " 50% 390/780 [00:32<00:26, 14.90it/s]\n",
      "100% 6/6 [00:00<00:00, 38.88it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0823\n",
      "  eval/accuracy             0.9731\n",
      "  eval/precision            0.8289\n",
      "  eval/recall               0.9598\n",
      "  eval/f1                   0.8896\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9847\n",
      "    eval/f1_class_1         0.8896\n",
      "    eval/precision_class_0  0.9948\n",
      "    eval/precision_class_1  0.8289\n",
      "    eval/recall_class_0     0.9748\n",
      "    eval/recall_class_1     0.9598\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0794, 'grad_norm': 0.4387197196483612, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0752, 'grad_norm': 0.3887485861778259, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.074, 'grad_norm': 0.3966725468635559, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0729, 'grad_norm': 0.36336204409599304, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 467/780 [00:37<00:21, 14.50it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08324778825044632, 'eval_accuracy': 0.9726528829009671, 'eval_precision': 0.8241886318809396, 'eval_recall': 0.9628194386258903, 'eval_f1': 0.888126751038547, 'eval_precision_class_0': 0.9951723669001156, 'eval_precision_class_1': 0.8241886318809396, 'eval_recall_class_0': 0.9739023968272981, 'eval_recall_class_1': 0.9628194386258903, 'eval_f1_class_0': 0.9844225026231537, 'eval_f1_class_1': 0.888126751038547, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.603, 'eval_samples_per_second': 313.45, 'eval_steps_per_second': 9.951, 'epoch': 6.0}\n",
      " 60% 468/780 [00:38<00:21, 14.50it/s]\n",
      "100% 6/6 [00:00<00:00, 38.88it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0832\n",
      "  eval/accuracy             0.9727\n",
      "  eval/precision            0.8242\n",
      "  eval/recall               0.9628\n",
      "  eval/f1                   0.8881\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9844\n",
      "    eval/f1_class_1         0.8881\n",
      "    eval/precision_class_0  0.9952\n",
      "    eval/precision_class_1  0.8242\n",
      "    eval/recall_class_0     0.9739\n",
      "    eval/recall_class_1     0.9628\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0683, 'grad_norm': 0.4296802580356598, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0674, 'grad_norm': 0.43997493386268616, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0701, 'grad_norm': 0.3973921835422516, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0691, 'grad_norm': 0.44229981303215027, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 545/780 [00:44<00:15, 14.80it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08363117277622223, 'eval_accuracy': 0.9741524873360177, 'eval_precision': 0.8344087976006543, 'eval_recall': 0.9615626309174696, 'eval_f1': 0.8934845019707071, 'eval_precision_class_0': 0.9950194066715523, 'eval_precision_class_1': 0.8344087976006543, 'eval_recall_class_0': 0.9757522524320943, 'eval_recall_class_1': 0.9615626309174696, 'eval_f1_class_0': 0.9852916473489354, 'eval_f1_class_1': 0.8934845019707071, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.606, 'eval_samples_per_second': 311.862, 'eval_steps_per_second': 9.9, 'epoch': 7.0}\n",
      " 70% 546/780 [00:44<00:15, 14.80it/s]\n",
      "100% 6/6 [00:00<00:00, 38.81it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0836\n",
      "  eval/accuracy             0.9742\n",
      "  eval/precision            0.8344\n",
      "  eval/recall               0.9616\n",
      "  eval/f1                   0.8935\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9853\n",
      "    eval/f1_class_1         0.8935\n",
      "    eval/precision_class_0  0.9950\n",
      "    eval/precision_class_1  0.8344\n",
      "    eval/recall_class_0     0.9758\n",
      "    eval/recall_class_1     0.9616\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0703, 'grad_norm': 0.3619251847267151, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0649, 'grad_norm': 0.3532651662826538, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.066, 'grad_norm': 0.39749282598495483, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0639, 'grad_norm': 0.4134107530117035, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 623/780 [00:50<00:10, 14.82it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08201608061790466, 'eval_accuracy': 0.9761480239464393, 'eval_precision': 0.8495542347696879, 'eval_recall': 0.9581064097193129, 'eval_f1': 0.9005709785390825, 'eval_precision_class_0': 0.9945888177919671, 'eval_precision_class_1': 0.8495542347696879, 'eval_recall_class_0': 0.9784405318002156, 'eval_recall_class_1': 0.9581064097193129, 'eval_f1_class_0': 0.9864485918611048, 'eval_f1_class_1': 0.9005709785390825, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5961, 'eval_samples_per_second': 317.038, 'eval_steps_per_second': 10.065, 'epoch': 8.0}\n",
      " 80% 624/780 [00:51<00:10, 14.82it/s]\n",
      "100% 6/6 [00:00<00:00, 38.85it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0820\n",
      "  eval/accuracy             0.9761\n",
      "  eval/precision            0.8496\n",
      "  eval/recall               0.9581\n",
      "  eval/f1                   0.9006\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9864\n",
      "    eval/f1_class_1         0.9006\n",
      "    eval/precision_class_0  0.9946\n",
      "    eval/precision_class_1  0.8496\n",
      "    eval/recall_class_0     0.9784\n",
      "    eval/recall_class_1     0.9581\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0606, 'grad_norm': 0.9729111194610596, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0618, 'grad_norm': 0.3627774715423584, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0652, 'grad_norm': 0.40078920125961304, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0636, 'grad_norm': 0.3304186761379242, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 701/780 [00:56<00:05, 14.77it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08092301338911057, 'eval_accuracy': 0.9760181369481279, 'eval_precision': 0.8479126168656854, 'eval_recall': 0.9593632174277336, 'eval_f1': 0.9002014643015085, 'eval_precision_class_0': 0.9947486668651707, 'eval_precision_class_1': 0.8479126168656854, 'eval_recall_class_0': 0.9781344405850335, 'eval_recall_class_1': 0.9593632174277336, 'eval_f1_class_0': 0.9863715970931441, 'eval_f1_class_1': 0.9002014643015085, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6061, 'eval_samples_per_second': 311.848, 'eval_steps_per_second': 9.9, 'epoch': 9.0}\n",
      " 90% 702/780 [00:57<00:05, 14.77it/s]\n",
      "100% 6/6 [00:00<00:00, 38.92it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0809\n",
      "  eval/accuracy             0.9760\n",
      "  eval/precision            0.8479\n",
      "  eval/recall               0.9594\n",
      "  eval/f1                   0.9002\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9864\n",
      "    eval/f1_class_1         0.9002\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8479\n",
      "    eval/recall_class_0     0.9781\n",
      "    eval/recall_class_1     0.9594\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0599, 'grad_norm': 0.3792441487312317, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.056, 'grad_norm': 0.39927345514297485, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0617, 'grad_norm': 0.37557053565979004, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0652, 'grad_norm': 0.37204739451408386, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [01:03<00:00, 14.79it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.08276272565126419, 'eval_accuracy': 0.9762070634911264, 'eval_precision': 0.8495591647331786, 'eval_recall': 0.9587348135735233, 'eval_f1': 0.9008512522757467, 'eval_precision_class_0': 0.9946694807478962, 'eval_precision_class_1': 0.8495591647331786, 'eval_recall_class_0': 0.978427223486512, 'eval_recall_class_1': 0.9587348135735233, 'eval_f1_class_0': 0.9864815001174063, 'eval_f1_class_1': 0.9008512522757467, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.6046, 'eval_samples_per_second': 312.61, 'eval_steps_per_second': 9.924, 'epoch': 10.0}\n",
      "100% 780/780 [01:03<00:00, 14.79it/s]\n",
      "100% 6/6 [00:00<00:00, 38.80it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0828\n",
      "  eval/accuracy             0.9762\n",
      "  eval/precision            0.8496\n",
      "  eval/recall               0.9587\n",
      "  eval/f1                   0.9009\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9865\n",
      "    eval/f1_class_1         0.9009\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8496\n",
      "    eval/recall_class_0     0.9784\n",
      "    eval/recall_class_1     0.9587\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 63.8702, 'train_samples_per_second': 195.24, 'train_steps_per_second': 12.212, 'train_loss': 0.1151650913250752, 'epoch': 10.0}\n",
      "100% 780/780 [01:03<00:00, 12.21it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 63.8702\n",
      "train_samples_per_second.......................... 195.2400\n",
      "train_steps_per_second............................ 12.2120\n",
      "total_flos........................................ 3281024357376000.0000\n",
      "train_loss........................................ 0.1152\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      " 67% 4/6 [00:00<00:00, 38.73it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0828\n",
      "  eval/accuracy             0.9762\n",
      "  eval/precision            0.8496\n",
      "  eval/recall               0.9587\n",
      "  eval/f1                   0.9009\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9865\n",
      "    eval/f1_class_1         0.9009\n",
      "    eval/precision_class_0  0.9947\n",
      "    eval/precision_class_1  0.8496\n",
      "    eval/recall_class_0     0.9784\n",
      "    eval/recall_class_1     0.9587\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 13.59it/s]\n",
      "eval_loss......................................... 0.0828\n",
      "eval_accuracy..................................... 0.9762\n",
      "eval_precision.................................... 0.8496\n",
      "eval_recall....................................... 0.9587\n",
      "eval_f1........................................... 0.9009\n",
      "eval_precision_class_0............................ 0.9947\n",
      "eval_precision_class_1............................ 0.8496\n",
      "eval_recall_class_0............................... 0.9784\n",
      "eval_recall_class_1............................... 0.9587\n",
      "eval_f1_class_0................................... 0.9865\n",
      "eval_f1_class_1................................... 0.9009\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.6078\n",
      "eval_samples_per_second........................... 310.9600\n",
      "eval_steps_per_second............................. 9.8720\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 25.41it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0606\n",
      "eval_model_preparation_time....................... 0.0044\n",
      "eval_accuracy..................................... 0.9795\n",
      "eval_precision.................................... 0.8694\n",
      "eval_recall....................................... 0.9598\n",
      "eval_f1........................................... 0.9123\n",
      "eval_precision_class_0............................ 0.9949\n",
      "eval_precision_class_1............................ 0.8694\n",
      "eval_recall_class_0............................... 0.9820\n",
      "eval_recall_class_1............................... 0.9598\n",
      "eval_f1_class_0................................... 0.9884\n",
      "eval_f1_class_1................................... 0.9123\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.7942\n",
      "eval_samples_per_second........................... 308.4990\n",
      "eval_steps_per_second............................. 20.1470\n",
      "100% 16/16 [00:00<00:00, 25.13it/s]\n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9949    0.9820    0.9884     99651\n",
      "   EDU Start (1)     0.8694    0.9598    0.9123     12438\n",
      "\n",
      "        accuracy                         0.9795    112089\n",
      "       macro avg     0.9321    0.9709    0.9504    112089\n",
      "    weighted avg     0.9810    0.9795    0.9800    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9762               0.9795              \n",
      "Precision                      0.8496               0.8694              \n",
      "Recall                         0.9587               0.9598              \n",
      "F1                             0.9009               0.9123              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-sweccoje-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-sweccoje-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-sweccoje-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-sweccoje-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-sweccoje-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-sweccoje-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (4.3s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-sweccoje-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-sweccoje-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-sweccoje-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-sweccoje-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-sweccoje-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÑ‚ñÑ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.97621\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.90085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98648\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.90085\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.08276\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.84956\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99467\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.84956\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.95873\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbert-base-uncased_cw0.5_r16_a256_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/sweccoje\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_032126-sweccoje/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 256 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drxbOw12kiiS"
   },
   "outputs": [],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pP_Qo0JfkitI"
   },
   "outputs": [],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name bert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp3_cw_rank_alpha  \\\n",
    "  --class_1_weight_multiplier  0.5 \\\n",
    "  --lora_r 16 \\\n",
    "  --lora_alpha 256 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./bert_grp2_cw0_5_rank_16_alpha_256_ep10_noES\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iVwmhDzC6KZN",
    "outputId": "bb56fe57-9655-47e9-b9cb-d681456d94bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/trainer_state.json (deflated 77%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/optimizer.pt (deflated 9%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/README.md (deflated 66%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/best_model/README.md (deflated 66%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/trainer_state.json (deflated 79%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scheduler.pt (deflated 62%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/optimizer.pt (deflated 9%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/README.md (deflated 66%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/trainer_state.json (deflated 78%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/optimizer.pt (deflated 9%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/README.md (deflated 66%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/trainer_state.json (deflated 77%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/optimizer.pt (deflated 9%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/README.md (deflated 66%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/ (stored 0%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/trainer_state.json (deflated 78%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/optimizer.pt (deflated 9%)\n",
      "  adding: bert_grp1_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/README.md (deflated 66%)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iFrv01YFJWiy",
    "outputId": "b939acf9-252d-466a-fe8f-b6587491a9b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "EDU SEGMENTATION: BERT + LoRA + Linear Head\n",
      "===================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mk_vighnesh\u001b[0m (\u001b[33mk_vighnesh-concordia-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.22.3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/DISRPT-Segmenter/wandb/run-20251122_040057-s2tq4k53\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdistilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/s2tq4k53\u001b[0m\n",
      "‚úì W&B initialized: Multiple-Run-edu-segmentation/distilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\n",
      "\n",
      "======================================================================\n",
      "STEP 1: Download Dataset\n",
      "======================================================================\n",
      "======================================================================\n",
      "Downloading eng.erst.gum dataset from DISRPT 2025\n",
      "======================================================================\n",
      "gum: 3 files\n",
      "scidtb: 3 files\n",
      "oll: 3 files\n",
      "sts: 3 files\n",
      "umuc: 3 files\n",
      "msdc: 3 files\n",
      "stac: 3 files\n",
      "\n",
      "Total files across selected corpora: 21\n",
      "\n",
      "\n",
      "======================================================================\n",
      "STEP 2: Load Datasets\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Discovering datasets in: dataset\n",
      "======================================================================\n",
      "Found 7 dataset(s): ['gum', 'scidtb', 'oll', 'stac', 'sts', 'umuc', 'msdc']\n",
      "  ‚úì gum/train.conllu\n",
      "  ‚úì gum/dev.conllu\n",
      "  ‚úì gum/test.conllu\n",
      "  ‚úì scidtb/train.conllu\n",
      "  ‚úì scidtb/dev.conllu\n",
      "  ‚úì scidtb/test.conllu\n",
      "  ‚úì oll/train.conllu\n",
      "  ‚úì oll/dev.conllu\n",
      "  ‚úì oll/test.conllu\n",
      "  ‚úì stac/train.conllu\n",
      "  ‚úì stac/dev.conllu\n",
      "  ‚úì stac/test.conllu\n",
      "  ‚úì sts/train.conllu\n",
      "  ‚úì sts/dev.conllu\n",
      "  ‚úì sts/test.conllu\n",
      "  ‚úì umuc/train.conllu\n",
      "  ‚úì umuc/dev.conllu\n",
      "  ‚úì umuc/test.conllu\n",
      "  ‚úì msdc/train.conllu\n",
      "  ‚úì msdc/dev.conllu\n",
      "  ‚úì msdc/test.conllu\n",
      "\n",
      "======================================================================\n",
      "Loading and combining datasets for EDU segmentation\n",
      "======================================================================\n",
      "Train files: 7\n",
      "Dev files:   7\n",
      "Test files:  7\n",
      "\n",
      "Loading train: gum/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/train.conllu\n",
      "   Parsed 10910 individual sentences\n",
      "   Combined into 390 document chunks\n",
      "   Avg tokens per chunk: 487.6\n",
      "   Avg EDUs per chunk: 63.5\n",
      "   Tokenizing 390 document chunks...\n",
      "   Processed 100/390 chunks...\n",
      "   Processed 200/390 chunks...\n",
      "   Processed 300/390 chunks...\n",
      "‚úì Loaded 390 training examples\n",
      "\n",
      "Loading train: scidtb/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/train.conllu\n",
      "   Parsed 2570 individual sentences\n",
      "   Combined into 128 document chunks\n",
      "   Avg tokens per chunk: 487.7\n",
      "   Avg EDUs per chunk: 52.6\n",
      "   Tokenizing 128 document chunks...\n",
      "   Processed 100/128 chunks...\n",
      "‚úì Loaded 128 training examples\n",
      "\n",
      "Loading train: oll/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/train.conllu\n",
      "   Parsed 1770 individual sentences\n",
      "   Combined into 76 document chunks\n",
      "   Avg tokens per chunk: 487.1\n",
      "   Avg EDUs per chunk: 33.0\n",
      "   Tokenizing 76 document chunks...\n",
      "‚úì Loaded 76 training examples\n",
      "\n",
      "Loading train: stac/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/train.conllu\n",
      "   Parsed 6833 individual sentences\n",
      "   Combined into 86 document chunks\n",
      "   Avg tokens per chunk: 495.1\n",
      "   Avg EDUs per chunk: 118.1\n",
      "   Tokenizing 86 document chunks...\n",
      "‚úì Loaded 86 training examples\n",
      "\n",
      "Loading train: sts/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/train.conllu\n",
      "   Parsed 2084 individual sentences\n",
      "   Combined into 117 document chunks\n",
      "   Avg tokens per chunk: 482.8\n",
      "   Avg EDUs per chunk: 22.0\n",
      "   Tokenizing 117 document chunks...\n",
      "   Processed 100/117 chunks...\n",
      "‚úì Loaded 117 training examples\n",
      "\n",
      "Loading train: umuc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/train.conllu\n",
      "   Parsed 1950 individual sentences\n",
      "   Combined into 102 document chunks\n",
      "   Avg tokens per chunk: 485.2\n",
      "   Avg EDUs per chunk: 42.5\n",
      "   Tokenizing 102 document chunks...\n",
      "   Processed 100/102 chunks...\n",
      "‚úì Loaded 102 training examples\n",
      "\n",
      "Loading train: msdc/train.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/train.conllu\n",
      "   Parsed 10801 individual sentences\n",
      "   Combined into 348 document chunks\n",
      "   Avg tokens per chunk: 479.1\n",
      "   Avg EDUs per chunk: 46.8\n",
      "   Tokenizing 348 document chunks...\n",
      "   Processed 100/348 chunks...\n",
      "   Processed 200/348 chunks...\n",
      "   Processed 300/348 chunks...\n",
      "‚úì Loaded 348 training examples\n",
      "\n",
      "Loading dev: gum/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/dev.conllu\n",
      "   Parsed 1679 individual sentences\n",
      "   Combined into 61 document chunks\n",
      "   Avg tokens per chunk: 489.7\n",
      "   Avg EDUs per chunk: 63.9\n",
      "   Tokenizing 61 document chunks...\n",
      "‚úì Loaded 61 training examples\n",
      "\n",
      "Loading dev: scidtb/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/dev.conllu\n",
      "   Parsed 815 individual sentences\n",
      "   Combined into 42 document chunks\n",
      "   Avg tokens per chunk: 483.0\n",
      "   Avg EDUs per chunk: 50.7\n",
      "   Tokenizing 42 document chunks...\n",
      "‚úì Loaded 42 training examples\n",
      "\n",
      "Loading dev: oll/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/dev.conllu\n",
      "   Parsed 209 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.9\n",
      "   Avg EDUs per chunk: 28.0\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading dev: stac/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/dev.conllu\n",
      "   Parsed 822 individual sentences\n",
      "   Combined into 11 document chunks\n",
      "   Avg tokens per chunk: 468.1\n",
      "   Avg EDUs per chunk: 112.6\n",
      "   Tokenizing 11 document chunks...\n",
      "‚úì Loaded 11 training examples\n",
      "\n",
      "Loading dev: sts/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/dev.conllu\n",
      "   Parsed 264 individual sentences\n",
      "   Combined into 15 document chunks\n",
      "   Avg tokens per chunk: 470.5\n",
      "   Avg EDUs per chunk: 19.4\n",
      "   Tokenizing 15 document chunks...\n",
      "‚úì Loaded 15 training examples\n",
      "\n",
      "Loading dev: umuc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/dev.conllu\n",
      "   Parsed 236 individual sentences\n",
      "   Combined into 13 document chunks\n",
      "   Avg tokens per chunk: 459.7\n",
      "   Avg EDUs per chunk: 43.5\n",
      "   Tokenizing 13 document chunks...\n",
      "‚úì Loaded 13 training examples\n",
      "\n",
      "Loading dev: msdc/dev.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/dev.conllu\n",
      "   Parsed 1183 individual sentences\n",
      "   Combined into 37 document chunks\n",
      "   Avg tokens per chunk: 483.8\n",
      "   Avg EDUs per chunk: 50.2\n",
      "   Tokenizing 37 document chunks...\n",
      "‚úì Loaded 37 training examples\n",
      "\n",
      "Loading test: gum/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/gum/test.conllu\n",
      "   Parsed 1569 individual sentences\n",
      "   Combined into 62 document chunks\n",
      "   Avg tokens per chunk: 488.0\n",
      "   Avg EDUs per chunk: 60.9\n",
      "   Tokenizing 62 document chunks...\n",
      "‚úì Loaded 62 training examples\n",
      "\n",
      "Loading test: scidtb/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/scidtb/test.conllu\n",
      "   Parsed 817 individual sentences\n",
      "   Combined into 41 document chunks\n",
      "   Avg tokens per chunk: 481.6\n",
      "   Avg EDUs per chunk: 51.6\n",
      "   Tokenizing 41 document chunks...\n",
      "‚úì Loaded 41 training examples\n",
      "\n",
      "Loading test: oll/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/oll/test.conllu\n",
      "   Parsed 177 individual sentences\n",
      "   Combined into 10 document chunks\n",
      "   Avg tokens per chunk: 457.6\n",
      "   Avg EDUs per chunk: 28.8\n",
      "   Tokenizing 10 document chunks...\n",
      "‚úì Loaded 10 training examples\n",
      "\n",
      "Loading test: stac/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/stac/test.conllu\n",
      "   Parsed 840 individual sentences\n",
      "   Combined into 9 document chunks\n",
      "   Avg tokens per chunk: 499.0\n",
      "   Avg EDUs per chunk: 127.1\n",
      "   Tokenizing 9 document chunks...\n",
      "‚úì Loaded 9 training examples\n",
      "\n",
      "Loading test: sts/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/sts/test.conllu\n",
      "   Parsed 243 individual sentences\n",
      "   Combined into 14 document chunks\n",
      "   Avg tokens per chunk: 485.8\n",
      "   Avg EDUs per chunk: 23.9\n",
      "   Tokenizing 14 document chunks...\n",
      "‚úì Loaded 14 training examples\n",
      "\n",
      "Loading test: umuc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/umuc/test.conllu\n",
      "   Parsed 238 individual sentences\n",
      "   Combined into 12 document chunks\n",
      "   Avg tokens per chunk: 485.1\n",
      "   Avg EDUs per chunk: 43.6\n",
      "   Tokenizing 12 document chunks...\n",
      "‚úì Loaded 12 training examples\n",
      "\n",
      "Loading test: msdc/test.conllu\n",
      "\n",
      "üìö Loading dataset from: dataset/msdc/test.conllu\n",
      "   Parsed 3200 individual sentences\n",
      "   Combined into 97 document chunks\n",
      "   Avg tokens per chunk: 481.5\n",
      "   Avg EDUs per chunk: 51.7\n",
      "   Tokenizing 97 document chunks...\n",
      "‚úì Loaded 97 training examples\n",
      "\n",
      "======================================================================\n",
      "Dataset Loading Complete!\n",
      "======================================================================\n",
      "Train: 1247 examples (from 7 file(s))\n",
      "Dev:   189 examples (from 7 file(s))\n",
      "Test:  245 examples (from 7 file(s))\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "STEP 3: Initialize Model\n",
      "======================================================================\n",
      "Device for training: cuda\n",
      "\n",
      "======================================================================\n",
      "Initializing distilbert-base-uncased with LoRA\n",
      "======================================================================\n",
      "Some weights of DistilBertForTokenClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "\n",
      "‚úì LoRA target validation passed\n",
      "  Target modules: ['q_lin', 'v_lin']\n",
      "\n",
      "======================================================================\n",
      "LORA CONFIGURATION\n",
      "======================================================================\n",
      "  Rank (r):              32\n",
      "  Alpha:                 128\n",
      "  Effective LR scale:    4.00x\n",
      "  Dropout:               0.1\n",
      "  Target modules:        q_lin, v_lin\n",
      "  Modules to save:       None\n",
      "  Bias:                  all\n",
      "  Task type:             TaskType.TOKEN_CLS\n",
      "======================================================================\n",
      "\n",
      "üìä Trainable Parameters:\n",
      "trainable params: 642,818 || all params: 66,955,780 || trainable%: 0.9601\n",
      "\n",
      "======================================================================\n",
      "STEP 4: Train Model\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "TRAINING CONFIGURATION\n",
      "======================================================================\n",
      "Model:                    distilbert-base-uncased\n",
      "Output directory:         ./distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES\n",
      "Resume from checkpoint:   No\n",
      "Training examples:        1247\n",
      "Validation examples:      189\n",
      "Epochs:                   10\n",
      "Batch size:               16\n",
      "Learning rate:            0.0003\n",
      "Save every N epochs:      2\n",
      "Early stopping patience:  100\n",
      "Device:                   cuda\n",
      "======================================================================\n",
      "\n",
      "‚öñÔ∏è Adjusted Class Weights (multiplier=0.5):\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "‚öñÔ∏è Class Weights Applied:\n",
      "   Label 0 (Continue): 0.5622\n",
      "   Label 1 (Start):    2.2582\n",
      "   Ratio (1/0):        4.0164x\n",
      "\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "TRAINING STARTED\n",
      "üöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄüöÄ\n",
      "\n",
      "{'loss': 0.7223, 'grad_norm': 0.8852260708808899, 'learning_rate': 5.6999999999999996e-05, 'epoch': 0.26}\n",
      "{'loss': 0.6045, 'grad_norm': 0.5048426985740662, 'learning_rate': 0.000117, 'epoch': 0.51}\n",
      "{'loss': 0.4217, 'grad_norm': 1.8886140584945679, 'learning_rate': 0.00017699999999999997, 'epoch': 0.77}\n",
      " 10% 76/780 [00:03<00:24, 28.68it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                    \n",
      "\u001b[A{'eval_loss': 0.1945909708738327, 'eval_accuracy': 0.9397324327834784, 'eval_precision': 0.6821012948696935, 'eval_recall': 0.8717008797653959, 'eval_f1': 0.7653333333333333, 'eval_precision_class_0': 0.9831004180059872, 'eval_precision_class_1': 0.6821012948696935, 'eval_recall_class_0': 0.9483770511438495, 'eval_recall_class_1': 0.8717008797653959, 'eval_f1_class_0': 0.9654266128376731, 'eval_f1_class_1': 0.7653333333333333, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4978, 'eval_samples_per_second': 379.649, 'eval_steps_per_second': 12.052, 'epoch': 1.0}\n",
      " 10% 78/780 [00:04<00:24, 28.68it/s]\n",
      "100% 6/6 [00:00<00:00, 47.28it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 1.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1946\n",
      "  eval/accuracy             0.9397\n",
      "  eval/precision            0.6821\n",
      "  eval/recall               0.8717\n",
      "  eval/f1                   0.7653\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9654\n",
      "    eval/f1_class_1         0.7653\n",
      "    eval/precision_class_0  0.9831\n",
      "    eval/precision_class_1  0.6821\n",
      "    eval/recall_class_0     0.9484\n",
      "    eval/recall_class_1     0.8717\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.2392, 'grad_norm': 1.0839680433273315, 'learning_rate': 0.000237, 'epoch': 1.03}\n",
      "{'loss': 0.1834, 'grad_norm': 0.5838130712509155, 'learning_rate': 0.00029699999999999996, 'epoch': 1.28}\n",
      "{'loss': 0.1626, 'grad_norm': 0.7643641233444214, 'learning_rate': 0.0002916176470588235, 'epoch': 1.54}\n",
      "{'loss': 0.1503, 'grad_norm': 0.6187259554862976, 'learning_rate': 0.0002827941176470588, 'epoch': 1.79}\n",
      " 20% 154/780 [00:07<00:21, 28.49it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.12660425901412964, 'eval_accuracy': 0.9607387027831241, 'eval_precision': 0.7747946657246313, 'eval_recall': 0.9188311688311688, 'eval_f1': 0.8406880360308562, 'eval_precision_class_0': 0.9894365237303383, 'eval_precision_class_1': 0.7747946657246313, 'eval_recall_class_0': 0.9660638000558949, 'eval_recall_class_1': 0.9188311688311688, 'eval_f1_class_0': 0.977610483007535, 'eval_f1_class_1': 0.8406880360308562, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.495, 'eval_samples_per_second': 381.794, 'eval_steps_per_second': 12.12, 'epoch': 2.0}\n",
      " 20% 156/780 [00:07<00:21, 28.49it/s]\n",
      "100% 6/6 [00:00<00:00, 48.82it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 2.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1266\n",
      "  eval/accuracy             0.9607\n",
      "  eval/precision            0.7748\n",
      "  eval/recall               0.9188\n",
      "  eval/f1                   0.8407\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9776\n",
      "    eval/f1_class_1         0.8407\n",
      "    eval/precision_class_0  0.9894\n",
      "    eval/precision_class_1  0.7748\n",
      "    eval/recall_class_0     0.9661\n",
      "    eval/recall_class_1     0.9188\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1347, 'grad_norm': 0.3027133345603943, 'learning_rate': 0.0002739705882352941, 'epoch': 2.05}\n",
      "{'loss': 0.1313, 'grad_norm': 0.44284477829933167, 'learning_rate': 0.00026514705882352935, 'epoch': 2.31}\n",
      "{'loss': 0.1332, 'grad_norm': 0.19733533263206482, 'learning_rate': 0.0002563235294117647, 'epoch': 2.56}\n",
      "{'loss': 0.1277, 'grad_norm': 0.6765261888504028, 'learning_rate': 0.00024749999999999994, 'epoch': 2.82}\n",
      " 30% 232/780 [00:10<00:19, 28.44it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.11262887716293335, 'eval_accuracy': 0.9633364427493535, 'eval_precision': 0.7836077119464742, 'eval_recall': 0.9322371177209887, 'eval_f1': 0.8514851485148515, 'eval_precision_class_0': 0.9911768716759852, 'eval_precision_class_1': 0.7836077119464742, 'eval_recall_class_0': 0.9672881649166234, 'eval_recall_class_1': 0.9322371177209887, 'eval_f1_class_0': 0.9790868250365391, 'eval_f1_class_1': 0.8514851485148515, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.497, 'eval_samples_per_second': 380.249, 'eval_steps_per_second': 12.071, 'epoch': 3.0}\n",
      " 30% 234/780 [00:11<00:19, 28.44it/s]\n",
      "100% 6/6 [00:00<00:00, 47.33it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 3.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1126\n",
      "  eval/accuracy             0.9633\n",
      "  eval/precision            0.7836\n",
      "  eval/recall               0.9322\n",
      "  eval/f1                   0.8515\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9791\n",
      "    eval/f1_class_1         0.8515\n",
      "    eval/precision_class_0  0.9912\n",
      "    eval/precision_class_1  0.7836\n",
      "    eval/recall_class_0     0.9673\n",
      "    eval/recall_class_1     0.9322\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1183, 'grad_norm': 0.5473125576972961, 'learning_rate': 0.00023867647058823527, 'epoch': 3.08}\n",
      "{'loss': 0.1179, 'grad_norm': 0.37951767444610596, 'learning_rate': 0.0002298529411764706, 'epoch': 3.33}\n",
      "{'loss': 0.1118, 'grad_norm': 0.23257605731487274, 'learning_rate': 0.00022102941176470588, 'epoch': 3.59}\n",
      "{'loss': 0.1161, 'grad_norm': 0.25559094548225403, 'learning_rate': 0.00021220588235294118, 'epoch': 3.85}\n",
      " 40% 310/780 [00:14<00:16, 28.68it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.10326066613197327, 'eval_accuracy': 0.9651666686346515, 'eval_precision': 0.788929760028026, 'eval_recall': 0.9434436531210725, 'eval_f1': 0.8592960030525613, 'eval_precision_class_0': 0.9926300992207012, 'eval_precision_class_1': 0.788929760028026, 'eval_recall_class_0': 0.9679269639743948, 'eval_recall_class_1': 0.9434436531210725, 'eval_f1_class_0': 0.9801229011131175, 'eval_f1_class_1': 0.8592960030525613, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5088, 'eval_samples_per_second': 371.44, 'eval_steps_per_second': 11.792, 'epoch': 4.0}\n",
      " 40% 312/780 [00:14<00:16, 28.68it/s]\n",
      "100% 6/6 [00:00<00:00, 47.18it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 4.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.1033\n",
      "  eval/accuracy             0.9652\n",
      "  eval/precision            0.7889\n",
      "  eval/recall               0.9434\n",
      "  eval/f1                   0.8593\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9801\n",
      "    eval/f1_class_1         0.8593\n",
      "    eval/precision_class_0  0.9926\n",
      "    eval/precision_class_1  0.7889\n",
      "    eval/recall_class_0     0.9679\n",
      "    eval/recall_class_1     0.9434\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1087, 'grad_norm': 0.36177369952201843, 'learning_rate': 0.00020338235294117647, 'epoch': 4.1}\n",
      "{'loss': 0.106, 'grad_norm': 0.36645281314849854, 'learning_rate': 0.00019455882352941177, 'epoch': 4.36}\n",
      "{'loss': 0.102, 'grad_norm': 0.2832944989204407, 'learning_rate': 0.00018573529411764704, 'epoch': 4.62}\n",
      "{'loss': 0.1039, 'grad_norm': 0.1840124875307083, 'learning_rate': 0.00017691176470588233, 'epoch': 4.87}\n",
      " 50% 388/780 [00:17<00:13, 28.08it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09892434626817703, 'eval_accuracy': 0.9635726009281016, 'eval_precision': 0.7741114598354398, 'eval_recall': 0.9558022622538752, 'eval_f1': 0.8554154754651544, 'eval_precision_class_0': 0.9942112482853224, 'eval_precision_class_1': 0.7741114598354398, 'eval_recall_class_0': 0.9645599606073915, 'eval_recall_class_1': 0.9558022622538752, 'eval_f1_class_0': 0.9791611783222215, 'eval_f1_class_1': 0.8554154754651544, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5097, 'eval_samples_per_second': 370.798, 'eval_steps_per_second': 11.771, 'epoch': 5.0}\n",
      " 50% 390/780 [00:18<00:13, 28.08it/s]\n",
      "100% 6/6 [00:00<00:00, 48.15it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 5.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0989\n",
      "  eval/accuracy             0.9636\n",
      "  eval/precision            0.7741\n",
      "  eval/recall               0.9558\n",
      "  eval/f1                   0.8554\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9792\n",
      "    eval/f1_class_1         0.8554\n",
      "    eval/precision_class_0  0.9942\n",
      "    eval/precision_class_1  0.7741\n",
      "    eval/recall_class_0     0.9646\n",
      "    eval/recall_class_1     0.9558\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.1056, 'grad_norm': 0.21349509060382843, 'learning_rate': 0.00016808823529411763, 'epoch': 5.13}\n",
      "{'loss': 0.0984, 'grad_norm': 0.22527679800987244, 'learning_rate': 0.00015926470588235292, 'epoch': 5.38}\n",
      "{'loss': 0.099, 'grad_norm': 0.24906538426876068, 'learning_rate': 0.00015044117647058822, 'epoch': 5.64}\n",
      "{'loss': 0.0967, 'grad_norm': 0.2785646319389343, 'learning_rate': 0.0001416176470588235, 'epoch': 5.9}\n",
      " 60% 466/780 [00:21<00:11, 28.29it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09739576280117035, 'eval_accuracy': 0.9667135047054517, 'eval_precision': 0.7935607713114039, 'eval_recall': 0.9525555090071219, 'eval_f1': 0.8658194107287353, 'eval_precision_class_0': 0.9938138416998962, 'eval_precision_class_1': 0.7935607713114039, 'eval_recall_class_0': 0.9685125297773519, 'eval_recall_class_1': 0.9525555090071219, 'eval_f1_class_0': 0.9810000741394765, 'eval_f1_class_1': 0.8658194107287353, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5032, 'eval_samples_per_second': 375.59, 'eval_steps_per_second': 11.923, 'epoch': 6.0}\n",
      " 60% 468/780 [00:22<00:11, 28.29it/s]\n",
      "100% 6/6 [00:00<00:00, 48.95it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 6.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0974\n",
      "  eval/accuracy             0.9667\n",
      "  eval/precision            0.7936\n",
      "  eval/recall               0.9526\n",
      "  eval/f1                   0.8658\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9810\n",
      "    eval/f1_class_1         0.8658\n",
      "    eval/precision_class_0  0.9938\n",
      "    eval/precision_class_1  0.7936\n",
      "    eval/recall_class_0     0.9685\n",
      "    eval/recall_class_1     0.9526\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0911, 'grad_norm': 0.1906311959028244, 'learning_rate': 0.0001327941176470588, 'epoch': 6.15}\n",
      "{'loss': 0.0913, 'grad_norm': 0.26400500535964966, 'learning_rate': 0.0001239705882352941, 'epoch': 6.41}\n",
      "{'loss': 0.0941, 'grad_norm': 0.23215311765670776, 'learning_rate': 0.00011514705882352941, 'epoch': 6.67}\n",
      "{'loss': 0.0919, 'grad_norm': 0.20735253393650055, 'learning_rate': 0.0001063235294117647, 'epoch': 6.92}\n",
      " 70% 544/780 [00:25<00:08, 28.75it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09606014937162399, 'eval_accuracy': 0.9661703408943311, 'eval_precision': 0.7887323943661971, 'eval_recall': 0.9560117302052786, 'eval_f1': 0.8643530135883718, 'eval_precision_class_0': 0.994255703266043, 'eval_precision_class_1': 0.7887323943661971, 'eval_recall_class_0': 0.9674611729947699, 'eval_recall_class_1': 0.9560117302052786, 'eval_f1_class_0': 0.9806754487140573, 'eval_f1_class_1': 0.8643530135883718, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4918, 'eval_samples_per_second': 384.294, 'eval_steps_per_second': 12.2, 'epoch': 7.0}\n",
      " 70% 546/780 [00:25<00:08, 28.75it/s]\n",
      "100% 6/6 [00:00<00:00, 49.19it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 7.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0961\n",
      "  eval/accuracy             0.9662\n",
      "  eval/precision            0.7887\n",
      "  eval/recall               0.9560\n",
      "  eval/f1                   0.8644\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9807\n",
      "    eval/f1_class_1         0.8644\n",
      "    eval/precision_class_0  0.9943\n",
      "    eval/precision_class_1  0.7887\n",
      "    eval/recall_class_0     0.9675\n",
      "    eval/recall_class_1     0.9560\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0958, 'grad_norm': 0.27189552783966064, 'learning_rate': 9.75e-05, 'epoch': 7.18}\n",
      "{'loss': 0.0924, 'grad_norm': 0.19142812490463257, 'learning_rate': 8.867647058823529e-05, 'epoch': 7.44}\n",
      "{'loss': 0.089, 'grad_norm': 0.1937096118927002, 'learning_rate': 7.985294117647057e-05, 'epoch': 7.69}\n",
      "{'loss': 0.0882, 'grad_norm': 0.4070001542568207, 'learning_rate': 7.102941176470587e-05, 'epoch': 7.95}\n",
      " 80% 622/780 [00:28<00:05, 28.20it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09381521493196487, 'eval_accuracy': 0.9698662163917392, 'eval_precision': 0.8142292490118577, 'eval_recall': 0.9493087557603687, 'eval_f1': 0.8765957446808511, 'eval_precision_class_0': 0.99342006879019, 'eval_precision_class_1': 0.8142292490118577, 'eval_recall_class_0': 0.972478407261016, 'eval_recall_class_1': 0.9493087557603687, 'eval_f1_class_0': 0.9828376978843024, 'eval_f1_class_1': 0.8765957446808511, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.5002, 'eval_samples_per_second': 377.811, 'eval_steps_per_second': 11.994, 'epoch': 8.0}\n",
      " 80% 624/780 [00:29<00:05, 28.20it/s]\n",
      "100% 6/6 [00:00<00:00, 47.54it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 8.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0938\n",
      "  eval/accuracy             0.9699\n",
      "  eval/precision            0.8142\n",
      "  eval/recall               0.9493\n",
      "  eval/f1                   0.8766\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9828\n",
      "    eval/f1_class_1         0.8766\n",
      "    eval/precision_class_0  0.9934\n",
      "    eval/precision_class_1  0.8142\n",
      "    eval/recall_class_0     0.9725\n",
      "    eval/recall_class_1     0.9493\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0839, 'grad_norm': 0.29846498370170593, 'learning_rate': 6.220588235294118e-05, 'epoch': 8.21}\n",
      "{'loss': 0.0876, 'grad_norm': 0.2412969022989273, 'learning_rate': 5.3382352941176466e-05, 'epoch': 8.46}\n",
      "{'loss': 0.0907, 'grad_norm': 0.3677862584590912, 'learning_rate': 4.455882352941176e-05, 'epoch': 8.72}\n",
      "{'loss': 0.0873, 'grad_norm': 0.23944830894470215, 'learning_rate': 3.5735294117647056e-05, 'epoch': 8.97}\n",
      " 90% 700/780 [00:32<00:02, 28.63it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.0933687835931778, 'eval_accuracy': 0.9695946344861789, 'eval_precision': 0.8116006792385378, 'eval_recall': 0.9510892333472979, 'eval_f1': 0.8758258185851376, 'eval_precision_class_0': 0.9936462585034014, 'eval_precision_class_1': 0.8116006792385378, 'eval_recall_class_0': 0.9719460747128731, 'eval_recall_class_1': 0.9510892333472979, 'eval_f1_class_0': 0.9826763813483493, 'eval_f1_class_1': 0.8758258185851376, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4951, 'eval_samples_per_second': 381.755, 'eval_steps_per_second': 12.119, 'epoch': 9.0}\n",
      " 90% 702/780 [00:32<00:02, 28.63it/s]\n",
      "100% 6/6 [00:00<00:00, 48.07it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 9.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0934\n",
      "  eval/accuracy             0.9696\n",
      "  eval/precision            0.8116\n",
      "  eval/recall               0.9511\n",
      "  eval/f1                   0.8758\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9827\n",
      "    eval/f1_class_1         0.8758\n",
      "    eval/precision_class_0  0.9936\n",
      "    eval/precision_class_1  0.8116\n",
      "    eval/recall_class_0     0.9719\n",
      "    eval/recall_class_1     0.9511\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'loss': 0.0845, 'grad_norm': 0.1715136468410492, 'learning_rate': 2.691176470588235e-05, 'epoch': 9.23}\n",
      "{'loss': 0.0795, 'grad_norm': 0.3131415545940399, 'learning_rate': 1.8088235294117645e-05, 'epoch': 9.49}\n",
      "{'loss': 0.0887, 'grad_norm': 0.1920175552368164, 'learning_rate': 9.26470588235294e-06, 'epoch': 9.74}\n",
      "{'loss': 0.0896, 'grad_norm': 0.25322893261909485, 'learning_rate': 4.4117647058823526e-07, 'epoch': 10.0}\n",
      "100% 780/780 [00:35<00:00, 28.48it/s]\n",
      "  0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
      "                                     \n",
      "\u001b[A{'eval_loss': 0.09301222860813141, 'eval_accuracy': 0.9692758209448689, 'eval_precision': 0.8086562388908638, 'eval_recall': 0.9529744449099288, 'eval_f1': 0.8749038461538462, 'eval_precision_class_0': 0.9938859158190013, 'eval_precision_class_1': 0.8086562388908638, 'eval_recall_class_0': 0.9713472005962125, 'eval_recall_class_1': 0.9529744449099288, 'eval_f1_class_0': 0.9824873130611531, 'eval_f1_class_1': 0.8749038461538462, 'eval_support_class_0': 75141, 'eval_support_class_1': 9548, 'eval_runtime': 0.4927, 'eval_samples_per_second': 383.567, 'eval_steps_per_second': 12.177, 'epoch': 10.0}\n",
      "100% 780/780 [00:36<00:00, 28.48it/s]\n",
      "100% 6/6 [00:00<00:00, 49.28it/s]\u001b[A\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0930\n",
      "  eval/accuracy             0.9693\n",
      "  eval/precision            0.8087\n",
      "  eval/recall               0.9530\n",
      "  eval/f1                   0.8749\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9825\n",
      "    eval/f1_class_1         0.8749\n",
      "    eval/precision_class_0  0.9939\n",
      "    eval/precision_class_1  0.8087\n",
      "    eval/recall_class_0     0.9713\n",
      "    eval/recall_class_1     0.9530\n",
      "======================================================================\n",
      "\n",
      "\n",
      "{'train_runtime': 36.6612, 'train_samples_per_second': 340.142, 'train_steps_per_second': 21.276, 'train_loss': 0.14668730298678082, 'epoch': 10.0}\n",
      "100% 780/780 [00:36<00:00, 21.28it/s]\n",
      "\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "TRAINING COMPLETED\n",
      "‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ‚úÖ\n",
      "\n",
      "======================================================================\n",
      "FINAL TRAINING METRICS\n",
      "======================================================================\n",
      "train_runtime..................................... 36.6612\n",
      "train_samples_per_second.......................... 340.1420\n",
      "train_steps_per_second............................ 21.2760\n",
      "total_flos........................................ 1651897958277120.0000\n",
      "train_loss........................................ 0.1467\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "VALIDATION METRICS\n",
      "======================================================================\n",
      "100% 6/6 [00:00<00:00, 47.31it/s]\n",
      "======================================================================\n",
      "üìä Epoch 10.00 - Evaluation Metrics\n",
      "======================================================================\n",
      "  eval/loss                 0.0938\n",
      "  eval/accuracy             0.9699\n",
      "  eval/precision            0.8142\n",
      "  eval/recall               0.9493\n",
      "  eval/f1                   0.8766\n",
      "\n",
      "  Per-class metrics:\n",
      "    eval/f1_class_0         0.9828\n",
      "    eval/f1_class_1         0.8766\n",
      "    eval/precision_class_0  0.9934\n",
      "    eval/precision_class_1  0.8142\n",
      "    eval/recall_class_0     0.9725\n",
      "    eval/recall_class_1     0.9493\n",
      "======================================================================\n",
      "\n",
      "100% 6/6 [00:00<00:00, 16.60it/s]\n",
      "eval_loss......................................... 0.0938\n",
      "eval_accuracy..................................... 0.9699\n",
      "eval_precision.................................... 0.8142\n",
      "eval_recall....................................... 0.9493\n",
      "eval_f1........................................... 0.8766\n",
      "eval_precision_class_0............................ 0.9934\n",
      "eval_precision_class_1............................ 0.8142\n",
      "eval_recall_class_0............................... 0.9725\n",
      "eval_recall_class_1............................... 0.9493\n",
      "eval_f1_class_0................................... 0.9828\n",
      "eval_f1_class_1................................... 0.8766\n",
      "eval_support_class_0.............................. 75141.0000\n",
      "eval_support_class_1.............................. 9548.0000\n",
      "eval_runtime...................................... 0.5193\n",
      "eval_samples_per_second........................... 363.9200\n",
      "eval_steps_per_second............................. 11.5530\n",
      "epoch............................................. 10.0000\n",
      "\n",
      "======================================================================\n",
      "Saving best model to: distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "======================================================================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES/best_model)... Done. 0.0s\n",
      "‚úì Model saved successfully\n",
      "\n",
      "======================================================================\n",
      "STEP 5: Final Test Set Evaluation\n",
      "======================================================================\n",
      "‚úì PEFT model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "LOADING MODEL FOR TEST EVALUATION\n",
      "======================================================================\n",
      "Model path: distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "Test examples: 245\n",
      "‚úì Model loaded successfully\n",
      "\n",
      "======================================================================\n",
      "TEST SET EVALUATION\n",
      "======================================================================\n",
      "100% 16/16 [00:00<00:00, 30.73it/s]\n",
      "\n",
      "üìä TEST METRICS:\n",
      "----------------------------------------------------------------------\n",
      "eval_loss......................................... 0.0667\n",
      "eval_model_preparation_time....................... 0.0022\n",
      "eval_accuracy..................................... 0.9744\n",
      "eval_precision.................................... 0.8421\n",
      "eval_recall....................................... 0.9473\n",
      "eval_f1........................................... 0.8916\n",
      "eval_precision_class_0............................ 0.9933\n",
      "eval_precision_class_1............................ 0.8421\n",
      "eval_recall_class_0............................... 0.9778\n",
      "eval_recall_class_1............................... 0.9473\n",
      "eval_f1_class_0................................... 0.9855\n",
      "eval_f1_class_1................................... 0.8916\n",
      "eval_support_class_0.............................. 99651.0000\n",
      "eval_support_class_1.............................. 12438.0000\n",
      "eval_runtime...................................... 0.6604\n",
      "eval_samples_per_second........................... 370.9800\n",
      "eval_steps_per_second............................. 24.2270\n",
      "100% 16/16 [00:00<00:00, 32.48it/s] \n",
      "\n",
      "======================================================================\n",
      "DETAILED CLASSIFICATION REPORT\n",
      "======================================================================\n",
      "\n",
      "Class Labels:\n",
      "  0 = EDU Continue (token within current EDU)\n",
      "  1 = EDU Start (token begins new EDU)\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "EDU Continue (0)     0.9933    0.9778    0.9855     99651\n",
      "   EDU Start (1)     0.8421    0.9473    0.8916     12438\n",
      "\n",
      "        accuracy                         0.9744    112089\n",
      "       macro avg     0.9177    0.9625    0.9385    112089\n",
      "    weighted avg     0.9765    0.9744    0.9751    112089\n",
      "\n",
      "\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "TRAINING PIPELINE COMPLETE!\n",
      "=*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*==*=\n",
      "\n",
      "üìä FINAL RESULTS SUMMARY:\n",
      "======================================================================\n",
      "Metric                         Validation           Test                \n",
      "----------------------------------------------------------------------\n",
      "Accuracy                       0.9699               0.9744              \n",
      "Precision                      0.8142               0.8421              \n",
      "Recall                         0.9493               0.9473              \n",
      "F1                             0.8766               0.8916              \n",
      "======================================================================\n",
      "\n",
      "‚úÖ Model saved at: distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES/best_model\n",
      "‚úÖ Logs saved at: ./distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES/logs\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m updating run metadata (3.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-s2tq4k53-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚¢ø\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m updating run metadata (3.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-s2tq4k53-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ª\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m updating run metadata (3.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-s2tq4k53-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ω\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m updating run metadata (3.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-s2tq4k53-test_confusion_matrix_table (0.6s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£æ\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.2s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£∑\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£Ø\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m‚£ü\u001b[0m uploading artifact run-s2tq4k53-final_results_table (0.7s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss ‚ñà‚ñà‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 ‚ñÅ‚ñÅ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall ‚ñÅ‚ñÅ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +57 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                  epoch 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/accuracy 0.96987\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                eval/f1 0.8766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_0 0.98284\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/f1_class_1 0.8766\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              eval/loss 0.09382\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         eval/precision 0.81423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_0 0.99342\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/precision_class_1 0.81423\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/recall 0.94931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    +62 ...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mdistilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation/runs/s2tq4k53\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/k_vighnesh-concordia-university/Multiple-Run-edu-segmentation\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 2 media file(s), 13 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20251122_040057-s2tq4k53/logs\u001b[0m\n",
      "‚úÖ W&B run completed\n"
     ]
    }
   ],
   "source": [
    "!./.venv/bin/python -m com.disrpt.segmenter.fine_tune_with_linear_head \\\n",
    "  --model_name distilbert-base-uncased \\\n",
    "  --use_wandb \\\n",
    "  --wandb_project Multiple-Run-edu-segmentation \\\n",
    "  --wandb_group grp4_bert_distillbert_cw_rank_alpha  \\\n",
    "  --wandb_run_name \"distilbert-base-uncased_cw0.5_r32_a128_do0.1_lr0.0003_ep10\" \\\n",
    "  --class_1_weight_multiplier 0.5 \\\n",
    "  --lora_r 32 \\\n",
    "  --lora_alpha 128 \\\n",
    "  --lora_dropout 0.1 \\\n",
    "  --epochs 10 \\\n",
    "  --early_stopping_patience 100 \\\n",
    "  --output_dir \"./distilbert_grp4_cw0_5_rank_32_alpha_128_ep10_noES\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Yyp5nnyraB6",
    "outputId": "6c8d3959-71c7-437d-ccd3-5741bbc163cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/DISRPT-Segmenter\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/trainer_state.json (deflated 77%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/optimizer.pt (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-468/README.md (deflated 66%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/best_model/README.md (deflated 66%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/trainer_state.json (deflated 79%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/scheduler.pt (deflated 62%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/optimizer.pt (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-780/README.md (deflated 66%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/trainer_state.json (deflated 78%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/optimizer.pt (deflated 9%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-702/README.md (deflated 66%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/trainer_state.json (deflated 77%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/optimizer.pt (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-546/README.md (deflated 66%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/ (stored 0%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/vocab.txt (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_config.json (deflated 56%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/rng_state.pth (deflated 26%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/adapter_model.safetensors (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer_config.json (deflated 75%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scaler.pt (deflated 64%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/special_tokens_map.json (deflated 42%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/trainer_state.json (deflated 78%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/training_args.bin (deflated 53%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/tokenizer.json (deflated 71%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/scheduler.pt (deflated 61%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/optimizer.pt (deflated 8%)\n",
      "  adding: bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES/checkpoint-624/README.md (deflated 66%)\n"
     ]
    }
   ],
   "source": [
    "%cd /content/DISRPT-Segmenter\n",
    "\n",
    "!zip -r bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES.zip \\\n",
    "  bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mOHSictku-tI",
    "outputId": "a7805fd0-b8c1-4a9f-cdef-fc152c7f08da"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "    async function download(id, filename, size) {\n",
       "      if (!google.colab.kernel.accessAllowed) {\n",
       "        return;\n",
       "      }\n",
       "      const div = document.createElement('div');\n",
       "      const label = document.createElement('label');\n",
       "      label.textContent = `Downloading \"${filename}\": `;\n",
       "      div.appendChild(label);\n",
       "      const progress = document.createElement('progress');\n",
       "      progress.max = size;\n",
       "      div.appendChild(progress);\n",
       "      document.body.appendChild(div);\n",
       "\n",
       "      const buffers = [];\n",
       "      let downloaded = 0;\n",
       "\n",
       "      const channel = await google.colab.kernel.comms.open(id);\n",
       "      // Send a message to notify the kernel that we're ready.\n",
       "      channel.send({})\n",
       "\n",
       "      for await (const message of channel.messages) {\n",
       "        // Send a message to notify the kernel that we're ready.\n",
       "        channel.send({})\n",
       "        if (message.buffers) {\n",
       "          for (const buffer of message.buffers) {\n",
       "            buffers.push(buffer);\n",
       "            downloaded += buffer.byteLength;\n",
       "            progress.value = downloaded;\n",
       "          }\n",
       "        }\n",
       "      }\n",
       "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
       "      const a = document.createElement('a');\n",
       "      a.href = window.URL.createObjectURL(blob);\n",
       "      a.download = filename;\n",
       "      div.appendChild(a);\n",
       "      a.click();\n",
       "      div.remove();\n",
       "    }\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "download(\"download_2dca2ef2-4e49-4c69-b256-d92f7f0d8f77\", \"bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES.zip\", 43499079)"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from google.colab import files\n",
    "files.download(\"bert_grp2_cw0_5_rank_16_alpha_128_ep10_noES.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hl5vrFrdvpWn"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
